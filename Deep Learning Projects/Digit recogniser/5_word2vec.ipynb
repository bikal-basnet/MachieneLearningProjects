{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Word Vector : Skip-gram Model\n",
    "=============\n",
    "\n",
    "With this project, the words are translated into vector embedding using skip-gram neural network model, in turn capturing the semantic and syntactic  knowledge of the word in the process. Semantic : house ~ apartment. Semantic : Puppy is to dog, what Kitten is to cat. ",
    "\n",
    "The Word2Vec skip-gram model if generated in this project over [Text8](http://mattmahoney.net/dc/textdata) data.\n",
    "\n",
    "**REF:To Know more on the skip-gram models , the theoritical foundation please visit [Skip-gram Word2Vector Models](https://deepdatascience.wordpress.com/2017/04/22/word2vec-skip-gram-model/),\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0K1ZyLn04QZf"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCjPJE944bkV"
   },
   "source": [
    "STEP 1 : Download Text data\n",
    "----------------------------\n",
    "Download the text data from the source website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 14640,
     "status": "ok",
     "timestamp": 1445964482948,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "c4ec222c-80b5-4298-e635-93ca9f79c3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Completed 2017-05-09 07:41:31.418077\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "print('Completed', datetime.datetime.now() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zqz3XiqI4mZT"
   },
   "source": [
    "**Step 2 :Tokenize text data into words.**\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 28844,
     "status": "ok",
     "timestamp": 1445964497165,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "e3a928b4-1645-4fe8-be17-fcf47de5716d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size in number of words 5094672\n",
      "Completed 2017-05-09 07:41:32.112716\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "  return data\n",
    "  \n",
    "words = read_data('text8_mid_30MB.zip')\n",
    "print('Data size in number of words %d' % len(words))\n",
    "print('Completed', datetime.datetime.now() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing\n"
     ]
    }
   ],
   "source": [
    "print(type(words))\n",
    "print(words[0:10])\n",
    "print( \" \".join(words[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Step 3: Build the  word  dictionary  i.e word - index \n",
    "------------------------------------------\n",
    "1. Dictionary :  word - index dictionary as in  { word1 : 1, word2 : 2 }\n",
    "2. Reverse_dictionary :  contains all words with key as index and value as word, so that it's easy to reference them e.g { word1 : 1, word2 : 2 } to reverse_dictionary becomes\n",
    "{  1: word1 , 2: word2  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 28849,
     "status": "ok",
     "timestamp": 1445964497178,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "3fb4ecd1-df67-44b6-a2dc-2291730970b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 97870], ('the', 325643), ('of', 182950), ('and', 127938), ('one', 116440)]\n",
      "Sample data [3181, 2980, 12, 6, 181, 2, 3736, 49, 58, 149]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary -  {'fawn': 42300, 'homomorphism': 15286, 'nunnery': 29778, 'chthonic': 31253, 'sonja': 42301, 'woods': 6307, 'clotted': 42302, 'spiders': 13019, 'gai': 34017, 'hanging': 7083}\n",
      "reverse dictionary -  {0: 'UNK', 1: 'the', 2: 'of', 3: 'and', 4: 'one', 5: 'in', 6: 'a', 7: 'to', 8: 'zero', 9: 'nine'}\n"
     ]
    }
   ],
   "source": [
    "n_pairs_dict = {k : dictionary[k] for k in dictionary.keys()[:10]}\n",
    "print('dictionary - ', n_pairs_dict )\n",
    "\n",
    "n_pairs_reverse_dict = {k : reverse_dictionary[k] for k in reverse_dictionary.keys()[:10]}\n",
    "print('reverse dictionary - ',n_pairs_reverse_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Step 4:  Morph word frequency into Skip-gram data\n",
    "------------------------------\n",
    "Skip gram model aims to predict the n-context words given a input target word. And in doins so, the nice side effect is that the word vectors are represented in an meaningful way capturing semantic and syntactic  relevance. \n",
    "In our case, since we have removed the commonly occuring words, our skip-gram model will  capture semantic similarity better and lesser of the syntactic relevance.\n",
    "\n",
    "\n",
    "**skip_window** = 1 # How many words to consider left and right.\n",
    "\n",
    "**num_skips** = 2 # How many times to reuse an input to generate a label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "[2980, 12, 6, 181, 2, 3736, 49, 58, 149]\n",
      "ohio\n",
      "2980\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "print(data[1:10])\n",
    "print(reverse_dictionary[2307])\n",
    "print(dictionary['originated']  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1445964901989,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w9APjA-zmfjV",
    "outputId": "67cccb02-cdaf-4e47-d489-43bcc8d57bb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Train / Test Data from the text.\n",
      "Sample of first - 8 words data\n",
      "anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english\n",
      "\n",
      "with number of words to skip = 1 and skip_window = 1:\n",
      "train_data : label is  [('originated', 'as'), ('as', 'a'), ('a', 'term'), ('term', 'of'), ('of', 'abuse'), ('abuse', 'first'), ('first', 'abuse'), ('used', 'first')]\n",
      "\n",
      "with number of words to skip = 1 and skip_window = 2:\n",
      "train_data : label is  [('as', 'term'), ('a', 'originated'), ('term', 'abuse'), ('of', 'abuse'), ('abuse', 'of'), ('first', 'against'), ('used', 'abuse'), ('against', 'early')]\n",
      "\n",
      "with number of words to skip = 2 and skip_window = 1:\n",
      "train_data : label is  [('originated', 'anarchism'), ('originated', 'as'), ('as', 'originated'), ('as', 'a'), ('a', 'term'), ('a', 'as'), ('term', 'of'), ('term', 'a')]\n",
      "\n",
      "with number of words to skip = 2 and skip_window = 2:\n",
      "train_data : label is  [('as', 'originated'), ('as', 'anarchism'), ('a', 'term'), ('a', 'of'), ('term', 'abuse'), ('term', 'a'), ('of', 'term'), ('of', 'first')]\n",
      "\n",
      "with number of words to skip = 4 and skip_window = 2:\n",
      "train_data : label is  [('as', 'originated'), ('as', 'term'), ('as', 'anarchism'), ('as', 'a'), ('a', 'as'), ('a', 'term'), ('a', 'of'), ('a', 'originated')]\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window, reverse_dictionary = None , verbose = 0):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):    \n",
    "    if verbose ==1:print('appending to buffer ',reverse_dictionary[data[data_index]])\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  if verbose ==1:print('iteration from 0 to ', batch_size // num_skips - 1,' inclusive' )\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer    \n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    if verbose ==1:print('targets to avoid is skip_window ', targets_to_avoid)\n",
    "    if verbose ==1:print('iterating from 0 to ', num_skips -1 ,' inclusive')\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "        if verbose ==1:print('getting target  word  randomly between 0 and ',span-1,' but not in the targets to avoid list', targets_to_avoid)\n",
    "      targets_to_avoid.append(target)\n",
    "      if verbose ==1:print('now add word ', reverse_dictionary[target], ' to targets_to_avoid list. After addition it is ',targets_to_avoid)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]      \n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "      if verbose ==1:print( 'Adding word to batch , adding word to label  ',\\\n",
    "            reverse_dictionary[batch[i * num_skips + j]], reverse_dictionary[labels[i * num_skips + j, 0]] )\n",
    "    buffer.append(data[data_index])\n",
    "    if verbose ==1:print('adding ',reverse_dictionary[data[data_index]], ' to buffer ')\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "    if verbose ==1:print('new data_index now is ', reverse_dictionary[data_index], ' from (data_index + 1) % len(data)' ,(data_index + 1) , len(data) )\n",
    "    if verbose ==1:print(batch, labels)\n",
    "  return batch, labels\n",
    "\n",
    "print('Generating Train / Test Data from the text.')\n",
    "print('Sample of first - 8 words data')\n",
    "print(' '.join([reverse_dictionary[di] for di in data[:20]]) )\n",
    "\n",
    "for num_skips, skip_window in [(1 , 1) , (1 , 2), (2, 1), (2,2) ,  (4, 2)]:\n",
    "    data_index = 0\n",
    "    _batch_size = 8\n",
    "    #_batch_size = 12\n",
    "    batch, labels = generate_batch(batch_size=_batch_size, num_skips=num_skips, skip_window=skip_window,\\\n",
    "                                   reverse_dictionary = reverse_dictionary)\n",
    "    print('\\nwith number of words to skip = %d and skip_window = %d:' % (num_skips, skip_window))        \n",
    "    train_word_list = [reverse_dictionary[bi] for bi in batch] \n",
    "    label_word_list = [reverse_dictionary[li] for li in labels.reshape(_batch_size)]\n",
    "    print('train_data : label is ', zip (train_word_list , label_word_list ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Analysis : \n",
    "--------------\n",
    "word_window_size = 2 *  skip_window  + 1 \n",
    "i.e skip_window  @ 1  =  w-1 w w +1 \n",
    "i.e skip_window  @ 2  =  w-2 w-1 w w +1 w+2\n",
    "i.e skip_window  @ 3  =  w-3 w-2 w-1 w w +1 w+2 w+3\n",
    "\n",
    "num_skips = how many words can be skipped\n",
    "i.e num_skips @ 1 = w-2 w w+2 , w-1 w w+1\n",
    "\n",
    "\n",
    "In the code above, we have designed the   train data and labels, on the proximity basis. i.e when we are looking for 1 word near proximity i.e skip_window = 1,  for the word \"originated\" nearest words are \"anarchism\" and \"as\", as in the phrase \"anarchism **originated** as\". Hence, it is why for the word originated, our labels are \"anarchism\" and \"as\".\n",
    "\n",
    "Similary, when the skip_window = 2 i.e we are considering the words 2 distance away as the labels, then for the word \"as\", the labels would be  2 words prior to \"as\" i.e  \"anarchism\" and \"originated\" and 2 words post to \"as\" i.e \"a\" and \"term\" as in the phrase \"anarchism originated **as** a term\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ofd1MbBuwiva"
   },
   "source": [
    "Train a skip-gram model  with negative sampling\n",
    "--------------------\n",
    "**num_sampled ** : Number of negative  examples to sample i.e no of unrelatd word pairs to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 12 12 12  6  6  6  6]\n",
      "[[2980]\n",
      " [ 181]\n",
      " [3181]\n",
      " [   6]\n",
      " [  12]\n",
      " [ 181]\n",
      " [   2]\n",
      " [2980]]\n"
     ]
    }
   ],
   "source": [
    "print(batch)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "8pQKsV4Vwlzy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset size is  (128,)  train_labels size is  (128, 1)\n",
      "embeddings has size [vocabulary_size, embedding_size] = [ 50000 128 ]\n",
      "From embedding vector and train_dataset, input  embedding of  following size is computed (128, 128)\n",
      "Tensor(\"embedding_lookup:0\", shape=(128, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "#with graph.as_default(), tf.device('/cpu:0'):\n",
    "with graph.as_default():\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  print( 'train_dataset size is ', train_dataset.get_shape() ,' train_labels size is ', train_labels.get_shape() )\n",
    "  print('embeddings has size [vocabulary_size, embedding_size] = [', vocabulary_size, embedding_size,']')\n",
    "  # Variables.\n",
    "  embeddings = tf.Variable( tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(  tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Model.\n",
    "  # Look up embeddings for inputs.\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "  print('From embedding vector and train_dataset, input  embedding of  following size is computed', embed.get_shape())  \n",
    "  print(embed)  \n",
    "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "  # This is because the embeddings are defined as a variable quantity and the\n",
    "  # optimizer's `minimize` method will by default modify all variable quantities \n",
    "  # that contribute to the tensor it is passed.\n",
    "  # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "  # Compute the similarity between minibatch examples and all embeddings.\n",
    "  # We use the cosine distance:\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "    normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 23
      },
      {
       "item_id": 48
      },
      {
       "item_id": 61
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 436189,
     "status": "ok",
     "timestamp": 1445965429787,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "1bQFGceBxrWW",
    "outputId": "5ebd6d9a-33c6-4bcd-bf6d-252b0b6055e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "1st Batch Training data  is  [   58    58   149   149   125   125   850   850   489   489  8335  8335\n",
      "   133   133     1     1 35460 35460     2     2     1     1   115   115\n",
      "  1016  1016     3     3     1     1 21303 21303     0     0     2     2\n",
      "     1     1   175   175  1016  1016  3536  3536     1     1   181   181\n",
      "    11    11   187   187    58    58     5     5     6     6 11754 11754\n",
      "   212   212     7     7  1338  1338   102   102   426   426    20    20\n",
      "    58    58  3237  3237   379   379     7     7  3782  3782     1     1\n",
      "   796   796     2     2   399   399    28    28    41    41    37    37\n",
      "    53    53   492   492    99    99    12    12     6     6  1422  1422\n",
      "  3257  3257    17    17   565   565   779   779  4168  4168     1     1\n",
      "   263   263  3181  3181    11    11   981   981]\n",
      "1st Batch Training label  is  [   58    58   149   149   125   125   850   850   489   489  8335  8335\n",
      "   133   133     1     1 35460 35460     2     2     1     1   115   115\n",
      "  1016  1016     3     3     1     1 21303 21303     0     0     2     2\n",
      "     1     1   175   175  1016  1016  3536  3536     1     1   181   181\n",
      "    11    11   187   187    58    58     5     5     6     6 11754 11754\n",
      "   212   212     7     7  1338  1338   102   102   426   426    20    20\n",
      "    58    58  3237  3237   379   379     7     7  3782  3782     1     1\n",
      "   796   796     2     2   399   399    28    28    41    41    37    37\n",
      "    53    53   492   492    99    99    12    12     6     6  1422  1422\n",
      "  3257  3257    17    17   565   565   779   779  4168  4168     1     1\n",
      "   263   263  3181  3181    11    11   981   981]\n",
      "1st Batch Training dataset size : 128  Batch Training label size :  128\n",
      "Average loss at step 0: 7.853448\n",
      "Nearest to after: flagging, movna, jacky, nexus, transvestite, clipped, overs, caveat,\n",
      "Nearest to b: panoramas, remixed, touristic, solutes, tailpiece, stratofortress, pinyin, prewar,\n",
      "Nearest to american: annexes, anza, gong, volt, playable, circa, phoenix, says,\n",
      "Nearest to which: autres, reporting, marianne, cyrene, cvc, strategic, owe, cemetery,\n",
      "Nearest to eight: curving, civilization, universita, doctor, heterosexuality, exclaims, les, phraseology,\n",
      "Nearest to such: municipalities, ecuador, coffin, warszawa, napalm, awful, zeeman, arnold,\n",
      "Nearest to other: abkhazian, yearly, smyrna, tiwanaku, brasil, engelm, salting, riff,\n",
      "Nearest to it: natives, blackfoot, inoculated, tranquilizers, relativity, dispensing, precipitating, responding,\n",
      "Nearest to on: lennon, voortrekker, librorum, transversae, monsanto, counterarguments, nimitz, mannitol,\n",
      "Nearest to th: architectures, annually, xsl, chamaeleon, hebrides, pardubice, parallelization, burundian,\n",
      "Nearest to has: backus, lutheran, nsaid, capitalisation, mg, ward, souterrain, sonja,\n",
      "Nearest to united: affection, bardot, retreats, intertwined, executioner, remaining, asynchronous, bulldog,\n",
      "Nearest to at: gabriel, yamaha, genius, sistema, moseley, alif, presbyterian, philanthropist,\n",
      "Nearest to use: anderssons, neurobiological, hedging, deuterocanon, weizmann, aix, jambalaya, jammu,\n",
      "Nearest to states: ferrero, stamps, reasonably, lottery, maas, copyrightable, tended, numbered,\n",
      "Nearest to in: waterfall, elephant, drafting, nand, swarm, dispersions, viable, cyprus,\n",
      "Average loss at step 2000: 4.374070\n",
      "Average loss at step 4000: 3.886903\n",
      "Average loss at step 6000: 3.803111\n",
      "Average loss at step 8000: 3.700870\n",
      "Average loss at step 10000: 3.622752\n",
      "Nearest to after: for, in, obliged, per, gaborone, catarina, flagging, on,\n",
      "Nearest to b: leto, icbms, remixed, pinyin, d, tailpiece, dialectical, vara,\n",
      "Nearest to american: anza, playable, gong, and, earning, petrarch, micha, annexes,\n",
      "Nearest to which: it, this, also, that, mollusca, sanderson, pikemen, newborn,\n",
      "Nearest to eight: nine, six, five, seven, four, three, zero, two,\n",
      "Nearest to such: well, awful, lamenting, evaporation, dumbarton, doth, abbotsford, hryniewiecki,\n",
      "Nearest to other: rts, some, qualify, alcibiades, bathing, abt, icao, nitric,\n",
      "Nearest to it: he, this, which, there, they, that, she, natives,\n",
      "Nearest to on: in, voortrekker, of, cannula, osterreich, flynn, after, albury,\n",
      "Nearest to th: six, four, paterfamilias, micrococcus, hebrides, architectures, five, three,\n",
      "Nearest to has: is, have, had, was, stenosis, beach, satisfy, mg,\n",
      "Nearest to united: remaining, bardot, affection, bats, repository, intertwined, bulldog, aulus,\n",
      "Nearest to at: in, gabriel, of, restaurateur, yamaha, weak, aulus, by,\n",
      "Nearest to use: neurobiological, bundles, folks, federals, deuterocanon, nth, enclosing, jambalaya,\n",
      "Nearest to states: ferrero, stamps, subsystems, copyrightable, fari, involution, numbered, reasonably,\n",
      "Nearest to in: of, on, at, with, from, after, enum, under,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "#num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  #tf.global_variables_initializer().run()\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_batch(\n",
    "      batch_size, num_skips, skip_window)\n",
    "    if(step == 0):\n",
    "        print('1st Batch Training data  is ',batch_data)\n",
    "        print('1st Batch Training label  is ',batch_data)\n",
    "        print('1st Batch Training dataset size :',len(batch_data), ' Batch Training label size : ',len(batch_labels)  )\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step %d: %f' % (step, average_loss))\n",
    "      average_loss = 0\n",
    "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "        print(log)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Word Vector Output\n",
    "----------------------\n",
    "**final_embeddings :** Final Word Vector Output is stored in the final_embeddings vector.\n",
    "**[reverse_dictionary[i]** :  Labels of the word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of the word label and vector\n",
      "{'fawn': array([-0.13989708,  0.12158715,  0.13733332, -0.12666136, -0.11127454,\n",
      "        0.03098324,  0.08410942, -0.12983011, -0.02694082,  0.005929  ,\n",
      "       -0.04977843,  0.04833758, -0.11463884,  0.11959696,  0.13467336,\n",
      "        0.11713647,  0.02579618, -0.0366492 ,  0.10889158, -0.01245765,\n",
      "        0.04406659, -0.0419971 , -0.1110612 ,  0.10188457, -0.09823871,\n",
      "        0.13638397, -0.01920055,  0.09617165, -0.00555415,  0.13425246,\n",
      "        0.09791777, -0.09968832,  0.00692748, -0.10811123,  0.10371564,\n",
      "        0.13871436, -0.03038054, -0.09540132,  0.11353511, -0.12389319,\n",
      "        0.07030345, -0.11490811, -0.01584335,  0.05678115,  0.09257349,\n",
      "       -0.13135044, -0.09739818, -0.11123001,  0.06076265, -0.13553317,\n",
      "       -0.09210048, -0.0321669 ,  0.04128449, -0.12536123, -0.09572752,\n",
      "        0.02859585, -0.03789193, -0.03738993,  0.06962638, -0.11181191,\n",
      "        0.04655303,  0.04477959,  0.12777688,  0.07807186,  0.08800848,\n",
      "        0.0586958 ,  0.09420969,  0.00867922, -0.13041022,  0.07351997,\n",
      "        0.10306472, -0.07963784,  0.05037851, -0.11393907,  0.11446793,\n",
      "        0.07506577,  0.11805498, -0.10873818,  0.11264878, -0.1050165 ,\n",
      "       -0.08130777, -0.01495591, -0.11031959,  0.00089132,  0.10251486,\n",
      "        0.04591155, -0.0904774 , -0.09754318,  0.05416813, -0.10894904,\n",
      "        0.04823742, -0.07081313, -0.06950017,  0.08907815,  0.12481486,\n",
      "        0.10290307, -0.04673168,  0.11254631, -0.0591972 , -0.03721593,\n",
      "       -0.09739249,  0.01844277,  0.12129709, -0.00178964, -0.03690149,\n",
      "        0.13835193,  0.09301066, -0.02156322,  0.10716198, -0.09001669,\n",
      "        0.01139772,  0.13564679, -0.05655568,  0.00719641,  0.0729866 ,\n",
      "        0.10979824, -0.05320119, -0.04526418, -0.07087109,  0.12258962,\n",
      "       -0.02484216, -0.06408707, -0.06543987, -0.0284762 ,  0.13458759,\n",
      "       -0.1123043 ,  0.04072211,  0.06481283], dtype=float32), 'homomorphism': array([-0.02184212, -0.07355748, -0.14125969,  0.06194997, -0.12639123,\n",
      "        0.03342163, -0.09902971,  0.02557083, -0.14277916, -0.02439911,\n",
      "        0.09116703,  0.01940138, -0.05549613, -0.07877605, -0.05807556,\n",
      "        0.06610165, -0.13354658, -0.03337768, -0.1026775 , -0.04390072,\n",
      "       -0.00922067,  0.08201311, -0.1124615 , -0.04616168,  0.00999242,\n",
      "        0.14927025,  0.12556517, -0.1495304 ,  0.023636  ,  0.13653716,\n",
      "        0.0134901 ,  0.1314068 ,  0.00876435,  0.07668602,  0.00532961,\n",
      "        0.13933018,  0.0978919 , -0.04557332, -0.07965325, -0.07804377,\n",
      "        0.1237837 , -0.04161897,  0.01567119, -0.08909718,  0.03149448,\n",
      "       -0.13094684,  0.13903199,  0.16035782,  0.01801015,  0.02704784,\n",
      "        0.14689767,  0.06596413, -0.06931116, -0.05920788, -0.00382845,\n",
      "        0.01921143,  0.09576394, -0.05692961,  0.09100937,  0.05720002,\n",
      "       -0.15978526, -0.00969416, -0.08320324, -0.09250456,  0.00161394,\n",
      "        0.00170389, -0.13401246,  0.04676518, -0.09658314,  0.07111567,\n",
      "        0.12014564, -0.06769293, -0.06758823,  0.04524526, -0.05703522,\n",
      "       -0.00356253, -0.0911679 , -0.02805184,  0.05802556, -0.06527642,\n",
      "       -0.08915341,  0.12539996, -0.01459981,  0.12595575, -0.02334374,\n",
      "        0.14226548,  0.14516206, -0.05179225, -0.0391315 , -0.15974228,\n",
      "       -0.01797069, -0.14637572, -0.04322007,  0.01468962,  0.08282196,\n",
      "        0.0722916 , -0.08923478, -0.03192959, -0.09589671,  0.12077227,\n",
      "        0.04332045, -0.05076648,  0.11590667, -0.10417005,  0.03497273,\n",
      "       -0.02764423, -0.02697729, -0.02115997,  0.04070961, -0.12860154,\n",
      "        0.11592015,  0.05058748,  0.09940331,  0.10676359, -0.16093512,\n",
      "       -0.02004041, -0.07379418,  0.07787828,  0.14432076,  0.00347803,\n",
      "       -0.13431761, -0.13039564, -0.09250935,  0.12569764,  0.11369206,\n",
      "       -0.06998812, -0.15151179,  0.02905099], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "#print(reverse_dictionary[1] )\n",
    "#print(final_embeddings[1])\n",
    "word_n_its_vector = { reverse_dictionary[i] :final_embeddings[i]  for i in range(len(reverse_dictionary)) } \n",
    "print( 'Sample of the word label and vector' )\n",
    "print( { key : word_n_its_vector[key] for key in  word_n_its_vector.keys()[:2] } )\n",
    "#print( word_n_its_vector[2] )        \n",
    "#print( [word_n_its_vector[i] for i in range(3,5)] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Vector Simialrity between words :\n",
    "-----------------------\n",
    "**Case 1 :** he / she\n",
    "\n",
    "**Case 2 :** he / originated\n",
    "\n",
    "**Case 3 :** they / that    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine distance similarity between  he /she is  0.515669968902\n",
      "cosine distance similarity between  he /originated is  -0.0176419312914\n",
      "cosine distance similarity between  they vs that is  0.278272049324\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "#print(type(word_n_its_vector['he']))\n",
    "#print(word_n_its_vector['the'])\n",
    "print('cosine distance similarity between  he /she is ',\\\n",
    "      1 - spatial.distance.cosine( word_n_its_vector['he'], word_n_its_vector['she']))\n",
    "print('cosine distance similarity between  he /originated is ',\\\n",
    "      1 - spatial.distance.cosine( word_n_its_vector['he'], word_n_its_vector['originated']))\n",
    "print('cosine distance similarity between  they vs that is ',\\\n",
    "      1 - spatial.distance.cosine( word_n_its_vector['they'], word_n_its_vector['that']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "--------------------\n",
    "**\"He vs She\"** are very similar compared to  **\"He vs Originated\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "jjJXYA_XzV79"
   },
   "outputs": [],
   "source": [
    "num_points = 400\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "128\n",
      "[-0.10473408 -0.04715362 -0.06638023  0.05636352  0.22023238  0.10935123\n",
      "  0.21247007 -0.00649589 -0.0273759  -0.00662011 -0.11963457 -0.06362706\n",
      "  0.20612746 -0.00543853 -0.04609589  0.01948957 -0.19230266  0.06368847\n",
      "  0.08081017  0.01458507 -0.01219707  0.1040906  -0.00628894  0.00685745\n",
      " -0.02352546 -0.15802979  0.03076531 -0.0181194   0.11044451 -0.02193434\n",
      " -0.02483573 -0.16204436  0.0075853   0.02824622  0.05256828  0.11274089\n",
      "  0.00669346  0.023488    0.04508581 -0.08424472 -0.07304773 -0.01650908\n",
      " -0.16038799  0.14383519  0.12079301  0.08757117  0.04898544  0.07272018\n",
      "  0.01530478  0.04640829  0.01883251  0.19935384  0.10565738  0.00942819\n",
      "  0.03475559  0.03978069 -0.07929466  0.0100785  -0.00998897  0.085611\n",
      "  0.02543803  0.13312721 -0.10454167 -0.12847829 -0.029019    0.08624271\n",
      "  0.06861328 -0.08197025  0.00885739 -0.04062694  0.00100554  0.06103877\n",
      "  0.00572084  0.1667206  -0.02013417 -0.28820726 -0.04972165 -0.09361\n",
      "  0.05357289  0.05451857  0.13149372  0.03949748 -0.15703164 -0.0723285\n",
      " -0.00147067  0.12213926 -0.0276043  -0.08256014  0.01626789  0.05322772\n",
      "  0.06754996  0.02594166  0.03030778  0.03873729  0.07749521 -0.14676248\n",
      " -0.10862564 -0.04735119  0.01216103  0.06546962  0.00121052  0.16507988\n",
      "  0.04339518 -0.05488282 -0.03060646 -0.08300594 -0.03252016 -0.09611195\n",
      " -0.02554107 -0.031703   -0.00408331 -0.1011001   0.00989715 -0.06574171\n",
      " -0.12321451  0.00403552 -0.04207466 -0.06798863 -0.07056259 -0.08985683\n",
      " -0.07694877  0.06849953  0.16661434 -0.1139519  -0.05977999  0.11294569\n",
      "  0.0795184   0.05681736]\n",
      "UNK [-0.10473408 -0.04715362 -0.06638023  0.05636352  0.22023238  0.10935123\n",
      "  0.21247007 -0.00649589 -0.0273759  -0.00662011 -0.11963457 -0.06362706\n",
      "  0.20612746 -0.00543853 -0.04609589  0.01948957 -0.19230266  0.06368847\n",
      "  0.08081017  0.01458507 -0.01219707  0.1040906  -0.00628894  0.00685745\n",
      " -0.02352546 -0.15802979  0.03076531 -0.0181194   0.11044451 -0.02193434\n",
      " -0.02483573 -0.16204436  0.0075853   0.02824622  0.05256828  0.11274089\n",
      "  0.00669346  0.023488    0.04508581 -0.08424472 -0.07304773 -0.01650908\n",
      " -0.16038799  0.14383519  0.12079301  0.08757117  0.04898544  0.07272018\n",
      "  0.01530478  0.04640829  0.01883251  0.19935384  0.10565738  0.00942819\n",
      "  0.03475559  0.03978069 -0.07929466  0.0100785  -0.00998897  0.085611\n",
      "  0.02543803  0.13312721 -0.10454167 -0.12847829 -0.029019    0.08624271\n",
      "  0.06861328 -0.08197025  0.00885739 -0.04062694  0.00100554  0.06103877\n",
      "  0.00572084  0.1667206  -0.02013417 -0.28820726 -0.04972165 -0.09361\n",
      "  0.05357289  0.05451857  0.13149372  0.03949748 -0.15703164 -0.0723285\n",
      " -0.00147067  0.12213926 -0.0276043  -0.08256014  0.01626789  0.05322772\n",
      "  0.06754996  0.02594166  0.03030778  0.03873729  0.07749521 -0.14676248\n",
      " -0.10862564 -0.04735119  0.01216103  0.06546962  0.00121052  0.16507988\n",
      "  0.04339518 -0.05488282 -0.03060646 -0.08300594 -0.03252016 -0.09611195\n",
      " -0.02554107 -0.031703   -0.00408331 -0.1011001   0.00989715 -0.06574171\n",
      " -0.12321451  0.00403552 -0.04207466 -0.06798863 -0.07056259 -0.08985683\n",
      " -0.07694877  0.06849953  0.16661434 -0.1139519  -0.05977999  0.11294569\n",
      "  0.0795184   0.05681736]\n"
     ]
    }
   ],
   "source": [
    "print(len(final_embeddings))\n",
    "print(len(final_embeddings[0]))\n",
    "print(final_embeddings[0])\n",
    "#words = [reverse_dictionary[i] for i in range(1, num_points+1)]\n",
    "print(reverse_dictionary[0], final_embeddings[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 4763,
     "status": "ok",
     "timestamp": 1445965465525,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "o_e0D_UezcDe",
    "outputId": "df22e4a5-e8ec-4e5e-d384-c6cf37c68c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "[ 0.04228502 -0.12075232 -0.11376955 -0.09859844  0.03488731  0.04238981\n",
      " -0.08082084  0.07563843 -0.33936015 -0.00882445  0.01562099  0.01937421\n",
      "  0.1458983   0.10876398  0.05715398 -0.07455908  0.00659716 -0.00160836\n",
      " -0.036729   -0.01661832  0.0181931   0.05461675 -0.05124519 -0.01658517\n",
      " -0.08221611 -0.07063289  0.16708677 -0.00475863  0.05763826  0.02416596\n",
      " -0.0887311   0.0588704  -0.05295033 -0.09269209 -0.0109619  -0.00141115\n",
      "  0.01545913 -0.03750875  0.05250677 -0.02560061 -0.0290512   0.12946072\n",
      " -0.08673239  0.07256972 -0.13782959 -0.07461096  0.07296053 -0.10163458\n",
      " -0.02597805 -0.08332466  0.03471394  0.08870099  0.03882752  0.01831625\n",
      " -0.17916873  0.11881664  0.06109766 -0.14970326 -0.11096389  0.02409375\n",
      " -0.11245652  0.13605402  0.07440074 -0.04972687  0.04708791 -0.14046784\n",
      "  0.0202703  -0.01933543  0.04447813 -0.17779694  0.106643    0.11037834\n",
      " -0.0594922   0.0245506   0.06362081  0.08165073  0.09565093 -0.08075899\n",
      " -0.03995282  0.01146993 -0.04160945 -0.09428892  0.01106939  0.00256032\n",
      "  0.06216286 -0.01744408  0.04836453 -0.03331359 -0.04127354 -0.00572818\n",
      "  0.00050832 -0.02268343  0.11817434 -0.1048055   0.19980778 -0.05664511\n",
      "  0.12721525  0.00078715  0.03528695 -0.0422302  -0.03626233  0.13920677\n",
      " -0.04958793  0.05306383  0.09574497  0.03090445 -0.1003357  -0.01352941\n",
      " -0.01103218  0.05825115 -0.11587333  0.09777652 -0.14304744 -0.072419\n",
      "  0.17400032 -0.05730321  0.16309902  0.07639902  0.03595328 -0.06322987\n",
      " -0.00366211 -0.05458682  0.07301008 -0.0569562  -0.16528192  0.2565755\n",
      "  0.0180415   0.00208413]\n"
     ]
    }
   ],
   "source": [
    "def plot(embeddings, labels):\n",
    "  assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  pylab.figure(figsize=(15,15))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = embeddings[i,:]\n",
    "    pylab.scatter(x, y)\n",
    "    pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                   ha='right', va='bottom')\n",
    "  pylab.show()\n",
    "\n",
    "words = [reverse_dictionary[i] for i in range(1, num_points+1)]\n",
    "plot(two_d_embeddings, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion \n",
    "------------------------\n",
    "From  the above skip-gram model, we are able to train a model that given a text can automatically find the words similar to each other with skip-gram models.\n",
    "\n",
    "In the process they are able to capture the relationshiop between words i.e. Dog, cat are alike (e.g such that they are both animals, four-legged, pets, etc).Also they are able to gather the semantic  relationship i.e Kitten is to cat what puppy is to dog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB5EFrBnpNnc"
   },
   "source": [
    "---\n",
    "\n",
    "Continous Bag of Words\n",
    "-------\n",
    "\n",
    "An alternative to skip-gram is another Word2Vec model called [CBOW](http://arxiv.org/abs/1301.3781) (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. We Implement and evaluate a CBOW model trained on the text8 dataset in the next project.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "5_word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
