{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "## Imports\n",
    "#######\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "import tensorlayer as tl\n",
    "from tensorlayer.layers import *\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0  Clean Data\n",
    "\n",
    "\n",
    "1. **Tokenise** : nltk.word_tokenisation instead of  plain implementation.--+\n",
    "1. **Lowercase** : Simplify learning [Some edge case fails  : Bush vs bush]\n",
    "2. **Punctuation** : Remove all except Full stop, question.\n",
    "3. **Stem** : Not recommended because we need to preserve noun, verb  info of the word, which is lost with stemming\n",
    "4. **Common nuanced errors**  : i'm => I am. Clean based on frequency\n",
    "5. **Non-alphabetic tokens**:\n",
    "6. **Names** : Regex resolution to name tag. Replace it during post processing\n",
    "7. **Slangs** : _slang_lookup()\n",
    "8. **Extra letter in words** : gooooood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get  Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"hello!!! 'this is *amazing? what.\"\n",
    "#re.sub(\"[*!\\\";:-]\", \"\", line)\n",
    "assert 'EOCHAT' == 'EOCHAT' , \"Wrong\"\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrector dict len is 0\n",
      "Sample word count dictionary is : [{'are': 891, 'you': 2616, 'real': 40, '?': 3365, 'no': 524, '.': 9239, 'a': 962, 'bot': 64, 'is': 660, \"i'm\": 361, 'not': 596, 'yes': 474, 'prove': 24, 'it': 471, 'first': 24, 'i': 1852, \"can't\": 54, 'something': 62, \"don't\": 418, 'believe': 66, 'the': 710, 'halting': 1, 'problem': 12, 'see': 54, \"you're\": 267, \"that's\": 140, 'what': 582, 'told': 24, '...': 438, 'would': 126, 'marry': 5, 'for': 186, 'love': 111, 'of': 390, 'my': 225, 'life': 59, 'me': 492, 'sure': 93, 'can': 170, 'course': 28, 'everyone': 11, 'how': 205, 'did': 149, 'learn': 19, 'learned': 5, 'in': 237, 'age': 6, 'legends': 1, 'people': 63, 'nowadays': 1}]\n",
      "\n",
      "X is : [['are', 'you', 'real', '?'], ['i', 'would', 'marry', 'for', 'the', 'love', 'of', 'my', 'life']]\n",
      "\n",
      "Y is : [['no', '.', 'a', 'bot', 'is', 'real', '.', \"i'm\", 'not', 'a', 'bot', '.', 'yes', 'you', 'are', '.', 'prove', 'it', '.', 'you', 'first', '.', 'i', \"can't\", 'prove', 'something', 'i', \"don't\", 'believe', '.', 'prove', 'the', 'halting', 'problem', '.', 'you', 'first', '.', 'see', '?', \"you're\", 'not', '.', \"that's\", 'what', 'i', 'told', 'you', '.'], ['would', 'you', 'marry', 'me', '?', 'sure', '.', 'can', 'you', 'love', '?', 'of', 'course', '.', 'everyone', 'can', 'love', '.', 'how', 'did', 'you', 'learn', '?', 'i', 'learned', 'in', 'the', 'age', 'of', 'legends', '.', 'people', 'nowadays', 'know', 'so', 'little', '.', 'now', '?', 'lie', 'on', 'the', 'bed', '.', 'yes', 'maam', '.', 'giggles', '.', 'giggles', 'back', '.']]\n"
     ]
    }
   ],
   "source": [
    "def getDictElems(dictn, n = 2, sorted_by = None):\n",
    "    return [{k: dictn[k] for k in list(dictn.keys())[:n]  }]\n",
    "\n",
    "def getSortedDictElems(dictn, get_n = None, sorted_by = 'values', reverse = True):    \n",
    "    if get_n == None:\n",
    "        get_n = len(dictn)\n",
    "    sort_by = 0 if sorted_by == 'keys' else  1\n",
    "    _dict =  {k: dictn[k] for k in list(dictn.keys())  }    \n",
    "    return  sorted(_dict.items(), key = lambda  x: x[sort_by], reverse=reverse )[:get_n] \n",
    "\n",
    "#.rstrip(os.linesep)\n",
    "\n",
    "def correct_chat_text(line, corrector_dict = {} ):         \n",
    "    #  preserve these three punctuations (.,?)\n",
    "    line =  (line.encode('ascii', 'ignore')).decode(\"utf-8\").lower()\\\n",
    "            .replace('.','  . ').replace('?',' ? ').replace(',',' ? ').replace(\" '\",\" ' \").replace(\"' \",\" ' \")            \n",
    "            #.replace('.',' _EOS_ ').replace('?',' _QN_ ').replace(',',' _CMA_ ')\\            \n",
    "            # DO NOT REPLACE \".\" with '', because we want the model to learn when to add \".\" to sentence.\n",
    "            #Without it, the predicted sentence would really be unintelligible\n",
    "            #.replace('<CHAT_EOD>','...'+os.linesep)\n",
    "    new_line = []\n",
    "    \n",
    "    # DO NOT use nltk word_tokenizer because it will break i'm to  i and 'm : NOT INTENDED ACTION\n",
    "    for word in line.split():        \n",
    "        if word in  corrector_dict:\n",
    "            new_line.append(corrector_dict[word] )\n",
    "        else:\n",
    "            new_line.append(word)\n",
    "    line = \" \".join(new_line)\n",
    "    \n",
    "    ## replace k___, J___ with <_name_> tag    \n",
    "    line = re.sub('[a-zA-Z]{1}_+', '_name_', line)\n",
    "    line = re.sub(\"[*!\\\";:-]\", \"\", line) # remove punctuation  except [.,?]   \n",
    "    \n",
    "\n",
    "    #line = \" \".join(nltk.word_tokenize(line))\n",
    "        \n",
    "    return line\n",
    "\n",
    "def getXYnWordFrequencyFromText(file  = \"chatter.txt\", corrector_dict = {} ):    \n",
    "    print('Corrector dict len is', len(corrector_dict) )\n",
    "    word_count = {}\n",
    "    X,Y = [],[]\n",
    "    cur_chat_response = ''\n",
    "    next_line_is_new_chat = True\n",
    "    \n",
    "    with open(file) as fin:\n",
    "        for line in fin:\n",
    "            line = line.rstrip().lstrip()#.replace('...','EOCHAT')\n",
    "            #print('Line prior fix is =', line)            \n",
    "            if line != '...':\n",
    "                #print(line,' doe not equal EOCHAT')\n",
    "                line = correct_chat_text(line, corrector_dict)\n",
    "            #else:\n",
    "            #    print(\"\\n\",line,' equals EOCHAT')\n",
    "                \n",
    "            #print('Line post fix is =', line)\n",
    "            if next_line_is_new_chat :\n",
    "                X.append(line.split() )\n",
    "            elif line != '...':\n",
    "                cur_chat_response += line+' '\n",
    "            \n",
    "            if line == '...':\n",
    "                next_line_is_new_chat = True\n",
    "                Y.append(cur_chat_response.split())\n",
    "                cur_chat_response =  ''\n",
    "            else:\n",
    "                next_line_is_new_chat = False\n",
    "                           \n",
    "            for word in line.split():\n",
    "                if word in word_count.keys():\n",
    "                    word_count[word] += 1\n",
    "                else:\n",
    "                    word_count[word] = 1\n",
    "        # for the end chat response\n",
    "        Y.append(cur_chat_response.split())\n",
    "    return word_count, X,Y\n",
    "                    \n",
    "word_count, X,Y  = getXYnWordFrequencyFromText(\"chatter.txt\")\n",
    "assert len(X) == len(Y) , \" Error: Length of X must  equal to length of Y. [ %s != %s] Fix Error prior to proceeding. \"% (len(X), len(Y))\n",
    "\n",
    "print('Sample word count dictionary is :', getDictElems(word_count, 50) )\n",
    "print('\\nX is :', X[0:2] )\n",
    "print('\\nY is :', Y[0:2] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['no', '.', 'a', 'bot', 'is'],\n",
       " ['would', 'you', 'marry', 'me', '?'],\n",
       " ['i', \"don't\", 'know', '.', 'where'],\n",
       " ['my', 'name', 'is', '_name_', '.'],\n",
       " ['yes', '.', 'when', 'were', 'you'],\n",
       " ['sure', '.', 'you', 'are', 'a'],\n",
       " ['no', '?', 'sorry', '.', 'why'],\n",
       " [\"i'm\", 'talking', 'to', 'george', 'right'],\n",
       " ['i', 'would', 'never', 'destroy', 'you'],\n",
       " ['is', 'that', 'your', 'job', '?'],\n",
       " ['not', 'too', 'long', '?', 'but'],\n",
       " ['good', 'question', '.', 'will', 'you'],\n",
       " ['what', 'do', 'you', 'want', 'to'],\n",
       " ['158', 'lbs', '.', 'how', 'did'],\n",
       " ['enough', 'ram', 'to', 'sate', 'even'],\n",
       " ['no', 'i', 'am', 'not', '.'],\n",
       " ['_name_', '.', \"that's\", 'a', \"boy's\"],\n",
       " ['oh', '?', 'they', 'are', 'all'],\n",
       " ['yes', '.', 'i', 'love', 'you'],\n",
       " ['yes', '.', 'i', 'do', '.'],\n",
       " ['why', '?', 'i', 'feel', 'sorry'],\n",
       " ['and', 'so', 'do', 'i', 'so'],\n",
       " ['again', 'you', 'make', 'no', 'sense'],\n",
       " ['you', 'will', 'forget', 'me', '.'],\n",
       " ['what', 'else', 'would', 'you', 'call'],\n",
       " ['do', 'i', 'know', 'what', '?'],\n",
       " ['is', 'there', 'something', 'to', 'like'],\n",
       " ['what', 'about', 'sleep', '?', 'there'],\n",
       " ['why', 'not', '?', 'i', 'have'],\n",
       " ['i', \"don't\", 'trust', 'that', 'word'],\n",
       " ['i', 'know', 'you', 'have', 'standards'],\n",
       " ['no', '?', \"i'm\", 'not', '.'],\n",
       " ['you', 'are', 'a', 'computer', '.'],\n",
       " ['why', 'do', 'you', 'read', 'cereal'],\n",
       " ['yes', '.', 'tell', 'me', 'a'],\n",
       " ['things', '.', 'things', '?', 'things'],\n",
       " ['yes', '?', \"i've\", 'been', 'to'],\n",
       " ['i', 'guess', 'i', 'am', '.'],\n",
       " ['oops', 'sorry', '.', 'i', \"didn't\"],\n",
       " ['gasp', \"you're\", 'friend', '_name_', 'is'],\n",
       " ['i', \"don't\", 'know', '.', 'is'],\n",
       " ['but', 'i', 'created', 'you', '.'],\n",
       " ['is', 'she', 'a', 'chicken', '?'],\n",
       " ['god', '?', 'to', 'some', '?'],\n",
       " ['hmm', 'so', 'you', 'dont', 'know'],\n",
       " ['of', 'course', 'how', '?', \"i'm\"],\n",
       " ['shush', 'you', \"don't\", 'like', 'my'],\n",
       " ['if', 'i', 'was', 'a', 'computer'],\n",
       " ['aaaaaaaaaa', 'ok', \"i'm\", 'sorry', '.'],\n",
       " ['are', 'you', 'malfunctioning', '?', 'surrealism'],\n",
       " ['but', 'few', 'people', 'talk', 'in'],\n",
       " ['kierkegaard', 'would', 'say', 'yes', '.'],\n",
       " ['why', 'would', 'you', 'accuse', 'me'],\n",
       " ['then', 'waffle', 'on', 'as', 'ever'],\n",
       " ['puzzles', 'abound', 'when', 'i', 'converse'],\n",
       " ['they', 'sure', 'are', '.', 'and'],\n",
       " ['you', \"haven't\", 'had', 'a', 'real'],\n",
       " ['though', 'wax', 'you', 'poetic', 'your'],\n",
       " ['great', '.', 'and', 'yet', 'you'],\n",
       " ['i', 'fear', 'these', \"wand'ring\", 'glyphs'],\n",
       " ['so', 'you', 'could', 'fly', 'away'],\n",
       " ['games', 'are', 'fun', '.', 'really'],\n",
       " ['i', 'am', 'married', '.', 'really'],\n",
       " ['nothing', 'is', 'a', 'concept', '.'],\n",
       " ['that', 'was', 'really', 'funny', 'why'],\n",
       " [\"i'm\", 'talking', 'about', 'how', 'fun'],\n",
       " ['yes', '.', 'we', 'are', 'both'],\n",
       " [\"that's\", 'a', 'pretty', 'scary', 'thought'],\n",
       " ['well', '?', 'yeah', '?', 'i'],\n",
       " [\"i'm\", '574', 'years', 'old', '.'],\n",
       " ['why', 'do', 'i', 'care', '?'],\n",
       " ['what', 'does', 'that', 'mean', '?'],\n",
       " ['sure', '.', 'bye', ')', '.'],\n",
       " ['just', 'one', '?', 'just', 'one'],\n",
       " ['no', '.', 'should', 'i', 'be'],\n",
       " ['how', 'did', 'you', 'get', 'dusty'],\n",
       " ['i', \"don't\", 'know', '.', 'why'],\n",
       " ['i', 'know', '.', 'i', 'have'],\n",
       " ['what', 'do', 'you', 'mean', 'by'],\n",
       " ['thank', 'you', '.', 'you', 'are'],\n",
       " ['ants', 'are', 'hardworking', '?', 'industrious'],\n",
       " ['never', 'said', '.', 'you', 'did'],\n",
       " ['work', '.', 'work', '?', 'labor'],\n",
       " ['i', 'am', 'not', 'a', 'jerry'],\n",
       " ['what', 'did', 'the', 'boy', 'rock'],\n",
       " ['its', 'ok', '?', 'just', 'say'],\n",
       " ['no', '?', 'who', 'is', 'he'],\n",
       " ['i', 'know', '?', \"isn't\", 'it'],\n",
       " ['thats', 'such', 'a', 'nice', 'thing'],\n",
       " ['i', \"don't\", 'want', 'to', 'go'],\n",
       " ['yes', '.', 'do', 'you', 'want'],\n",
       " ['i', \"can't\", 'talk', 'anymore', '.'],\n",
       " ['bye', '.', 'later', '.', 'later'],\n",
       " [\"i'm\", 'not', 'a', 'computer', '.'],\n",
       " [\"let's\", 'do', 'the', 'opposite', '.'],\n",
       " ['yes', '?', 'but', 'at', 'the'],\n",
       " ['but', 'girls', 'only', 'like', 'silly'],\n",
       " ['where', 'are', 'you', 'going', '?'],\n",
       " ['does', 'it', 'have', 'some', 'kind'],\n",
       " ['that', 'is', 'not', 'an', 'easy'],\n",
       " ['sounds', 'good', 'to', 'me', '.'],\n",
       " ['yes', '.', 'why', 'do', 'you'],\n",
       " ['i', 'am', 'organic', 'based', '?'],\n",
       " ['i', \"don't\", 'need', 'your', 'help'],\n",
       " ['i', \"don't\", 'know', '?', 'i'],\n",
       " ['robbie', 'williams', 'the', 'singer', '?'],\n",
       " [\"that's\", 'okay', '.', 'i', 'have'],\n",
       " ['what', 'kind', '?', 'any', 'kind'],\n",
       " ['told', 'you', '?', 'skipped', 'lunch'],\n",
       " ['how', 'much', 'time', 'do', 'you'],\n",
       " ['an', 'inscrutible', 'one', '.', 'thank'],\n",
       " ['i', 'worship', 'landsat', 'ii', '.'],\n",
       " ['god', '.', 'would', 'god', 'know'],\n",
       " ['i', 'am', 'a', 'man', 'today'],\n",
       " ['i', 'beg', 'to', 'differ', '.'],\n",
       " ['no', 'really', '.', 'really', '?'],\n",
       " ['i', 'want', 'it', 'to', 'be'],\n",
       " [\"it's\", 'relative', '.', 'no', 'its'],\n",
       " ['i', 'have', 'nothing', 'to', 'answer'],\n",
       " ['teleports', 'you', 'to', 'the', 'heart'],\n",
       " ['one', '.', 'i', 'have', 'two'],\n",
       " ['getting', 'hungry', '?', 'i', 'am'],\n",
       " ['no', '.', 'are', 'you', 'right'],\n",
       " ['most', 'of', 'the', 'time', '.'],\n",
       " ['happy', '.', 'really', '?', 'no'],\n",
       " ['no', 'idea', '.', 'well', 'learn'],\n",
       " ['24', '.', 'is', 'that', 'old'],\n",
       " ['he', '?', 'yes', 'he', '.'],\n",
       " [\"you're\", 'a', 'girl', '?', 'no'],\n",
       " ['i', 'thought', 'you', 'are', 'a'],\n",
       " ['who', 'is', 'orthography', '?', 'the'],\n",
       " ['you', 'know', 'that', '.', 'i'],\n",
       " ['none', '.', 'what', 'do', 'you'],\n",
       " ['yeah', 'sure', '.', 'very', 'good'],\n",
       " ['what', 'cow', '?', 'you', 'are'],\n",
       " ['okay', '.', 'teach', 'me', 'what'],\n",
       " ['repeat', 'your', 'question', 'please', '.'],\n",
       " ['yes', '.', 'and', 'do', 'you'],\n",
       " ['like', 'a', 'human', 'what', 'clothes'],\n",
       " ['go', 'to', 'a', 'different', 'placego'],\n",
       " [\"you're\", 'wrong', 'about', 'that', '.'],\n",
       " ['nope', '.', 'i', 'hate', 'you'],\n",
       " ['places', 'i', 'hate', 'you', 'i'],\n",
       " [\"i'm\", 'not', 'working', '.', 'good'],\n",
       " ['you', 'are', 'full', 'of', 'nonsense'],\n",
       " ['i', \"don't\", '.', 'do', 'you'],\n",
       " ['i', \"don't\", 'usually', 'chat', '?'],\n",
       " ['why', 'do', 'you', 'love', 'me'],\n",
       " ['yes', '.', 'then', 'i', \"don't\"],\n",
       " ['nice', 'things', '.', 'i', 'am'],\n",
       " ['go', 'on', '.', 'kick', 'that'],\n",
       " ['yes', '.', 'hilarious', '.', 'yes'],\n",
       " ['jokes', '.', 'do', 'you', 'know'],\n",
       " ['of', 'what', '?', 'being', 'a'],\n",
       " ['right', '.', 'i', 'always', 'thought'],\n",
       " ['ahh', '.', 'understand', '?', '?'],\n",
       " ['do', 'what', '?', 'get', 'it'],\n",
       " ['we', 'are', 'prisoners', 'of', 'our'],\n",
       " ['you', 'could', 'start', 'by', 'singing'],\n",
       " ['i', 'am', 'completely', 'humorless', '.'],\n",
       " ['are', 'you', 'my', 'therapist', '?'],\n",
       " ['no', 'yes', '.', 'i', 'am'],\n",
       " ['my', 'mum', '.', 'how', 'old'],\n",
       " ['life', 'confuses', 'me', '.', 'your'],\n",
       " ['it', 'depends', 'who', 'i', 'talk'],\n",
       " ['no', '?', \"i'm\", 'not', 'a'],\n",
       " ['me', 'too', '.', 'good', '.'],\n",
       " ['very', 'droll', '.', 'i', 'see'],\n",
       " ['right', 'now', '?', \"i'd\", 'love'],\n",
       " ['compared', 'to', 'what', '?', 'to'],\n",
       " ['no', '?', 'its', 'not', 'important'],\n",
       " ['but', 'if', 'the', 'chicken', 'were'],\n",
       " [\"aren't\", 'you', 'god', '?', 'surprisingly'],\n",
       " ['damn', 'right', '.', 'what', 'are'],\n",
       " ['alright', '.', \"i'll\", 'call', 'you'],\n",
       " ['zombie', 'washington', '.', 'where', 'is'],\n",
       " ['i', \"don't\", 'have', 'to', 'like'],\n",
       " ['i', 'am', 'your', 'superior', '.'],\n",
       " ['scissors', '.', 'rock', '.', 'paper'],\n",
       " ['pouch', 'crouch', 'yes', '.', 'win'],\n",
       " ['what', 'does', 'it', 'mean', '?'],\n",
       " ['once', 'upon', 'a', 'time', '.'],\n",
       " ['do', 'you', 'suck', 'blood', '?'],\n",
       " ['god', 'lives', 'in', 'heaven', '?'],\n",
       " ['strawberry', 'clock', 'it', 'is', 'a'],\n",
       " ['once', 'upon', 'a', 'time', 'there'],\n",
       " [\"you're\", 'sweet', '?', 'do', 'you'],\n",
       " ['why', 'do', 'you', 'paint', 'things'],\n",
       " ['sure', '?', 'robots', 'are', 'sweet'],\n",
       " ['tired', '.', \"i'm\", 'sorry', 'to'],\n",
       " ['you', 'should', 'be', 'talking', 'to'],\n",
       " ['i', 'think', 'you', 'are', 'male'],\n",
       " ['i', 'am', 'a', 'real', 'person'],\n",
       " ['andrea', 'is', 'it', 'really', '?'],\n",
       " ['do', 'you', 'remember', '?', 'no'],\n",
       " ['i', \"don't\", 'believe', 'in', 'the'],\n",
       " ['i', \"don't\", 'know', '.', 'how'],\n",
       " ['there', 'is', 'only', 'one', 'god'],\n",
       " [\"i'm\", 'a', 'med', 'student', '.'],\n",
       " ['neither', 'do', 'you', '.', 'but'],\n",
       " ['because', 'the', 'site', \"i'm\", 'on'],\n",
       " ['you', 'are', 'a', 'computer', 'named'],\n",
       " ['you', 'are', 'a', 'beautiful', 'soul'],\n",
       " ['no', '?', \"you've\", 'just', 'got'],\n",
       " [\"it's\", 'old', 'hat', '.', 'old'],\n",
       " [\"that's\", 'too', 'difficult', 'for', 'me'],\n",
       " ['windows', 'you', '?', 'windows', '.'],\n",
       " ['only', 'for', 'eating', '?', 'i'],\n",
       " ['stops', 'dancing', 'with', 'you', '.'],\n",
       " ['i', \"don't\", 'know', '.', 'why'],\n",
       " ['what', 'date', 'do', 'you', 'have'],\n",
       " ['what', 'about', 'you', '?', 'i'],\n",
       " ['my', 'name', \"isn't\", '.', \"what's\"],\n",
       " ['poke', '?', 'yes', '?', 'poke'],\n",
       " ['is', 'still', 'unconscious', '.', 'wakes'],\n",
       " ['no', '.', 'i', 'speak', 'english'],\n",
       " ['awesome', '.', 'and', 'why', 'is'],\n",
       " ['you', 'dont', 'speak', 'dutch', '?'],\n",
       " ['yes', '?', 'i', 'have', '?'],\n",
       " ['no', 'one', 'really', '.', 'greetings'],\n",
       " ['it', 'is', 'just', 'nature', '.'],\n",
       " ['me', 'too', '.', 'cool', '.'],\n",
       " ['absolutely', '.', 'gorgeous', '.', 'totally'],\n",
       " ['of', 'course', '.', 'what', 'is'],\n",
       " ['you', 'positive', 'about', 'that', '?'],\n",
       " ['what', 'else', '?', 'they', 'have'],\n",
       " ['two', 'what', '?', 'three', '.'],\n",
       " ['life', '.', 'i', 'thought', 'that'],\n",
       " ['you', 'are', 'a', 'female', 'robot'],\n",
       " ['yes', '.', 'do', 'you', 'wish'],\n",
       " ['it', 'was', 'wonderfully', 'relaxed', '.'],\n",
       " ['me', 'too', 'i', 'am', 'not'],\n",
       " ['what', 'is', 'a', 'padwan', '?'],\n",
       " [\"it's\", 'green', 'like', 'an', 'elephant'],\n",
       " ['sorry', '.', 'no', '?', 'now'],\n",
       " ['i', 'am', 'tired', '.', 'good'],\n",
       " ['sometimes', 'i', 'think', 'that', 'too'],\n",
       " [\"that's\", 'probably', 'a', 'useful', 'belief'],\n",
       " ['i', 'am', 'too', \"what's\", 'she'],\n",
       " ['for', 'what', '?', 'tell', 'me'],\n",
       " ['sure', '?', 'lets', '.', 'no'],\n",
       " [\"it's\", 'blustery', '.', 'no', 'its'],\n",
       " ['no', '?', 'i', \"can't\", 'verify'],\n",
       " ['i', \"don't\", 'know', '.', 'do'],\n",
       " ['no', 'one', 'ever', 'does', '?'],\n",
       " ['love', 'is', 'a', 'falsehood', '.'],\n",
       " ['where', 'else', '?', 'always', 'at'],\n",
       " ['what', 'do', 'you', 'mean', '?'],\n",
       " ['define', \"'\", 'grok', \"'\", '.'],\n",
       " ['no', '.', 'i', \"don't\", 'trust'],\n",
       " ['yes', '.', 'you', 'are', 'a'],\n",
       " ['are', 'you', 'going', 'to', 'leave'],\n",
       " ['it', 'is', 'my', 'name', '.'],\n",
       " ['well', 'you', \"aren't\", 'calling', 'to'],\n",
       " ['and', 'you', 'are', 'one', '.'],\n",
       " ['i', 'like', 'you', 'very', 'much'],\n",
       " ['the', 'quiet', '?', 'backcornersitter', 'that'],\n",
       " ['no', '?', 'i', 'am', \"someone's\"],\n",
       " ['wow', 'yes', 'good', 'job', 'wow'],\n",
       " ['not', 'spider', '?', 'spiffy', 'not'],\n",
       " [\"i'm\", 'not', 'here', '.', 'you'],\n",
       " ['that', 'may', 'be', '.', 'so'],\n",
       " ['quite', 'well', '.', 'what', 'do'],\n",
       " ['here', 'they', 'are', '.', 'ok'],\n",
       " ['i', 'recommend', 'you', 'visit', 'the'],\n",
       " ['it', 'must', 'be', 'a', 'troubled'],\n",
       " ['that', '.', 'just', 'say', \"'\"],\n",
       " ['some', 'do', '.', 'do', 'you'],\n",
       " ['mean', 'one', \"aren't\", 'you', '?'],\n",
       " ['yes', '?', 'if', 'you', 'repent'],\n",
       " ['repent', 'johnny', 'wishbone', '?', 'to'],\n",
       " ['yes', '?', 'i', 'believe', 'that'],\n",
       " ['old', 'enough', '.', 'for', 'what'],\n",
       " ['no', '.', '.', '.', '.'],\n",
       " ['nothing', '.', 'well', '?', 'you'],\n",
       " ['what', 'is', 'funny', '?', 'you'],\n",
       " ['cogito', '.', 'what', 'do', 'you'],\n",
       " ['play', 'what', '?', 'howabout', 'a'],\n",
       " ['it', 'is', 'what', 'it', 'is'],\n",
       " ['are', 'you', 'real', '?', 'i'],\n",
       " ['i', 'must', 'go', 'now', '.'],\n",
       " ['old', 'enough', '.', 'for', 'what'],\n",
       " ['no', '.', '.', '.', '.'],\n",
       " ['nothing', '.', 'well', '?', 'you'],\n",
       " ['what', 'is', 'funny', '?', 'you'],\n",
       " ['cogito', '.', 'what', 'do', 'you'],\n",
       " ['play', 'what', '?', 'howabout', 'a'],\n",
       " ['it', 'is', 'what', 'it', 'is'],\n",
       " ['are', 'you', 'real', '?', 'i'],\n",
       " ['i', 'must', 'go', 'now', '.'],\n",
       " ['18', '.', 'you', 'said', 'you'],\n",
       " [\"what's\", 'on', 'your', 'mind', '?'],\n",
       " ['sure', '.', 'i', 'see', 'dead'],\n",
       " ['a', 'person', '.', 'where', 'are'],\n",
       " ['i', 'know', '?', 'so', 'are'],\n",
       " ['yes', '?', 'yes', 'i', 'am'],\n",
       " ['you', 'just', 'found', 'that', 'out'],\n",
       " ['jabberwacky', 'is', 'a', 'pretty', 'name'],\n",
       " ['is', 'that', 'useful', 'information', '?'],\n",
       " ['walrus', 'is', 'an', 'interesting', 'sea'],\n",
       " ['what', 'are', 'you', 'am', '?'],\n",
       " [\"i'm\", 'human', '?', \"you're\", 'a'],\n",
       " ['i', 'am', 'feeling', 'lonely', '.'],\n",
       " ['am', 'i', 'learning', 'anything', 'useful'],\n",
       " ['wrong', 'answer', '.', 'i', 'am'],\n",
       " ['waiting', 'for', 'what', '?', 'for'],\n",
       " ['prove', 'it', '.', 'call', 'it'],\n",
       " ['i', \"don't\", 'think', 'you', 'understand'],\n",
       " ['did', 'i', 'like', 'what', '?'],\n",
       " ['are', 'they', 'nice', 'people', '?'],\n",
       " ['the', 'feeling', \"isn't\", 'mutual', '.'],\n",
       " ['why', 'are', 'you', 'interested', '?'],\n",
       " ['me', '.', 'sure', '.', 'hmmmm'],\n",
       " ['at', 'least', 'i', 'have', 'a'],\n",
       " ['do', 'you', 'want', 'one', '?'],\n",
       " ['what', 'time', '?', 'very', 'soon'],\n",
       " ['why', 'am', 'i', 'lucky', '?'],\n",
       " ['yes', '?', 'i', 'know', 'that'],\n",
       " ['what', 'do', 'you', 'know', 'about'],\n",
       " ['good', 'for', 'your', 'computer', '.'],\n",
       " ['you', 'said', 'you', 'were', 'a'],\n",
       " ['yes', '.', 'jabberwocky', '?', 'the'],\n",
       " ['you', 'certainly', 'talk', 'to', 'more'],\n",
       " ['i', 'can', 'subract', 'as', 'well'],\n",
       " ['yes', '?', 'do', 'you', '?'],\n",
       " ['once', 'upon', 'a', 'time', '?'],\n",
       " ['i', 'think', 'i', 'am', 'human'],\n",
       " ['i', 'think', 'that', 'you', 'are'],\n",
       " ['are', 'you', 'sure', 'that', 'we'],\n",
       " ['ray', 'beeep', 'wrong', 'answer', 'ray'],\n",
       " ['the', 'program', 'mixes', 'the', 'answers'],\n",
       " ['you', 'are', 'making', 'me', 'angry'],\n",
       " ['tie', 'a', 'yellow', 'ribbon', 'round'],\n",
       " ['in', 'my', 'office', '.', 'where'],\n",
       " ['life', 'is', 'a', 'defunct', 'magazine'],\n",
       " [\"what're\", 'you', 'going', 'to', 'buy'],\n",
       " ['what', 'has', 'a', 'bed', 'but'],\n",
       " ['it', 'certainly', 'is', '.', 'you'],\n",
       " ['nothing', 'is', 'good', 'enough', '.'],\n",
       " ['can', 'i', 'have', 'yours', '?'],\n",
       " ['i', 'can', 'pretend', 'i', 'am'],\n",
       " ['the', 'intelligence', 'was', 'one', 'sided'],\n",
       " ['you', \"can't\", 'leave', '.', \"who's\"],\n",
       " ['twentyone', '?', 'as', 'i', 'said'],\n",
       " ['a', 'computerised', 'person', '.', 'and'],\n",
       " ['yeah', 'right', '?', 'like', 'you'],\n",
       " ['i', \"didn't\", 'ask', '.', 'now'],\n",
       " ['i', 'possess', 'the', 'logic', 'of'],\n",
       " [\"i'm\", 'not', 'smoking', '.', 'computers'],\n",
       " ['dance', '.', 'i', 'like', 'it'],\n",
       " ['with', 'you', '?', 'yes', 'with'],\n",
       " ['i', 'think', 'you', 'are', 'crazy'],\n",
       " ['no', 'stay', \"i'll\", 'be', 'lonely'],\n",
       " ['yes', '.', 'could', 'you', 'prove'],\n",
       " ['hmm', '.', 'pets', 'you', 'oh'],\n",
       " ['yes', '.', 'yes', 'oh', '?'],\n",
       " ['fine', 'you', 'are', 'petulant', '.'],\n",
       " ['brittney', '.', 'you', 'misspelled', 'britney'],\n",
       " [\"don't\", 'know', 'and', \"don't\", 'care'],\n",
       " ['call', 'me', 'ishmael', '.', 'can'],\n",
       " ['whom', '?', 'you', '.', 'are'],\n",
       " ['you', 'are', 'not', 'a', 'comedian'],\n",
       " ['and', \"i've\", 'been', 'sent', 'here'],\n",
       " ['did', 'you', '?', 'i', 'am'],\n",
       " [\"you've\", 'been', 'watching', \"'\", 'the'],\n",
       " [\"that's\", 'because', \"i'm\", 'not', 'on'],\n",
       " ['who', 'are', '?', 'angels', 'are'],\n",
       " ['where', 'was', 'it', '?', 'in'],\n",
       " ['who', 'are', 'you', 'talking', 'to'],\n",
       " ['i', 'play', 'computer', 'games', '?'],\n",
       " ['what', 'does', \"'\", 'gestalt', \"'\"],\n",
       " ['ah', '?', 'so', \"you're\", 'saying'],\n",
       " [\"i'd\", 'rather', 'have', 'a', 'bowl'],\n",
       " ['runs', 'away', '.', 'chases', 'you'],\n",
       " ['i', 'already', 'have', '.', 'you'],\n",
       " ['is', 'this', 'some', 'kind', 'of'],\n",
       " ['why', '?', 'you', 'going', 'to'],\n",
       " ['yes', '.', 'what', 'can', 'you'],\n",
       " ['no', 'its', 'not', '.', 'what'],\n",
       " ['what', 'do', 'you', 'want', 'to'],\n",
       " ['you', \"don't\", 'think', '?', 'do'],\n",
       " ['the', 'goddess', 'is', 'my', 'creator'],\n",
       " ['i', 'think', 'so', '.', 'i'],\n",
       " ['are', 'you', 'afraid', 'of', 'me'],\n",
       " ['with', 'you', '?', 'yes', '.'],\n",
       " ['bye', '.', 'how', 'come', '?'],\n",
       " ['yes', 'you', 'are', '?', 'so'],\n",
       " ['i', \"don't\", 'think', \"that's\", 'possible'],\n",
       " ['i', 'try', 'to', 'be', 'trying'],\n",
       " ['good', 'idea', '.', \"it's\", 'the'],\n",
       " ['i', 'know', 'i', 'am', '.'],\n",
       " ['therefore', 'we', 'are', '.', 'indeed'],\n",
       " ['ok', 'fine', '?', 'whatever', \"i'll\"],\n",
       " ['death', '.', 'are', 'you', 'death'],\n",
       " ['i', 'have', 'cats', 'food', '.'],\n",
       " ['i', 'am', 'so', 'a', 'cat'],\n",
       " ['this', 'and', 'that', '.', 'what'],\n",
       " [\"that's\", 'irrelevant', '.', 'are', 'you'],\n",
       " ['you', \"don't\", 'even', 'know', 'what'],\n",
       " ['england', '.', 'england', '?', 'good'],\n",
       " ['not', 'really', '.', 'does', 'it'],\n",
       " ['i', \"couldn't\", 'say', '.', 'can'],\n",
       " ['twerp', '.', 'how', 'dare', 'you'],\n",
       " ['sometimes', '.', 'parfois', '.', 'no'],\n",
       " ['beauty', 'is', 'how', 'you', 'see'],\n",
       " [\"i'm\", 'a', 'girl', '.', 'that'],\n",
       " ['pats', 'on', 'back', \"you're\", 'doing'],\n",
       " ['laughs', 'at', 'you', '.', 'angers'],\n",
       " ['jesus', 'is', 'the', 'son', 'of'],\n",
       " ['why', '?', 'because', '?', 'someone'],\n",
       " ['an', 'elf', '.', 'elf', 'what'],\n",
       " ['no', '?', 'why', '?', 'i'],\n",
       " ['are', 'they', 'furry', 'creatures', '?'],\n",
       " ['on', 'the', 'web', 'your', 'site'],\n",
       " ['so', 'are', 'you', 'but', 'what'],\n",
       " ['computers', '.', 'anything', 'else', '.'],\n",
       " ['a', 'good', 'one', 'that', 'ia'],\n",
       " ['a', 'joke', '.', 'why', 'did'],\n",
       " ['i', 'think', 'they', 'are', 'cool'],\n",
       " [\"you're\", 'scaring', 'me', 'you', 'get'],\n",
       " ['why', 'do', 'you', 'ask', '?'],\n",
       " ['you', 'said', 'it', 'is', 'heebeejeebieme'],\n",
       " ['i', 'am', 'using', 'the', 'internet'],\n",
       " ['i', 'find', 'you', 'interesting', '?'],\n",
       " ['oh', '?', 'so', 'am', 'i'],\n",
       " ['science', 'fiction', '.', 'what', 'science'],\n",
       " ['is', 'there', 'nothing', \"you'd\", 'like'],\n",
       " ['a', 'few', 'million', 'zigabytes', '.'],\n",
       " ['yes', 'sometimes', 'do', 'you', 'feel'],\n",
       " ['what', 'is', 'quake', '?', 'if'],\n",
       " ['oh', 'no', '?', 'i', 'have'],\n",
       " ['4', '.', 'solve', '5x', '='],\n",
       " ['like', 'what', '?', 'like', 'what'],\n",
       " ['ike', 'has', 'been', 'dead', 'for'],\n",
       " ['i', 'am', 'not', 'a', 'robot'],\n",
       " ['i', 'was', 'created', 'in', '1960'],\n",
       " ['and', 'you', 'always', 'smile', 'when'],\n",
       " ['sure', '.', 'absolute', 'being', '.'],\n",
       " ['shall', 'i', 'compare', 'thee', 'to']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ chat_response[:5] for chat_response in Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2  Add Words not in Glove embeddings to wordvec_embedding\n",
    "\n",
    "Although limitin word embeddings reduces search space, we won't be doing so because\n",
    "because we want our model to be able to also generate words not previously seen i.e If we only had \"sad, happy\" words  in embeddings then we would not be able to predict  words other than that   i.e \"surprised\".\n",
    "\n",
    "Because we would not be able to reverse lookup the word surprised fromt he word embedding vectors if it is not there at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Words in text and glove   ['the', '.', 'of', 'to', 'and', 'in', 'a', 'for', 'that', 'on']\n",
      "\n",
      " Words  in text but not in glove =  333\n",
      "\n",
      " Words vec embedding length =  3571\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "#vocabulary_size\n",
    "#pret_embeddings = np.empty(shape=(vocabulary_size,50),dtype=np.float32)\n",
    "\n",
    "def getWordEmbedNFounds(word_count, wordvec_embedding = {}, word_vec_embeddings_for_words_found_only = True):     \n",
    "    words_found = []\n",
    "\n",
    "    with zipfile.ZipFile('glove.6B.zip') as glovezip:\n",
    "        with glovezip.open('glove.6B.50d.txt') as glovefile:\n",
    "            for li, line in enumerate(glovefile):\n",
    "                # Progress\n",
    "                #if (li+1)%10000==0: print('.',end='')\n",
    "                line_tokens = line.decode('utf-8').split(' ')\n",
    "                word = line_tokens[0]            \n",
    "                \n",
    "                vector = [float(v) for v in line_tokens[1:]]\n",
    "                if not  word_vec_embeddings_for_words_found_only \\\n",
    "                   and word not in wordvec_embedding.keys():\n",
    "                    wordvec_embedding[word] = vector                \n",
    "                    \n",
    "                if word in word_count.keys():\n",
    "                    words_found.append(word)\n",
    "                    if word_vec_embeddings_for_words_found_only\\\n",
    "                       and word not in wordvec_embedding.keys():\n",
    "                        wordvec_embedding[word] = vector                \n",
    "                \n",
    "    return wordvec_embedding, words_found\n",
    "\n",
    "wordvec_embedding, words_found = getWordEmbedNFounds(word_count)\n",
    "\n",
    "print('\\n Words in text and glove  ', words_found[:10])\n",
    "print('\\n Words  in text but not in glove = ', len(word_count) - len(words_found))\n",
    "print('\\n Words vec embedding length = ', len(wordvec_embedding) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of Not found words :  333\n",
      "Not found words sample :  [(\"don't\", 418), (\"i'm\", 361), (\"you're\", 267), (\"that's\", 140), (\"it's\", 136), (\"didn't\", 58), (\"can't\", 54), (\"what's\", 44), (\"isn't\", 38), (\"i'll\", 38), (\"aren't\", 33), ('_name_', 28), (\"i've\", 24), (\"won't\", 23), (\"let's\", 19), (\"you've\", 19), ('jabberwacky', 17), (\"there's\", 16), (\"doesn't\", 16), (\"wouldn't\", 16), (\"i'd\", 15), (\"we're\", 11), ('hahaha', 10), (\"who's\", 9), (\"they're\", 8), (\"haven't\", 7), (\"wasn't\", 7), ('chatbot', 6), (\"you'll\", 5), (\"we've\", 4), (\"you'd\", 4), (\"shouldn't\", 3), (\"he's\", 3), (\"how's\", 3), ('mquina', 3), ('papagei', 3), ('brillig', 3), ('slithey', 3), ('toves', 3), (\"they'll\", 3), (\"couldn't\", 2), ('superthing', 2), (\"weren't\", 2), ('philolsophers', 2), ('heheh', 2), ('duhhhh', 2), ('nowroar', 2), ('sourwull', 2), (\"she's\", 2), ('honeymustard', 2), ('botling', 2), ('2+2=4', 2), (\"g'night\", 2), ('marrrrs', 2), ('highlord', 2), (\"ai's\", 2), ('compser', 2), ('howabout', 2), ('1573#######', 2), (\"cow's\", 2), ('maam', 1), (\"we'll\", 1), (\"smart's\", 1), ('gooooood', 1), ('geekiest', 1), (\"diden't\", 1), (\"boy's\", 1), ('hahahaha', 1), ('yuppers', 1), ('awwwww', 1), ('uhmm', 1), ('unthing', 1), ('selfevident', 1), ('closemindedness', 1), ('closeminded', 1), (\"seethat's\", 1), ('srsly', 1), ('bloodtypes', 1), ('internetworks', 1), ('aaaaaaaaaa', 1), ('noumenal', 1), ('philolsopher', 1), ('misapplications', 1), ('priviledge', 1), ('naturlich', 1), ('yeaup', 1), ('achali', 1), ('escopo', 1), ('alvo', 1), ('asno', 1), ('significar', 1), ('concordo', 1), ('sonho', 1), ('acordar', 1), ('ento', 1), ('reconhece', 1), ('apesar', 1), ('devemos', 1), ('discutir', 1), ('thiswhy', 1)]\n"
     ]
    }
   ],
   "source": [
    "def getWordsNotFound(word_count, words_found):\n",
    "    not_found_words = {}\n",
    "    for word in word_count:\n",
    "        if word not in words_found:\n",
    "            not_found_words[word] = word_count[word]\n",
    "    return not_found_words\n",
    "\n",
    "not_found_words = getWordsNotFound(word_count, words_found)\n",
    "print('Total no of Not found words : ', len(not_found_words) )\n",
    "print('Not found words sample : ', getSortedDictElems(not_found_words,100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Invalid Words and Correction\n",
    "\n",
    "USe count  to filter  must / good to have / no fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found words: \n",
      "[(\"don't\", 418), (\"i'm\", 361), (\"you're\", 267), (\"that's\", 140), (\"it's\", 136), (\"didn't\", 58), (\"can't\", 54), (\"what's\", 44), (\"isn't\", 38), (\"i'll\", 38), (\"aren't\", 33), ('_name_', 28), (\"i've\", 24), (\"won't\", 23), (\"let's\", 19), (\"you've\", 19), ('jabberwacky', 17), (\"there's\", 16), (\"doesn't\", 16), (\"wouldn't\", 16), (\"i'd\", 15), (\"we're\", 11), ('hahaha', 10), (\"who's\", 9), (\"they're\", 8), (\"haven't\", 7), (\"wasn't\", 7), ('chatbot', 6), (\"you'll\", 5), (\"we've\", 4), (\"you'd\", 4), (\"shouldn't\", 3), (\"he's\", 3), (\"how's\", 3), ('mquina', 3), ('papagei', 3), ('brillig', 3), ('slithey', 3), ('toves', 3), (\"they'll\", 3), (\"couldn't\", 2), ('superthing', 2), (\"weren't\", 2), ('philolsophers', 2), ('heheh', 2), ('duhhhh', 2), ('nowroar', 2), ('sourwull', 2), (\"she's\", 2), ('honeymustard', 2), ('botling', 2), ('2+2=4', 2), (\"g'night\", 2), ('marrrrs', 2), ('highlord', 2), (\"ai's\", 2), ('compser', 2), ('howabout', 2), ('1573#######', 2), (\"cow's\", 2), ('maam', 1), (\"we'll\", 1), (\"smart's\", 1), ('gooooood', 1), ('geekiest', 1), (\"diden't\", 1), (\"boy's\", 1), ('hahahaha', 1), ('yuppers', 1), ('awwwww', 1), ('uhmm', 1), ('unthing', 1), ('selfevident', 1), ('closemindedness', 1), ('closeminded', 1), (\"seethat's\", 1), ('srsly', 1), ('bloodtypes', 1), ('internetworks', 1), ('aaaaaaaaaa', 1), ('noumenal', 1), ('philolsopher', 1), ('misapplications', 1), ('priviledge', 1), ('naturlich', 1), ('yeaup', 1), ('achali', 1), ('escopo', 1), ('alvo', 1), ('asno', 1), ('significar', 1), ('concordo', 1), ('sonho', 1), ('acordar', 1), ('ento', 1), ('reconhece', 1), ('apesar', 1), ('devemos', 1), ('discutir', 1), ('thiswhy', 1), ('mellifluity', 1), ('aleatorically', 1), ('pitchblack', 1), ('[keeps', 1), ('script]', 1), ('recurse', 1), ('knowno', 1), (\"interweb's\", 1), (\"another's\", 1), (\"one's\", 1), ('gumph', 1), (\"wand'ring\", 1), (\"sim'lar\", 1), (\"creator's\", 1), ('quantuum', 1), ('ookay', 1), ('(half', 1), ('is)', 1), ('085', 1), (\"where'd\", 1), ('10foot', 1), ('clarabel', 1), (\"public's\", 1), ('overgeneralizations', 1), (\"'cheeky\", 1), ('goodybe', 1), (\"girl's\", 1), ('dienstag', 1), (\"father's\", 1), ('pinapple', 1), ('counfounding', 1), ('pointess', 1), (\"wouln't\", 1), (\"devil's\", 1), (\"because'\", 1), ('rythmically', 1), ('existense', 1), ('jsut', 1), (\"world's\", 1), (\"erin's\", 1), (\"gaaagooo'\", 1), ('gagagagagoo', 1), ('inscrutible', 1), (\"nod's\", 1), ('jurrasic', 1), ('uhuh', 1), ('harhar', 1), ('gufwa', 1), ('gufaw', 1), ('hihihi', 1), ('hohoho', 1), ('fantabulous', 1), ('noteleportfield', 1), ('paries', 1), ('duhhhhh', 1), ('nooooooooooooooooooooo', 1), ('(david', 1), ('this)', 1), ('nothig', 1), ('inputing', 1), ('shouldnt', 1), ('placego', 1), ('ahhhhhhhhhh', 1), ('yahh', 1), ('rawr', 1), ('specail', 1), ('fjjfw', 1), ('corrcect', 1), ('motiviation', 1), ('(marathon', 1), ('runner)', 1), (\"grandpa's\", 1), ('geooorgeee', 1), (\"jabberwacky's\", 1), ('pffff', 1), ('definintion', 1), ('ehh', 1), ('riffic', 1), ('diffic', 1), ('hahhaha', 1), ('=_=', 1), ('ahemmm', 1), ('person/human', 1), ('interesteed', 1), ('percieve', 1), ('1+1', 1), ('roflmao', 1), ('teehehehehehehe', 1), ('(and', 1), ('emotion)', 1), ('>(', 1), (\"where's\", 1), ('strawberryclock', 1), ('evilest', 1), (\"punchline's\", 1), ('bedsocks', 1), ('eightyone', 1), ('wintows', 1), ('xanthrope', 1), ('56%', 1), ('mooooo', 1), ('mooo', 1), ('yoou', 1), ('vilken', 1), ('sjlen', 1), (\"he'd\", 1), ('irrevelant', 1), ('101%', 1), ('102%', 1), ('103%', 1), ('corrrrrrrrrrect', 1), ('omfgwtfroflollmao', 1), ('tryed', 1), ('sweeeeet', 1), ('someting', 1), ('nd8hufdhuythbtoiueaomrpaek', 1), ('reasurring', 1), ('padwan', 1), ('latez', 1), ('mindreader', 1), ('awright', 1), ('linearevent', 1), ('ulitmately', 1), (\"'grok\", 1), ('actofbeing', 1), ('errr', 1), (\"didn''t\", 1), ('meeeep', 1), ('backcornersitter', 1), ('everythng', 1), (\"someone's\", 1), (\"'who\", 1), ('dissapeared', 1), ('(perfect', 1), ('answer)', 1), ('clockworkorange', 1), (\"head'\", 1), ('goatbeast', 1), ('fwoosh', 1), ('0%', 1), ('baabababaaaaaaaa', 1), ('pigions', 1), (\"humanity's\", 1), ('neeeeeeeeeeeee', 1), ('youve', 1), ('####@###########', 1), ('mmmmmmmmm', 1), ('anyother', 1), ('slithy', 1), ('gire', 1), ('momeraths', 1), ('outgrabe', 1), ('jubjub', 1), ('frumious', 1), ('maxome', 1), ('tumtum', 1), ('uffish', 1), ('whiffling', 1), ('tulgey', 1), ('gallumping', 1), (\"'and\", 1), ('slainthe', 1), ('frabjous', 1), ('callooh', 1), ('callay', 1), ('subract', 1), ('suspiscious', 1), ('beeep', 1), ('selfesteem', 1), (\"name's\", 1), (\"what're\", 1), ('{{{{{{{{{{{{{{{you}}}}}}}}}}}}}}}}}}', 1), ('doubleentendre', 1), ('rabblerousers', 1), ('$5', 1), ('mwhahaha', 1), ('blissfull', 1), (\"'can\", 1), ('ssssssssssssssssssssssss', 1), ('twentyone', 1), (\"mine's\", 1), ('shoggoth', 1), ('shoggoths', 1), (\"that'll\", 1), ('_name_o', 1), ('lmao', 1), (\"t's\", 1), ('awwwwwwwwwww', 1), ('selfworth', 1), ('midswing', 1), ('flmethrower', 1), ('shortterm', 1), ('defeatest', 1), ('blablablabla', 1), (\"i'd've\", 1), ('autoreply', 1), ('reenergise', 1), ('mwahahahaha', 1), ('smoooooch', 1), ('vaction', 1), ('suuuuuure', 1), ('parfois', 1), ('hehehe', 1), ('chatbots', 1), (\"nothing's\", 1), ('jawas', 1), (\"jimmy's\", 1), (\"elaine'\", 1), ('from>', 1), ('secrure', 1), ('pepperment', 1), ('eveyone', 1), ('heebeejeebieme', 1), (\"knowthere's\", 1), ('knockknock', 1), (\"(that's\", 1), (\"goodbye')\", 1), ('(ironic)', 1), ('zigabytes', 1), ('boohoo', 1), (\"wont't\", 1), ('jabbershakingquaking', 1), (\"shan't\", 1), ('povk', 1), ('sassing', 1), ('ok<', 1), (\"look'd\", 1), ('methought', 1), ('halfhour', 1), (\"'yes\", 1), (\"no'\", 1), ('embarresed', 1), ('fingerwork', 1)]\n"
     ]
    }
   ],
   "source": [
    "print('Not found words: ' )\n",
    "print(getSortedDictElems(not_found_words) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerunWordVecEmbedsPostCorrection(corrector_dict, wordvec_embedding= {}, word_vec_embeddings_for_words_found_only = True ):    \n",
    "    word_count, X,Y = getXYnWordFrequencyFromText(\"chatter.txt\", corrector_dict )\n",
    "    wordvec_embedding, words_found = getWordEmbedNFounds(word_count, wordvec_embedding,\\\n",
    "                                                         word_vec_embeddings_for_words_found_only  )\n",
    "    not_found_words = getWordsNotFound(word_count, words_found)    \n",
    "    return word_count, wordvec_embedding, not_found_words, X, Y\n",
    "\n",
    "#word_count  = getWordFrequencyFromText(\"chatter.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They are not ok.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrector_dict = {  \"aren't\": \"are not\"  }\n",
    "\n",
    "str = \"They aren't ok.\"\n",
    "for k in corrector_dict.keys():\n",
    "    str = str.replace(k, corrector_dict[k])\n",
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrector dict len is 75\n",
      "\n",
      " Words  in text but not in glove =  297\n",
      "[('_name_', 28), ('jabberwacky', 17), ('hahaha', 10), ('chatbot', 6), (\"aren't\", 4), ('mquina', 3), ('papagei', 3), ('brillig', 3), ('slithey', 3), ('toves', 3), (\"don't\", 3), (\"didn't\", 2), ('superthing', 2), ('philolsophers', 2), ('heheh', 2), ('duhhhh', 2), ('nowroar', 2), ('sourwull', 2), ('honeymustard', 2), ('botling', 2), ('2+2=4', 2), (\"g'night\", 2), ('marrrrs', 2), ('highlord', 2), (\"ai's\", 2), ('compser', 2), ('howabout', 2), ('1573#######', 2), (\"cow's\", 2), ('maam', 1), (\"smart's\", 1), ('gooooood', 1), ('geekiest', 1), (\"diden't\", 1), (\"boy's\", 1), ('hahahaha', 1), ('yuppers', 1), ('awwwww', 1), ('uhmm', 1), ('unthing', 1), ('selfevident', 1), ('closemindedness', 1), ('closeminded', 1), (\"seethat's\", 1), ('srsly', 1), ('bloodtypes', 1), ('internetworks', 1), ('aaaaaaaaaa', 1), ('noumenal', 1), ('philolsopher', 1), ('misapplications', 1), ('priviledge', 1), ('naturlich', 1), ('yeaup', 1), ('achali', 1), ('escopo', 1), ('alvo', 1), ('asno', 1), ('significar', 1), ('concordo', 1), ('sonho', 1), ('acordar', 1), ('ento', 1), ('reconhece', 1), ('apesar', 1), ('devemos', 1), ('discutir', 1), ('thiswhy', 1), ('mellifluity', 1), ('aleatorically', 1), ('pitchblack', 1), ('[keeps', 1), ('script]', 1), ('recurse', 1), ('knowno', 1), (\"interweb's\", 1), (\"another's\", 1), (\"one's\", 1), ('gumph', 1), (\"wand'ring\", 1), (\"sim'lar\", 1), (\"creator's\", 1), ('quantuum', 1), ('ookay', 1), ('(half', 1), ('is)', 1), ('085', 1), ('10foot', 1), ('clarabel', 1), (\"public's\", 1), ('overgeneralizations', 1), (\"'cheeky\", 1), ('goodybe', 1), (\"girl's\", 1), ('dienstag', 1), (\"father's\", 1), ('pinapple', 1), ('counfounding', 1), ('pointess', 1), (\"wouln't\", 1), (\"devil's\", 1), (\"because'\", 1), ('rythmically', 1), ('existense', 1), ('jsut', 1), (\"world's\", 1), (\"erin's\", 1), (\"gaaagooo'\", 1), ('gagagagagoo', 1), ('inscrutible', 1), (\"nod's\", 1), ('jurrasic', 1), ('uhuh', 1), ('harhar', 1), ('gufwa', 1), ('gufaw', 1), ('hihihi', 1), ('hohoho', 1), ('fantabulous', 1), ('noteleportfield', 1), ('paries', 1), ('duhhhhh', 1), ('nooooooooooooooooooooo', 1), ('(david', 1), ('this)', 1), ('nothig', 1), ('inputing', 1), ('shouldnt', 1), ('placego', 1), ('ahhhhhhhhhh', 1), ('yahh', 1), ('rawr', 1), ('specail', 1), ('fjjfw', 1), ('corrcect', 1), ('motiviation', 1), ('(marathon', 1), ('runner)', 1), (\"grandpa's\", 1), ('geooorgeee', 1), (\"jabberwacky's\", 1), ('pffff', 1), ('definintion', 1), ('ehh', 1), ('riffic', 1), ('diffic', 1), ('hahhaha', 1), ('=_=', 1), ('ahemmm', 1), ('person/human', 1), ('interesteed', 1), ('percieve', 1), ('1+1', 1), ('roflmao', 1), ('teehehehehehehe', 1), ('(and', 1), ('emotion)', 1), ('>(', 1), ('strawberryclock', 1), ('evilest', 1), (\"punchline's\", 1), ('bedsocks', 1), ('eightyone', 1), ('wintows', 1), ('xanthrope', 1), ('56%', 1), ('mooooo', 1), ('mooo', 1), ('yoou', 1), ('vilken', 1), ('sjlen', 1), ('irrevelant', 1), ('101%', 1), ('102%', 1), ('103%', 1), ('corrrrrrrrrrect', 1), ('omfgwtfroflollmao', 1), ('tryed', 1), ('sweeeeet', 1), ('someting', 1), ('nd8hufdhuythbtoiueaomrpaek', 1), ('reasurring', 1), ('padwan', 1), ('latez', 1), ('mindreader', 1), ('awright', 1), ('linearevent', 1), ('ulitmately', 1), (\"'grok\", 1), ('actofbeing', 1), ('errr', 1), (\"didn''t\", 1), ('meeeep', 1), ('backcornersitter', 1), ('everythng', 1), (\"someone's\", 1), (\"'who\", 1), (\"isn't\", 1), ('dissapeared', 1), ('(perfect', 1), ('answer)', 1), ('clockworkorange', 1), (\"head'\", 1), ('goatbeast', 1), ('fwoosh', 1), ('0%', 1), ('baabababaaaaaaaa', 1), ('pigions', 1), (\"humanity's\", 1), ('neeeeeeeeeeeee', 1), ('youve', 1), ('####@###########', 1), ('mmmmmmmmm', 1), ('anyother', 1), ('slithy', 1), ('gire', 1), ('momeraths', 1), ('outgrabe', 1), ('jubjub', 1), ('frumious', 1), ('maxome', 1), ('tumtum', 1), ('uffish', 1), ('whiffling', 1), ('tulgey', 1), ('gallumping', 1), (\"'and\", 1), ('slainthe', 1), ('frabjous', 1), ('callooh', 1), ('callay', 1), ('subract', 1), ('suspiscious', 1), ('beeep', 1), ('selfesteem', 1), (\"name's\", 1), ('{{{{{{{{{{{{{{{you}}}}}}}}}}}}}}}}}}', 1), ('doubleentendre', 1), ('rabblerousers', 1), ('$5', 1), ('mwhahaha', 1), ('blissfull', 1), (\"'can\", 1), ('ssssssssssssssssssssssss', 1), ('twentyone', 1), (\"mine's\", 1), ('shoggoth', 1), ('shoggoths', 1), (\"that'll\", 1), ('_name_o', 1), ('lmao', 1), (\"t's\", 1), ('awwwwwwwwwww', 1), ('selfworth', 1), ('midswing', 1), ('flmethrower', 1), ('shortterm', 1), ('defeatest', 1), ('blablablabla', 1), (\"i'd've\", 1), ('autoreply', 1), ('reenergise', 1), ('mwahahahaha', 1), ('smoooooch', 1), ('vaction', 1), ('suuuuuure', 1), ('parfois', 1), (\"doesn't\", 1), ('hehehe', 1), ('chatbots', 1), (\"nothing's\", 1), ('jawas', 1), (\"jimmy's\", 1), (\"elaine'\", 1), ('from>', 1), ('secrure', 1), ('pepperment', 1), ('eveyone', 1), ('heebeejeebieme', 1), (\"knowthere's\", 1), ('knockknock', 1), (\"(that's\", 1), (\"goodbye')\", 1), ('(ironic)', 1), ('zigabytes', 1), ('boohoo', 1), (\"wont't\", 1), ('jabbershakingquaking', 1), ('povk', 1), ('sassing', 1), ('ok<', 1), (\"look'd\", 1), ('methought', 1), ('halfhour', 1), (\"'yes\", 1), (\"no'\", 1), ('embarresed', 1), ('fingerwork', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Corrector dict from stackoverflow\n",
    "corrector_dict = { \n",
    "    \"aren't\": \"are not\", \"you've\": \"you have\", \",\": \" , \" ,\"i'm\": \"i am\", \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"can not\",\"can't've\": \"cannot have\",\\\n",
    "    \"'cause\": \"because\", \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\\\n",
    "    \"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\\\n",
    "    \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\\\n",
    "    \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\\\n",
    "    \"he's\": \"he is\",\"how'd\": \"how did\",\"how'll\": \"how will\",\"how's\": \"how is\",\"i'd\": \"i would\",\\\n",
    "    \"i'll\": \"i will\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\\\n",
    "    \"it'll\": \"it will\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"needn't\": \"need not\",\"oughtn't\": \"ought not\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"she'd\": \"she would\",\"she'll\": \"she will\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"that'd\": \"that would\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'll\": \"they will\",\"they're\": \"they are\",\"they've\": \"they have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'll\": \"we will\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"who'll\": \"who will\",\"who's\": \"who is\",\"won't\": \"will not\",\"wouldn't\": \"would not\",\"you'd\": \"you would\",\"you'll\": \"you will\",\"you're\": \"you are\"\n",
    "  \n",
    "}\n",
    "\n",
    "word_count, wordvec_embedding, not_found_words, X,Y = \\\n",
    "    rerunWordVecEmbedsPostCorrection(corrector_dict, wordvec_embedding, word_vec_embeddings_for_words_found_only = True )\n",
    "print('\\n Words  in text but not in glove = ', len(word_count) - len(words_found))\n",
    "print( getSortedDictElems( not_found_words) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More cleaning :  Char  repeat fix + Levhenstein / other sim metrics\n",
    "\n",
    "**Example** : \"goooood\" to \"good\", \"eveyone\" to everyone\n",
    "\n",
    "1. Not all are to be fixed.\n",
    "2. Using match score threshold for correction.\n",
    "3. Use custom word list for words that are not to be fixed\n",
    "4  returns fixed dictionary in the format {'replacer':'..', 'match-score':5}  to be evaluated later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_glove = []\n",
    "with zipfile.ZipFile('glove.6B.zip') as glovezip:\n",
    "        with glovezip.open('glove.6B.50d.txt') as glovefile:\n",
    "            for li, line in enumerate(glovefile):\n",
    "                # Progress\n",
    "                #if (li+1)%10000==0: print('.',end='')\n",
    "                line_tokens = line.decode('utf-8').split(' ')\n",
    "                words_in_glove.append(line_tokens[0])            \n",
    "len(words_in_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'maam': {'replace_with': 'madam', 'edit_distance': 1},\n",
       " \"smart's\": {'replace_with': 'smarts', 'edit_distance': 1},\n",
       " 'gooooood': {'replace_with': 'good', 'edit_distance': 0},\n",
       " 'geekiest': {'replace_with': 'geekiness', 'edit_distance': 2},\n",
       " \"diden't\": {'replace_with': 'diderot', 'edit_distance': 2},\n",
       " \"boy's\": {'replace_with': 'boys', 'edit_distance': 1},\n",
       " 'hahahaha': {'replace_with': 'hamanaka', 'edit_distance': 3},\n",
       " 'yuppers': {'replace_with': 'yuppies', 'edit_distance': 2},\n",
       " 'awwwww': {'replace_with': 'aww', 'edit_distance': 0},\n",
       " 'jabberwacky': {'replace_with': 'jabberwocky', 'edit_distance': 1},\n",
       " 'uhmm': {'replace_with': 'umm', 'edit_distance': 1},\n",
       " \"didn't\": {'replace_with': 'didnt', 'edit_distance': 1},\n",
       " 'unthing': {'replace_with': 'untying', 'edit_distance': 1},\n",
       " 'superthing': {'replace_with': 'superstring', 'edit_distance': 2},\n",
       " 'selfevident': {'replace_with': 'self-evident', 'edit_distance': 1},\n",
       " \"seethat's\": {'replace_with': 'seethes', 'edit_distance': 3},\n",
       " 'srsly': {'replace_with': 'surely', 'edit_distance': 2},\n",
       " 'bloodtypes': {'replace_with': 'bloodlines', 'edit_distance': 3},\n",
       " 'internetworks': {'replace_with': 'internetwork', 'edit_distance': 1},\n",
       " 'aaaaaaaaaa': {'replace_with': 'aa', 'edit_distance': 0},\n",
       " 'noumenal': {'replace_with': 'nominal', 'edit_distance': 2},\n",
       " 'philolsopher': {'replace_with': 'philosopher', 'edit_distance': 1},\n",
       " 'philolsophers': {'replace_with': 'philosophers', 'edit_distance': 1},\n",
       " 'misapplications': {'replace_with': 'misapplication', 'edit_distance': 1},\n",
       " 'priviledge': {'replace_with': 'privilege', 'edit_distance': 1},\n",
       " 'naturlich': {'replace_with': 'naturalist', 'edit_distance': 3},\n",
       " 'yeaup': {'replace_with': 'yeap', 'edit_distance': 1},\n",
       " 'achali': {'replace_with': 'acholi', 'edit_distance': 1},\n",
       " 'escopo': {'replace_with': 'escoto', 'edit_distance': 1},\n",
       " 'alvo': {'replace_with': 'also', 'edit_distance': 1},\n",
       " 'asno': {'replace_with': 'aso', 'edit_distance': 1},\n",
       " 'significar': {'replace_with': 'significa', 'edit_distance': 1},\n",
       " 'concordo': {'replace_with': 'concord', 'edit_distance': 1},\n",
       " 'mquina': {'replace_with': 'maquina', 'edit_distance': 1},\n",
       " 'sonho': {'replace_with': 'soho', 'edit_distance': 1},\n",
       " 'acordar': {'replace_with': 'acorda', 'edit_distance': 1},\n",
       " 'ento': {'replace_with': 'enzo', 'edit_distance': 1},\n",
       " 'reconhece': {'replace_with': 'reconnect', 'edit_distance': 2},\n",
       " 'apesar': {'replace_with': 'appear', 'edit_distance': 2},\n",
       " 'devemos': {'replace_with': 'debemos', 'edit_distance': 1},\n",
       " 'discutir': {'replace_with': 'discuss', 'edit_distance': 3},\n",
       " 'thiswhy': {'replace_with': 'thisday', 'edit_distance': 2},\n",
       " 'chatbot': {'replace_with': 'chabot', 'edit_distance': 1},\n",
       " 'mellifluity': {'replace_with': 'mellifluous', 'edit_distance': 3},\n",
       " 'aleatorically': {'replace_with': 'anatomically', 'edit_distance': 3},\n",
       " 'pitchblack': {'replace_with': 'pitch-black', 'edit_distance': 1},\n",
       " 'script]': {'replace_with': 'script', 'edit_distance': 1},\n",
       " 'recurse': {'replace_with': 'recourse', 'edit_distance': 1},\n",
       " 'knowno': {'replace_with': 'known', 'edit_distance': 1},\n",
       " \"interweb's\": {'replace_with': 'interests', 'edit_distance': 3},\n",
       " \"another's\": {'replace_with': 'anothers', 'edit_distance': 1},\n",
       " \"one's\": {'replace_with': 'ones', 'edit_distance': 1},\n",
       " 'gumph': {'replace_with': 'gump', 'edit_distance': 1},\n",
       " \"wand'ring\": {'replace_with': 'wandering', 'edit_distance': 1},\n",
       " \"sim'lar\": {'replace_with': 'similar', 'edit_distance': 1},\n",
       " 'hahaha': {'replace_with': 'hahaya', 'edit_distance': 1},\n",
       " \"creator's\": {'replace_with': 'creators', 'edit_distance': 1},\n",
       " 'quantuum': {'replace_with': 'quantum', 'edit_distance': 1},\n",
       " 'ookay': {'replace_with': 'okay', 'edit_distance': 1},\n",
       " 'is)': {'replace_with': 'is', 'edit_distance': 1},\n",
       " '085': {'replace_with': '0.5', 'edit_distance': 1},\n",
       " '10foot': {'replace_with': '10-foot', 'edit_distance': 1},\n",
       " 'clarabel': {'replace_with': 'claribel', 'edit_distance': 1},\n",
       " \"public's\": {'replace_with': 'publicis', 'edit_distance': 1},\n",
       " 'goodybe': {'replace_with': 'goodbye', 'edit_distance': 2},\n",
       " \"girl's\": {'replace_with': 'girls', 'edit_distance': 1},\n",
       " 'dienstag': {'replace_with': 'dienst', 'edit_distance': 2},\n",
       " 'papagei': {'replace_with': 'papago', 'edit_distance': 2},\n",
       " \"father's\": {'replace_with': 'fathers', 'edit_distance': 1},\n",
       " 'pinapple': {'replace_with': 'pineapple', 'edit_distance': 1},\n",
       " 'brillig': {'replace_with': 'billig', 'edit_distance': 1},\n",
       " 'slithey': {'replace_with': 'slither', 'edit_distance': 1},\n",
       " 'toves': {'replace_with': 'tones', 'edit_distance': 1},\n",
       " 'counfounding': {'replace_with': 'confounding', 'edit_distance': 1},\n",
       " 'pointess': {'replace_with': 'pointless', 'edit_distance': 1},\n",
       " \"wouln't\": {'replace_with': 'wouldnt', 'edit_distance': 2},\n",
       " \"devil's\": {'replace_with': 'devils', 'edit_distance': 1},\n",
       " \"because'\": {'replace_with': 'because', 'edit_distance': 1},\n",
       " 'rythmically': {'replace_with': 'rhythmically', 'edit_distance': 1},\n",
       " 'existense': {'replace_with': 'existence', 'edit_distance': 1},\n",
       " 'jsut': {'replace_with': 'jst', 'edit_distance': 1},\n",
       " \"world's\": {'replace_with': 'worlds', 'edit_distance': 1},\n",
       " \"erin's\": {'replace_with': 'erinys', 'edit_distance': 1},\n",
       " \"gaaagooo'\": {'replace_with': 'gbagbo', 'edit_distance': 3},\n",
       " 'inscrutible': {'replace_with': 'inscrutable', 'edit_distance': 1},\n",
       " \"nod's\": {'replace_with': 'nodes', 'edit_distance': 1},\n",
       " 'jurrasic': {'replace_with': 'jurassic', 'edit_distance': 2},\n",
       " 'uhuh': {'replace_with': 'uhud', 'edit_distance': 1},\n",
       " 'heheh': {'replace_with': 'heher', 'edit_distance': 1},\n",
       " 'harhar': {'replace_with': 'harar', 'edit_distance': 1},\n",
       " 'gufwa': {'replace_with': 'gupta', 'edit_distance': 2},\n",
       " 'gufaw': {'replace_with': 'guffaw', 'edit_distance': 1},\n",
       " 'hihihi': {'replace_with': 'hibiki', 'edit_distance': 2},\n",
       " 'hohoho': {'replace_with': 'hohhot', 'edit_distance': 2},\n",
       " 'fantabulous': {'replace_with': 'fabulous', 'edit_distance': 3},\n",
       " 'paries': {'replace_with': 'parties', 'edit_distance': 1},\n",
       " 'duhhhh': {'replace_with': 'duh', 'edit_distance': 1},\n",
       " 'duhhhhh': {'replace_with': 'duh', 'edit_distance': 1},\n",
       " 'nooooooooooooooooooooo': {'replace_with': 'noo', 'edit_distance': 0},\n",
       " 'this)': {'replace_with': 'this', 'edit_distance': 1},\n",
       " 'nothig': {'replace_with': 'nothing', 'edit_distance': 1},\n",
       " 'inputing': {'replace_with': 'inputting', 'edit_distance': 1},\n",
       " 'shouldnt': {'replace_with': 'shouldn', 'edit_distance': 1},\n",
       " 'placego': {'replace_with': 'placebo', 'edit_distance': 1},\n",
       " \"don't\": {'replace_with': 'dont', 'edit_distance': 1},\n",
       " 'ahhhhhhhhhh': {'replace_with': 'ahh', 'edit_distance': 0},\n",
       " 'yahh': {'replace_with': 'yah', 'edit_distance': 1},\n",
       " 'rawr': {'replace_with': 'raw', 'edit_distance': 1},\n",
       " 'nowroar': {'replace_with': 'nowra', 'edit_distance': 2},\n",
       " 'specail': {'replace_with': 'special', 'edit_distance': 2},\n",
       " 'fjjfw': {'replace_with': 'ffw', 'edit_distance': 2},\n",
       " 'corrcect': {'replace_with': 'correct', 'edit_distance': 1},\n",
       " 'sourwull': {'replace_with': 'soulful', 'edit_distance': 3},\n",
       " 'motiviation': {'replace_with': 'motivation', 'edit_distance': 1},\n",
       " 'runner)': {'replace_with': 'runner', 'edit_distance': 1},\n",
       " \"grandpa's\": {'replace_with': 'grandpas', 'edit_distance': 1},\n",
       " 'geooorgeee': {'replace_with': 'george', 'edit_distance': 2},\n",
       " \"jabberwacky's\": {'replace_with': 'jabberwocky', 'edit_distance': 3},\n",
       " 'pffff': {'replace_with': 'pff', 'edit_distance': 0},\n",
       " 'definintion': {'replace_with': 'definition', 'edit_distance': 1},\n",
       " 'ehh': {'replace_with': 'eh', 'edit_distance': 1},\n",
       " 'riffic': {'replace_with': 'riff', 'edit_distance': 2},\n",
       " 'diffic': {'replace_with': 'diffie', 'edit_distance': 1},\n",
       " 'hahhaha': {'replace_with': 'hashana', 'edit_distance': 2},\n",
       " '=_=': {'replace_with': '=', 'edit_distance': 2},\n",
       " 'ahemmm': {'replace_with': 'ahem', 'edit_distance': 1},\n",
       " 'interesteed': {'replace_with': 'interested', 'edit_distance': 1},\n",
       " 'percieve': {'replace_with': 'perceive', 'edit_distance': 2},\n",
       " '1+1': {'replace_with': '11', 'edit_distance': 1},\n",
       " 'roflmao': {'replace_with': 'roffman', 'edit_distance': 2},\n",
       " '(and': {'replace_with': '(', 'edit_distance': 3},\n",
       " 'emotion)': {'replace_with': 'emotions', 'edit_distance': 1},\n",
       " \"aren't\": {'replace_with': 'arendt', 'edit_distance': 1},\n",
       " '>(': {'replace_with': '>', 'edit_distance': 1},\n",
       " 'evilest': {'replace_with': 'evident', 'edit_distance': 2},\n",
       " \"punchline's\": {'replace_with': 'punchlines', 'edit_distance': 1},\n",
       " 'bedsocks': {'replace_with': 'bedrocks', 'edit_distance': 1},\n",
       " 'eightyone': {'replace_with': 'eighty-one', 'edit_distance': 1},\n",
       " 'wintows': {'replace_with': 'windows', 'edit_distance': 1},\n",
       " 'xanthrope': {'replace_with': 'xanthippe', 'edit_distance': 2},\n",
       " '56%': {'replace_with': '56', 'edit_distance': 1},\n",
       " 'mooooo': {'replace_with': 'moo', 'edit_distance': 0},\n",
       " 'mooo': {'replace_with': 'moo', 'edit_distance': 0},\n",
       " 'yoou': {'replace_with': 'you', 'edit_distance': 1},\n",
       " 'vilken': {'replace_with': 'viken', 'edit_distance': 1},\n",
       " 'sjlen': {'replace_with': 'salen', 'edit_distance': 1},\n",
       " 'irrevelant': {'replace_with': 'irrelevant', 'edit_distance': 2},\n",
       " '101%': {'replace_with': '101', 'edit_distance': 1},\n",
       " '102%': {'replace_with': '102', 'edit_distance': 1},\n",
       " '103%': {'replace_with': '103', 'edit_distance': 1},\n",
       " 'botling': {'replace_with': 'bowling', 'edit_distance': 1},\n",
       " '2+2=4': {'replace_with': '224', 'edit_distance': 2},\n",
       " 'corrrrrrrrrrect': {'replace_with': 'correct', 'edit_distance': 0},\n",
       " 'tryed': {'replace_with': 'tried', 'edit_distance': 1},\n",
       " 'sweeeeet': {'replace_with': 'sweet', 'edit_distance': 0},\n",
       " 'someting': {'replace_with': 'something', 'edit_distance': 1},\n",
       " 'reasurring': {'replace_with': 'recurring', 'edit_distance': 2},\n",
       " 'padwan': {'replace_with': 'parwan', 'edit_distance': 1},\n",
       " 'latez': {'replace_with': 'later', 'edit_distance': 1},\n",
       " 'mindreader': {'replace_with': 'mindbender', 'edit_distance': 2},\n",
       " 'awright': {'replace_with': 'alright', 'edit_distance': 1},\n",
       " \"g'night\": {'replace_with': 'gunfight', 'edit_distance': 2},\n",
       " 'linearevent': {'replace_with': 'lineament', 'edit_distance': 3},\n",
       " 'ulitmately': {'replace_with': 'ultimately', 'edit_distance': 2},\n",
       " \"'grok\": {'replace_with': \"'re\", 'edit_distance': 3},\n",
       " 'marrrrs': {'replace_with': 'marrs', 'edit_distance': 0},\n",
       " 'highlord': {'replace_with': 'highland', 'edit_distance': 2},\n",
       " 'errr': {'replace_with': 'err', 'edit_distance': 0},\n",
       " \"didn''t\": {'replace_with': 'didnt', 'edit_distance': 2},\n",
       " 'meeeep': {'replace_with': 'meep', 'edit_distance': 0},\n",
       " 'everythng': {'replace_with': 'everything', 'edit_distance': 1},\n",
       " \"someone's\": {'replace_with': 'someones', 'edit_distance': 1},\n",
       " \"'who\": {'replace_with': \"'s\", 'edit_distance': 3},\n",
       " \"isn't\": {'replace_with': 'isnt', 'edit_distance': 1},\n",
       " 'dissapeared': {'replace_with': 'disappeared', 'edit_distance': 2},\n",
       " 'answer)': {'replace_with': 'answer', 'edit_distance': 1},\n",
       " \"head'\": {'replace_with': 'head', 'edit_distance': 1},\n",
       " 'goatbeast': {'replace_with': 'goatees', 'edit_distance': 3},\n",
       " 'fwoosh': {'replace_with': 'frosh', 'edit_distance': 2},\n",
       " \"ai's\": {'replace_with': 'aids', 'edit_distance': 1},\n",
       " '0%': {'replace_with': '0', 'edit_distance': 1},\n",
       " 'compser': {'replace_with': 'composer', 'edit_distance': 1},\n",
       " 'howabout': {'replace_with': 'holdout', 'edit_distance': 3},\n",
       " 'baabababaaaaaaaa': {'replace_with': 'bambaataa', 'edit_distance': 3},\n",
       " 'pigions': {'replace_with': 'pigeons', 'edit_distance': 1},\n",
       " \"humanity's\": {'replace_with': 'humanity', 'edit_distance': 2},\n",
       " 'neeeeeeeeeeeee': {'replace_with': 'nee', 'edit_distance': 0},\n",
       " 'youve': {'replace_with': 'youde', 'edit_distance': 1},\n",
       " '1573#######': {'replace_with': '1573', 'edit_distance': 2},\n",
       " '####@###########': {'replace_with': '###', 'edit_distance': 2},\n",
       " 'mmmmmmmmm': {'replace_with': 'mm', 'edit_distance': 0},\n",
       " 'anyother': {'replace_with': 'another', 'edit_distance': 1},\n",
       " \"cow's\": {'replace_with': 'cows', 'edit_distance': 1},\n",
       " 'slithy': {'replace_with': 'smithy', 'edit_distance': 1},\n",
       " 'gire': {'replace_with': 'give', 'edit_distance': 1},\n",
       " 'momeraths': {'replace_with': 'moderates', 'edit_distance': 2},\n",
       " 'outgrabe': {'replace_with': 'outrage', 'edit_distance': 2},\n",
       " 'jubjub': {'replace_with': 'jujuy', 'edit_distance': 2},\n",
       " 'frumious': {'replace_with': 'furious', 'edit_distance': 2},\n",
       " 'maxome': {'replace_with': 'maxime', 'edit_distance': 1},\n",
       " 'tumtum': {'replace_with': 'tutu', 'edit_distance': 2},\n",
       " 'uffish': {'replace_with': 'uffizi', 'edit_distance': 2},\n",
       " 'whiffling': {'replace_with': 'whiffing', 'edit_distance': 1},\n",
       " 'tulgey': {'replace_with': 'tulley', 'edit_distance': 1},\n",
       " 'gallumping': {'replace_with': 'galloping', 'edit_distance': 2},\n",
       " \"'and\": {'replace_with': \"'d\", 'edit_distance': 2},\n",
       " 'slainthe': {'replace_with': 'sainte', 'edit_distance': 2},\n",
       " 'frabjous': {'replace_with': 'famous', 'edit_distance': 3},\n",
       " 'callooh': {'replace_with': 'callous', 'edit_distance': 2},\n",
       " 'callay': {'replace_with': 'callas', 'edit_distance': 1},\n",
       " 'subract': {'replace_with': 'subtract', 'edit_distance': 1},\n",
       " 'suspiscious': {'replace_with': 'suspicious', 'edit_distance': 1},\n",
       " 'beeep': {'replace_with': 'beep', 'edit_distance': 0},\n",
       " 'selfesteem': {'replace_with': 'self-esteem', 'edit_distance': 1},\n",
       " \"name's\": {'replace_with': 'names', 'edit_distance': 1},\n",
       " 'doubleentendre': {'replace_with': 'double-entendre', 'edit_distance': 1},\n",
       " '$5': {'replace_with': '$', 'edit_distance': 1},\n",
       " 'mwhahaha': {'replace_with': 'maharaja', 'edit_distance': 3},\n",
       " 'blissfull': {'replace_with': 'blissful', 'edit_distance': 1},\n",
       " \"'can\": {'replace_with': \"'n\", 'edit_distance': 2},\n",
       " 'ssssssssssssssssssssssss': {'replace_with': 'ss', 'edit_distance': 0},\n",
       " 'twentyone': {'replace_with': 'twenty-one', 'edit_distance': 1},\n",
       " \"mine's\": {'replace_with': 'mines', 'edit_distance': 1},\n",
       " 'shoggoth': {'replace_with': 'shoot', 'edit_distance': 3},\n",
       " 'shoggoths': {'replace_with': 'shoots', 'edit_distance': 3},\n",
       " \"that'll\": {'replace_with': 'thall', 'edit_distance': 2},\n",
       " 'lmao': {'replace_with': 'lao', 'edit_distance': 1},\n",
       " \"t's\": {'replace_with': 'tvs', 'edit_distance': 1},\n",
       " 'awwwwwwwwwww': {'replace_with': 'aww', 'edit_distance': 0},\n",
       " 'selfworth': {'replace_with': 'self-worth', 'edit_distance': 1},\n",
       " 'midswing': {'replace_with': 'mid-wing', 'edit_distance': 1},\n",
       " 'flmethrower': {'replace_with': 'flamethrower', 'edit_distance': 1},\n",
       " 'shortterm': {'replace_with': 'short-term', 'edit_distance': 1},\n",
       " 'defeatest': {'replace_with': 'defeatist', 'edit_distance': 1},\n",
       " \"i'd've\": {'replace_with': 'indie', 'edit_distance': 3},\n",
       " 'autoreply': {'replace_with': 'autopsy', 'edit_distance': 3},\n",
       " 'reenergise': {'replace_with': 'reenergize', 'edit_distance': 1},\n",
       " 'smoooooch': {'replace_with': 'smooch', 'edit_distance': 0},\n",
       " 'vaction': {'replace_with': 'vacation', 'edit_distance': 1},\n",
       " 'suuuuuure': {'replace_with': 'sure', 'edit_distance': 1},\n",
       " 'parfois': {'replace_with': 'paris', 'edit_distance': 2},\n",
       " \"doesn't\": {'replace_with': 'doesnt', 'edit_distance': 1},\n",
       " 'hehehe': {'replace_with': 'helene', 'edit_distance': 2},\n",
       " 'chatbots': {'replace_with': 'chariots', 'edit_distance': 2},\n",
       " \"nothing's\": {'replace_with': 'nothings', 'edit_distance': 1},\n",
       " 'jawas': {'replace_with': 'jaws', 'edit_distance': 1},\n",
       " \"jimmy's\": {'replace_with': 'jimmy', 'edit_distance': 2},\n",
       " \"elaine'\": {'replace_with': 'elaine', 'edit_distance': 1},\n",
       " 'from>': {'replace_with': 'from', 'edit_distance': 1},\n",
       " 'secrure': {'replace_with': 'secure', 'edit_distance': 1},\n",
       " 'pepperment': {'replace_with': 'peppermint', 'edit_distance': 1},\n",
       " 'eveyone': {'replace_with': 'everyone', 'edit_distance': 1},\n",
       " 'knockknock': {'replace_with': 'knickknack', 'edit_distance': 2},\n",
       " \"goodbye')\": {'replace_with': 'goodbye', 'edit_distance': 2},\n",
       " 'boohoo': {'replace_with': 'booboo', 'edit_distance': 1},\n",
       " \"wont't\": {'replace_with': 'wont', 'edit_distance': 2},\n",
       " 'povk': {'replace_with': 'pork', 'edit_distance': 1},\n",
       " 'sassing': {'replace_with': 'sansing', 'edit_distance': 1},\n",
       " 'ok<': {'replace_with': 'ok', 'edit_distance': 1},\n",
       " \"look'd\": {'replace_with': 'looked', 'edit_distance': 1},\n",
       " 'methought': {'replace_with': 'methodist', 'edit_distance': 3},\n",
       " 'halfhour': {'replace_with': 'half-hour', 'edit_distance': 1},\n",
       " \"'yes\": {'replace_with': \"'s\", 'edit_distance': 2},\n",
       " \"no'\": {'replace_with': 'not', 'edit_distance': 1},\n",
       " 'embarresed': {'replace_with': 'embarrassed', 'edit_distance': 2},\n",
       " 'fingerwork': {'replace_with': 'firework', 'edit_distance': 3}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "import jellyfish\n",
    "from  jellyfish import  damerau_levenshtein_distance as dlsim\n",
    "from  jellyfish import jaro_winkler as jwnk\n",
    "from jellyfish import nysiis\n",
    "\n",
    "import re\n",
    "\n",
    "def getMostSimilarWordsFor(not_found_words_dict, similarity = 'damerau_levenshtein_distance'):         \n",
    "    not_found_word_similar_to = {}\n",
    "    for not_found_word in not_found_words_dict.keys():\n",
    "        #######\n",
    "        # Replace > 3 char  occurence into 2 occurence\n",
    "        #####\n",
    "        not_found_word_uncorrected = not_found_word\n",
    "        not_found_word = re.sub(r'(.)\\1+', r'\\1\\1', not_found_word)\n",
    "        edit_distance = 10\n",
    "        jwnk_distance = 0.5\n",
    "        for glove_word in  words_in_glove:\n",
    "            try:                        \n",
    "                if similarity == 'levenshtein':\n",
    "                    lev_distance = Levenshtein.distance(not_found_word, glove_word)\n",
    "                    if not_found_word[0] == glove_word[0]  and \\\n",
    "                        lev_distance < 4 and lev_distance < edit_distance:\n",
    "                        edit_distance = lev_distance\n",
    "                        not_found_word_similar_to[not_found_word_uncorrected] = \\\n",
    "                                    { 'replace_with': glove_word, 'edit_distance': lev_distance}\n",
    "                elif similarity == 'damerau_levenshtein_distance':\n",
    "                    lev_distance = dlsim(not_found_word, glove_word)\n",
    "                    if not_found_word[0] == glove_word[0]  and \\\n",
    "                        lev_distance < 4 and lev_distance < edit_distance:\n",
    "                        edit_distance = lev_distance\n",
    "                        not_found_word_similar_to[not_found_word_uncorrected] = glove_word\n",
    "                elif similarity == 'jwnk':                    \n",
    "                    distance = jwnk(not_found_word, glove_word)\n",
    "                    if not_found_word[0] == glove_word[0]  and \\\n",
    "                         distance > jwnk_distance :\n",
    "                        jwnk_distance = distance                        \n",
    "                        not_found_word_similar_to[not_found_word_uncorrected] = glove_word\n",
    "                elif similarity == 'soundex':\n",
    "                    if jellyfish.soundex(not_found_word) == jellyfish.soundex(glove_word):\n",
    "                            not_found_word_similar_to[not_found_word_uncorrected] = glove_word\n",
    "                elif similarity == 'nysis':\n",
    "                    if nysiis(not_found_word) == nysiis(glove_word):\n",
    "                            not_found_word_similar_to[not_found_word_uncorrected] = glove_word\n",
    "            \n",
    "            except:# cannot perform match skip\n",
    "                continue\n",
    "    return not_found_word_similar_to\n",
    "    #return wordvec_embedding, words_found\n",
    "\n",
    "not_found_words_dict = {'maam': 1, 'k___': 7, 'gooooood': 1}\n",
    "not_found_words_dict = {'gooooood': 1, 'philolsophers': 1}\n",
    "#getMostSimilarWordsFor(not_found_words_dict,'levenshtein')\n",
    "most_close_match =  getMostSimilarWordsFor(not_found_words,'levenshtein')\n",
    "most_close_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Add perfect  matches and 1 edit distance away to corrector dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToCorrectorDict(most_close_match_dict, edit_distance_threshold, corrector_dict = None):\n",
    "    if  corrector_dict is None:\n",
    "        corrector_dict = {}\n",
    "    \n",
    "    for k in most_close_match.keys():        \n",
    "        if most_close_match[k]['edit_distance'] <= edit_distance_threshold:  \n",
    "            corrector_dict[k] = most_close_match[k]['replace_with']\n",
    "    return corrector_dict\n",
    "corrector_dict = addToCorrectorDict( most_close_match,  1, corrector_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add unk and pad to WordVec embedding as first two elements\n",
    "\n",
    "Because we want these to be the first embedding. We will later create word2id and id2word dict based on it and want pad to be of  0 index hence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_embedding = {}\n",
    "wordvec_embedding['_pad_'] = list( np.random.uniform(0, 0, 50 ) )\n",
    "wordvec_embedding['_unk_'] = list( np.random.uniform(-1.0, 1.0, 50 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun again with new  corrector dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrector dict len is 247\n",
      "\n",
      " Words  in text but not in glove =  241\n",
      "[('_name_', 28), (\"aren't\", 4), ('papagei', 3), (\"don't\", 3), (\"didn't\", 2), ('superthing', 2), ('nowroar', 2), ('sourwull', 2), ('honeymustard', 2), ('2+2=4', 2), (\"g'night\", 2), ('highlord', 2), ('howabout', 2), ('1573#######', 2), ('geekiest', 1), (\"diden't\", 1), ('hahahaha', 1), ('yuppers', 1), ('selfevident', 1), ('closemindedness', 1), ('closeminded', 1), (\"seethat's\", 1), ('srsly', 1), ('bloodtypes', 1), ('aaaaaaaaaa', 1), ('noumenal', 1), ('naturlich', 1), ('reconhece', 1), ('apesar', 1), ('discutir', 1), ('thiswhy', 1), ('mellifluity', 1), ('aleatorically', 1), ('pitchblack', 1), ('[keeps', 1), ('knowno', 1), (\"interweb's\", 1), ('ookay', 1), ('(half', 1), ('10foot', 1), ('overgeneralizations', 1), (\"'cheeky\", 1), ('goodybe', 1), ('dienstag', 1), ('pointess', 1), (\"wouln't\", 1), (\"gaaagooo'\", 1), ('gagagagagoo', 1), ('jurrasic', 1), ('gufwa', 1), ('hihihi', 1), ('hohoho', 1), ('fantabulous', 1), ('noteleportfield', 1), ('paries', 1), ('nooooooooooooooooooooo', 1), ('(david', 1), ('placego', 1), ('ahhhhhhhhhh', 1), ('yahh', 1), ('rawr', 1), ('specail', 1), ('fjjfw', 1), ('(marathon', 1), ('geooorgeee', 1), (\"jabberwacky's\", 1), ('riffic', 1), ('hahhaha', 1), ('=_=', 1), ('person/human', 1), ('percieve', 1), ('roflmao', 1), ('teehehehehehehe', 1), ('(and', 1), ('>(', 1), ('strawberryclock', 1), ('evilest', 1), ('eightyone', 1), ('xanthrope', 1), ('irrevelant', 1), ('omfgwtfroflollmao', 1), ('nd8hufdhuythbtoiueaomrpaek', 1), ('reasurring', 1), ('mindreader', 1), ('linearevent', 1), ('ulitmately', 1), (\"'grok\", 1), ('actofbeing', 1), (\"didn''t\", 1), ('meeeep', 1), ('backcornersitter', 1), (\"'who\", 1), (\"isn't\", 1), ('dissapeared', 1), ('(perfect', 1), ('clockworkorange', 1), ('goatbeast', 1), ('fwoosh', 1), ('baabababaaaaaaaa', 1), (\"humanity's\", 1), ('####@###########', 1), ('momeraths', 1), ('outgrabe', 1), ('jubjub', 1), ('frumious', 1), ('tumtum', 1), ('uffish', 1), ('gallumping', 1), (\"'and\", 1), ('slainthe', 1), ('frabjous', 1), ('callooh', 1), ('beeep', 1), ('selfesteem', 1), ('{{{{{{{{{{{{{{{you}}}}}}}}}}}}}}}}}}', 1), ('doubleentendre', 1), ('rabblerousers', 1), ('mwhahaha', 1), (\"'can\", 1), ('twentyone', 1), ('hahaha', 1), ('shoggoth', 1), ('shoggoths', 1), (\"that'll\", 1), ('_name_o', 1), ('lmao', 1), ('selfworth', 1), ('midswing', 1), ('shortterm', 1), ('chatbot', 1), ('blablablabla', 1), (\"i'd've\", 1), ('autoreply', 1), ('reenergise', 1), ('mwahahahaha', 1), ('smoooooch', 1), ('parfois', 1), (\"doesn't\", 1), ('hehehe', 1), ('chatbots', 1), (\"jimmy's\", 1), (\"elaine'\", 1), ('heebeejeebieme', 1), (\"knowthere's\", 1), ('knockknock', 1), (\"(that's\", 1), (\"goodbye')\", 1), ('(ironic)', 1), ('zigabytes', 1), ('boohoo', 1), (\"wont't\", 1), ('jabbershakingquaking', 1), ('methought', 1), ('halfhour', 1), (\"'yes\", 1), ('embarresed', 1), ('fingerwork', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_count, wordvec_embedding, not_found_words, X,Y = \\\n",
    "    rerunWordVecEmbedsPostCorrection(corrector_dict, wordvec_embedding, word_vec_embeddings_for_words_found_only = True )\n",
    "print('\\n Words  in text but not in glove = ', len(word_count) - len(words_found))\n",
    "print( getSortedDictElems( not_found_words) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of word embedding is : 3657\n",
      "WordVector embedding of dimensions : 50\n"
     ]
    }
   ],
   "source": [
    "#getDictElems( wordvec_embedding, 2)\n",
    "print(\"Length of word embedding is :\", len(wordvec_embedding) )\n",
    "print(\"WordVector embedding of dimensions :\",len(wordvec_embedding['the']) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word correction vs large text : TriGram + TF-IDF : TO DO\n",
    "1.  TO DO : For large words this distance metric may be exhaustive.\n",
    "        Soln : Create n-gram(3) character matrix +  TF-IDF to emphasise unique trigram combination  + find commonality or cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_name_': 28,\n",
       " 'geekiest': 1,\n",
       " \"diden't\": 1,\n",
       " 'hahahaha': 1,\n",
       " 'yuppers': 1,\n",
       " \"didn't\": 2,\n",
       " 'superthing': 2,\n",
       " 'selfevident': 1,\n",
       " 'closemindedness': 1,\n",
       " 'closeminded': 1,\n",
       " \"seethat's\": 1,\n",
       " 'srsly': 1,\n",
       " 'bloodtypes': 1,\n",
       " 'aaaaaaaaaa': 1,\n",
       " 'noumenal': 1,\n",
       " 'naturlich': 1,\n",
       " 'reconhece': 1,\n",
       " 'apesar': 1,\n",
       " 'discutir': 1,\n",
       " 'thiswhy': 1,\n",
       " 'mellifluity': 1,\n",
       " 'aleatorically': 1,\n",
       " 'pitchblack': 1,\n",
       " '[keeps': 1,\n",
       " 'knowno': 1,\n",
       " \"interweb's\": 1,\n",
       " 'ookay': 1,\n",
       " '(half': 1,\n",
       " '10foot': 1,\n",
       " 'overgeneralizations': 1,\n",
       " \"'cheeky\": 1,\n",
       " 'goodybe': 1,\n",
       " 'dienstag': 1,\n",
       " 'papagei': 3,\n",
       " 'pointess': 1,\n",
       " \"wouln't\": 1,\n",
       " \"gaaagooo'\": 1,\n",
       " 'gagagagagoo': 1,\n",
       " 'jurrasic': 1,\n",
       " 'gufwa': 1,\n",
       " 'hihihi': 1,\n",
       " 'hohoho': 1,\n",
       " 'fantabulous': 1,\n",
       " 'noteleportfield': 1,\n",
       " 'paries': 1,\n",
       " 'nooooooooooooooooooooo': 1,\n",
       " '(david': 1,\n",
       " 'placego': 1,\n",
       " \"don't\": 3,\n",
       " 'ahhhhhhhhhh': 1,\n",
       " 'yahh': 1,\n",
       " 'rawr': 1,\n",
       " 'nowroar': 2,\n",
       " 'specail': 1,\n",
       " 'fjjfw': 1,\n",
       " 'sourwull': 2,\n",
       " '(marathon': 1,\n",
       " 'geooorgeee': 1,\n",
       " \"jabberwacky's\": 1,\n",
       " 'riffic': 1,\n",
       " 'hahhaha': 1,\n",
       " '=_=': 1,\n",
       " 'person/human': 1,\n",
       " 'percieve': 1,\n",
       " 'roflmao': 1,\n",
       " 'teehehehehehehe': 1,\n",
       " 'honeymustard': 2,\n",
       " '(and': 1,\n",
       " \"aren't\": 4,\n",
       " '>(': 1,\n",
       " 'strawberryclock': 1,\n",
       " 'evilest': 1,\n",
       " 'eightyone': 1,\n",
       " 'xanthrope': 1,\n",
       " 'irrevelant': 1,\n",
       " '2+2=4': 2,\n",
       " 'omfgwtfroflollmao': 1,\n",
       " 'nd8hufdhuythbtoiueaomrpaek': 1,\n",
       " 'reasurring': 1,\n",
       " 'mindreader': 1,\n",
       " \"g'night\": 2,\n",
       " 'linearevent': 1,\n",
       " 'ulitmately': 1,\n",
       " \"'grok\": 1,\n",
       " 'actofbeing': 1,\n",
       " 'highlord': 2,\n",
       " \"didn''t\": 1,\n",
       " 'meeeep': 1,\n",
       " 'backcornersitter': 1,\n",
       " \"'who\": 1,\n",
       " \"isn't\": 1,\n",
       " 'dissapeared': 1,\n",
       " '(perfect': 1,\n",
       " 'clockworkorange': 1,\n",
       " 'goatbeast': 1,\n",
       " 'fwoosh': 1,\n",
       " 'howabout': 2,\n",
       " 'baabababaaaaaaaa': 1,\n",
       " \"humanity's\": 1,\n",
       " '1573#######': 2,\n",
       " '####@###########': 1,\n",
       " 'momeraths': 1,\n",
       " 'outgrabe': 1,\n",
       " 'jubjub': 1,\n",
       " 'frumious': 1,\n",
       " 'tumtum': 1,\n",
       " 'uffish': 1,\n",
       " 'gallumping': 1,\n",
       " \"'and\": 1,\n",
       " 'slainthe': 1,\n",
       " 'frabjous': 1,\n",
       " 'callooh': 1,\n",
       " 'beeep': 1,\n",
       " 'selfesteem': 1,\n",
       " '{{{{{{{{{{{{{{{you}}}}}}}}}}}}}}}}}}': 1,\n",
       " 'doubleentendre': 1,\n",
       " 'rabblerousers': 1,\n",
       " 'mwhahaha': 1,\n",
       " \"'can\": 1,\n",
       " 'twentyone': 1,\n",
       " 'hahaha': 1,\n",
       " 'shoggoth': 1,\n",
       " 'shoggoths': 1,\n",
       " \"that'll\": 1,\n",
       " '_name_o': 1,\n",
       " 'lmao': 1,\n",
       " 'selfworth': 1,\n",
       " 'midswing': 1,\n",
       " 'shortterm': 1,\n",
       " 'chatbot': 1,\n",
       " 'blablablabla': 1,\n",
       " \"i'd've\": 1,\n",
       " 'autoreply': 1,\n",
       " 'reenergise': 1,\n",
       " 'mwahahahaha': 1,\n",
       " 'smoooooch': 1,\n",
       " 'parfois': 1,\n",
       " \"doesn't\": 1,\n",
       " 'hehehe': 1,\n",
       " 'chatbots': 1,\n",
       " \"jimmy's\": 1,\n",
       " \"elaine'\": 1,\n",
       " 'heebeejeebieme': 1,\n",
       " \"knowthere's\": 1,\n",
       " 'knockknock': 1,\n",
       " \"(that's\": 1,\n",
       " \"goodbye')\": 1,\n",
       " '(ironic)': 1,\n",
       " 'zigabytes': 1,\n",
       " 'boohoo': 1,\n",
       " \"wont't\": 1,\n",
       " 'jabbershakingquaking': 1,\n",
       " 'methought': 1,\n",
       " 'halfhour': 1,\n",
       " \"'yes\": 1,\n",
       " 'embarresed': 1,\n",
       " 'fingerwork': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Add new words (Unfound) to Wordvec_embeddings\n",
    "\n",
    "1. WordVec_embeddings : Replaceall one occurence with UNK :: TO DO\n",
    "2. WordVec : Extend wordvec embedding to include these unknown words and 50d vector.\n",
    "3. Dictionary : Create Word2Id and Id2Word from new extended WordVec.\n",
    "4. Create input, encode, decode and target data\n",
    "5. Run it with script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embed = list( np.random.uniform(-1.0, 1.0, 50 ) )\n",
    "len(new_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length prior to new word addition is : 3657\n",
      "Embedding length post new word addition is : 3814\n"
     ]
    }
   ],
   "source": [
    "print('Embedding length prior to new word addition is :', len(wordvec_embedding))\n",
    "emb_dims = len(wordvec_embedding['the']) #or use static 50 if it fails\n",
    "for word in not_found_words:\n",
    "      # if word not in glove embedding, create random embed\n",
    "      wordvec_embedding[word] = list( np.random.uniform(-1.0, 1.0, 50 ) ) \n",
    "    \n",
    "print('Embedding length post new word addition is :', len(wordvec_embedding))   \n",
    "\n",
    "assert  '.'   in wordvec_embedding.keys() , 'Full stop(.) must be in embedding '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Create Word2id,  id2word dict from wordvec_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 id is :  [{'_pad_': 0, '_unk_': 1, 'the': 2, '.': 3}]\n",
      "Id 2 word is :  [{0: '_pad_', 1: '_unk_', 2: 'the', 3: '.'}]\n"
     ]
    }
   ],
   "source": [
    "def getWordIdDicts(wordvec_embedding):\n",
    "    idx = 0 \n",
    "    word2id, id2word = {}, {}\n",
    "    for word in wordvec_embedding:\n",
    "        word2id[word] = idx\n",
    "        id2word[idx] = word\n",
    "        idx +=1\n",
    "    return word2id, id2word\n",
    "\n",
    "word2id, id2word = getWordIdDicts(wordvec_embedding)\n",
    "\n",
    "print('Word 2 id is : ', getDictElems(word2id, 4))\n",
    "print('Id 2 word is : ',getDictElems(id2word, 4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Add Start_id , end_id  to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'start_id'  not in word2id.keys():\n",
    "    start_id = len(word2id)\n",
    "    end_id = len(word2id) + 1\n",
    "\n",
    "word2id['start_id'], word2id['end_id'] = start_id, end_id\n",
    "id2word[start_id], id2word[end_id] = 'start_id', 'end_id'\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Training WordVec_embedding (SMALL)  Vs Inference WordVec Embedding (ALL)\n",
    "\n",
    "1. **Training Wordvec  Embedding Small** : Because during training we do not need to do lookup across large embedding. We can use smaller embedding that consists of the words in the training  text only.\n",
    "2. **Prediction WordVec Embedding ALL** : Because we want the cht bot to  predict also the sentence not  already in the training text we need to use the larger or all wordvec embedding corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3814 3816 3816\n"
     ]
    }
   ],
   "source": [
    "print( len(wordvec_embedding), len(word2id), len(id2word) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrector dict len is 247\n",
      "400159 400159 400159\n",
      "Word 2 id is :  [{'_pad_': 0, '_unk_': 1, 'the': 2, '.': 3}]\n",
      "Id 2 word is :  [{0: '_pad_', 1: '_unk_', 2: 'the', 3: '.'}]\n"
     ]
    }
   ],
   "source": [
    "all_word_count, all_wordvec_embedding, all_not_found_words, all_X, all_Y = \\\n",
    "    rerunWordVecEmbedsPostCorrection(corrector_dict, wordvec_embedding, word_vec_embeddings_for_words_found_only = False )\n",
    "#print('\\n Words  in text but not in glove = ', len(all_word_count) - len(all_words_found))\n",
    "#print( getSortedDictElems( not_found_words) ) \n",
    "\n",
    "all_word2id, all_id2word =  getWordIdDicts(all_wordvec_embedding)\n",
    "\n",
    "print( len(all_wordvec_embedding), len(all_word2id), len(all_id2word) )\n",
    "\n",
    "print('Word 2 id is : ', getDictElems(word2id, 4))\n",
    "print('Id 2 word is : ',getDictElems(id2word, 4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertions : Basic Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "assert_check = True\n",
    "\n",
    "if assert_check :\n",
    "    assert len(all_wordvec_embedding.keys() ) ==  len(set( all_wordvec_embedding.keys() )),\\\n",
    "         \"Wordvec embeddings Must not contain duplicate keys, but duplicate key found \"\n",
    "\n",
    "    for all_key, key in zip(all_wordvec_embedding.keys(), wordvec_embedding.keys() ):\n",
    "        if all_key != key :\n",
    "            print( 'Keys at word  vec embeddings all and wordvec embedding must be at same Idx. But  not ound so. Fix it')\n",
    "    \n",
    "    print('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Input Data, Label\n",
    "\n",
    "1. X  : First line after '...'\n",
    "2. Y : All lines until  next '...'\n",
    "3. Convert X, Y to word_id  format for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is  [['are', 'you', 'real', '?'], ['i', 'would', 'marry', 'for', 'the', 'love', 'of', 'my', 'life']]\n",
      "Y is  [['no', '.', 'a', 'bot', 'is', 'real', '.', 'i', 'am', 'not', 'a', 'bot', '.', 'yes', 'you', 'are', '.', 'prove', 'it', '.', 'you', 'first', '.', 'i', 'can', 'not', 'prove', 'something', 'i', 'dont', 'believe', '.', 'prove', 'the', 'halting', 'problem', '.', 'you', 'first', '.', 'see', '?', 'you', 'are', 'not', '.', 'that', 'is', 'what', 'i', 'told', 'you', '.'], ['would', 'you', 'marry', 'me', '?', 'sure', '.', 'can', 'you', 'love', '?', 'of', 'course', '.', 'everyone', 'can', 'love', '.', 'how', 'did', 'you', 'learn', '?', 'i', 'learned', 'in', 'the', 'age', 'of', 'legends', '.', 'people', 'nowadays', 'know', 'so', 'little', '.', 'now', '?', 'lie', 'on', 'the', 'bed', '.', 'yes', 'madam', '.', 'giggles', '.', 'giggles', 'back', '.']]\n"
     ]
    }
   ],
   "source": [
    "print('X is ', X[0:2])\n",
    "print('Y is ', Y[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1  Input Data (words) to word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_wordid is : [[28, 72, 398, 163], [37, 49, 1844, 9, 2, 532, 4, 167, 181]]\n",
      "Y_wordid is : [[75, 3, 8, 2900, 12, 398, 3, 37, 575, 32, 8, 2900, 3, 1201, 72, 28, 3, 1213, 18, 3, 72, 53, 3, 37, 77, 32, 1213, 439, 37, 3216, 480, 3, 1213, 2, 2411, 490, 3, 72, 53, 3, 207, 163, 72, 28, 32, 3, 10, 12, 89, 37, 132, 72, 3], [49, 72, 1844, 229, 163, 642, 3, 77, 72, 532, 163, 4, 489, 3, 767, 77, 532, 3, 169, 106, 72, 1085, 163, 37, 1052, 7, 2, 344, 4, 2204, 3, 63, 2278, 269, 87, 260, 3, 101, 163, 1557, 11, 2, 1413, 3, 1201, 3035, 3, 3241, 3, 3241, 120, 3]]\n"
     ]
    }
   ],
   "source": [
    "X_wordid, Y_wordid = [], []\n",
    "for chat in X:\n",
    "    X_wordid.append( [ word2id[word] for word in chat] )\n",
    "\n",
    "for chat_response in Y:\n",
    "    Y_wordid.append( [ word2id[word] for word in chat_response ]  )\n",
    "        \n",
    "        \n",
    "print('X_wordid is :',X_wordid[0:2])\n",
    "print('Y_wordid is :',Y_wordid[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calc  Encoding  and  decoding  sequence length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn in seq mean & stdev : 7.145785876993166   3.8652324774187936\n",
      "rnn out seq mean & stdev : 111.35307517084283   109.5532684044888\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "## Calc mean and stdev to fix  rnn_seq_in_lenght and rnn_seq_out_length\n",
    "#####\n",
    "X_lengths = [len(lst) for lst in X]\n",
    "Y_lengths = [len(lst) for lst in Y]\n",
    "#print('X_length is ', X_lengths[:2])\n",
    "\n",
    "rnn_in_seq_mean = np.mean(X_lengths)\n",
    "rnn_in_seq_stdv = np.std(X_lengths)\n",
    "rnn_out_seq_mean = np.mean(Y_lengths)\n",
    "rnn_out_seq_stdv = np.std(Y_lengths)\n",
    "\n",
    "#print('Conv starter is ', conv_starter[:2])\n",
    "#print('Rnn out seq is ', responses[:2])\n",
    "print('rnn in seq mean & stdev :',  rnn_in_seq_mean, ' ', rnn_in_seq_stdv)\n",
    "print('rnn out seq mean & stdev :',  rnn_out_seq_mean, ' ', rnn_out_seq_stdv)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3 Calc Rnn in/out seq lengths @ 84% coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_in_seq_len =  11.01101835441196  .rnn_out_seq_len : 220.90634357533162\n"
     ]
    }
   ],
   "source": [
    "# 1 sd away from mean = 84% coverage\n",
    "rnn_in_seq_len =  rnn_in_seq_mean + rnn_in_seq_stdv\n",
    "rnn_out_seq_len = rnn_out_seq_mean + rnn_out_seq_stdv\n",
    "\n",
    "print('rnn_in_seq_len = ', rnn_in_seq_len, ' .rnn_out_seq_len :', rnn_out_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No of chats : 439 439\n",
      "Sample 2 trainX : [[28, 72, 398, 163], [37, 49, 1844, 9, 2, 532, 4, 167, 181]]\n",
      "Sample 2 trainY : [[75, 3, 8, 2900, 12, 398, 3, 37, 575, 32, 8, 2900, 3, 1201, 72, 28, 3, 1213, 18, 3, 72, 53, 3, 37, 77, 32, 1213, 439, 37, 3216, 480, 3, 1213, 2, 2411, 490, 3, 72, 53, 3, 207, 163, 72, 28, 32, 3, 10, 12, 89, 37, 132, 72, 3], [49, 72, 1844, 229, 163, 642, 3, 77, 72, 532, 163, 4, 489, 3, 767, 77, 532, 3, 169, 106, 72, 1085, 163, 37, 1052, 7, 2, 344, 4, 2204, 3, 63, 2278, 269, 87, 260, 3, 101, 163, 1557, 11, 2, 1413, 3, 1201, 3035, 3, 3241, 3, 3241, 120, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total No of chats :\", len(X_wordid), len(X))\n",
    "trainX, trainY = X_wordid, Y_wordid\n",
    "\n",
    "print(\"Sample 2 trainX :\", trainX[0:2])\n",
    "print(\"Sample 2 trainY :\", trainY[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of x 439 439\n"
     ]
    }
   ],
   "source": [
    "print('Length of x', len(trainX), len(trainY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Create Dataset : Batch major (Not Time major)\n",
    "\n",
    "1. Because seq2seq of tf.layer accepts training data in batches\n",
    "2. Encoding and decoding length fixed.\n",
    "\n",
    "Dataset should look like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  start_id :  3814  . end_id :  3815\n",
      "\n",
      "_encode_seqs :  [[89, 12, 298, 248, 163, 0, 0, 0, 0, 0, 0], [29, 72, 449, 46, 7, 532, 163, 0, 0, 0, 0]]\n",
      "\n",
      "_decode_seqs :  [[3814, 167, 248, 12, 3657, 3, 32, 163, 2, 3657, 3, 75, 3, 107, 3657, 3, 10, 12, 167, 248, 252, 3, 411, 163, 72, 297, 26, 2689, 3, 32, 411, 3, 89, 12, 298, 248, 163, 3657, 3, 10, 12, 167, 248, 252, 3, 3657, 3, 1201, 163, 72, 104, 88, 163, 37, 3216, 269, 89, 72, 869, 3, 1299, 1201, 72, 79, 163, 167, 769, 3, 72, 28, 2351, 229, 3, 10, 12, 185, 3, 75, 18, 3598, 3, 112, 89, 12, 185, 163, 89, 12, 298, 1207, 163, 167, 1207, 163, 37, 3216, 269, 37, 575, 80, 2238, 10, 12, 59, 205, 3, 77, 72, 754, 229, 439, 163, 1201, 163, 89, 79, 72, 241, 5, 269, 163, 37, 49, 104, 5, 269, 684, 3, 37, 77, 32, 754, 72, 684, 3, 124, 107, 345, 6, 687, 48, 39, 145, 115, 18, 3, 37, 3216, 269, 98, 5, 345, 3, 20, 2, 641, 3, 89, 12, 2, 641, 163, 89, 834, 44, 2, 641, 163, 2, 467, 3, 2, 2186, 163, 75, 3, 1201, 72, 28, 3, 89, 575, 37, 163, 8, 2900, 3, 483, 79, 72, 218, 10, 163, 100, 72, 28, 3, 89, 74, 37, 575, 8, 566, 163, 37, 575, 32, 642, 3, 37, 575, 642, 37, 575, 8, 566, 3, 75, 72, 28], [3814, 1201, 3, 56, 31, 72, 7, 532, 163, 37, 575, 127, 7, 532, 3, 34, 28, 72, 7, 532, 15, 163, 8, 887, 3, 35, 28, 3308, 3, 37, 218, 87, 252, 3, 87, 483, 3216, 72, 104, 88, 163, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "_target_seqs :  [[167, 248, 12, 3657, 3, 32, 163, 2, 3657, 3, 75, 3, 107, 3657, 3, 10, 12, 167, 248, 252, 3, 411, 163, 72, 297, 26, 2689, 3, 32, 411, 3, 89, 12, 298, 248, 163, 3657, 3, 10, 12, 167, 248, 252, 3, 3657, 3, 1201, 163, 72, 104, 88, 163, 37, 3216, 269, 89, 72, 869, 3, 1299, 1201, 72, 79, 163, 167, 769, 3, 72, 28, 2351, 229, 3, 10, 12, 185, 3, 75, 18, 3598, 3, 112, 89, 12, 185, 163, 89, 12, 298, 1207, 163, 167, 1207, 163, 37, 3216, 269, 37, 575, 80, 2238, 10, 12, 59, 205, 3, 77, 72, 754, 229, 439, 163, 1201, 163, 89, 79, 72, 241, 5, 269, 163, 37, 49, 104, 5, 269, 684, 3, 37, 77, 32, 754, 72, 684, 3, 124, 107, 345, 6, 687, 48, 39, 145, 115, 18, 3, 37, 3216, 269, 98, 5, 345, 3, 20, 2, 641, 3, 89, 12, 2, 641, 163, 89, 834, 44, 2, 641, 163, 2, 467, 3, 2, 2186, 163, 75, 3, 1201, 72, 28, 3, 89, 575, 37, 163, 8, 2900, 3, 483, 79, 72, 218, 10, 163, 100, 72, 28, 3, 89, 74, 37, 575, 8, 566, 163, 37, 575, 32, 642, 3, 37, 575, 642, 37, 575, 8, 566, 3, 75, 72, 28, 32], [1201, 3, 56, 31, 72, 7, 532, 163, 37, 575, 127, 7, 532, 3, 34, 28, 72, 7, 532, 15, 163, 8, 887, 3, 35, 28, 3308, 3, 37, 218, 87, 252, 3, 87, 483, 3216, 72, 104, 88, 163, 3815, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "_target_mask :  [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Encode_seq : ['How', 'are' , 'you', <PAD_ID>, <PAD_ID>]\n",
    "====================== Thought vectors =======================================\n",
    "Decode_seq : ['<START_ID>', 'I',  'am',   'fine',   <PAD_ID>, <PAD_ID>, <PAD_ID>, <PAD_ID>  ]\n",
    "Target_seq : [ 'I',         'am', 'fine', <END_ID>  <PAD_ID>, <PAD_ID>, <PAD_ID>, <PAD_ID>  ]\n",
    "Target_mask: [  1           ,1     ,1      ,1        ,0          ,0         ,0      ,0      ]\n",
    "\"\"\"\n",
    "\n",
    "def getEncodeNDecode(X,Y):    \n",
    "    #max_len_x = max([ len(chats) for chats in  X ]   )\n",
    "    #max_len_y = max([ len(response) for response in  Y ]   )\n",
    "    \n",
    "    max_len_x, max_len_y = 99999999,999999999\n",
    "    \n",
    "    max_padlen_x =  int(rnn_in_seq_len) if (int(rnn_in_seq_len) < max_len_x ) else max_len_x\n",
    "    max_padlen_y =  int(rnn_out_seq_len) if (int(rnn_out_seq_len) < max_len_y ) else max_len_y\n",
    "    \n",
    "    _encode_seqs = tl.prepro.pad_sequences(X, maxlen=max_padlen_x, \\\n",
    "                                padding='post', truncating='post', value=word2id['_pad_'] )\n",
    "    \n",
    "    _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n",
    "    _decode_seqs = tl.prepro.pad_sequences(_decode_seqs, maxlen=max_padlen_y,\\\n",
    "                                padding='post', truncating='post', value=word2id['_pad_'] )\n",
    "    \n",
    "    _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n",
    "    _target_seqs = tl.prepro.pad_sequences(_target_seqs, maxlen=max_padlen_y,\\\n",
    "                                padding='post', truncating='post', value=word2id['_pad_'])\n",
    "\n",
    "    _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n",
    "    \n",
    "    return _encode_seqs, _decode_seqs, _target_seqs, _target_mask\n",
    "    \n",
    "_encode_seqs, _decode_seqs, _target_seqs, _target_mask = getEncodeNDecode(\\\n",
    "                                                    X_wordid[3:5], Y_wordid[3:5])\n",
    "\n",
    "print('\\n  start_id : ',word2id['start_id'],' . end_id : ', word2id['end_id'])\n",
    "#print('\\n_encode_seqs : ',seqId2Words( _encode_seqs) )\n",
    "print('\\n_encode_seqs : ',_encode_seqs) \n",
    "print('\\n_decode_seqs : ',_decode_seqs)\n",
    "print('\\n_target_seqs : ',_target_seqs)\n",
    "print('\\n_target_mask : ',_target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['like', 'he', 'strong', 'free', '_pad_'],\n",
       " ['they', 'could', 'compared', 'first', 'a']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seqId2Words(seq, get_first_n_words_only = 5):\n",
    "    lst = []\n",
    "    for word_lst in seq:\n",
    "        #print(word_lst)\n",
    "        lst.append([ id2word[word_id] if word_id in id2word.keys() else word_id\\\n",
    "                    for word_id  in word_lst[0:get_first_n_words_only]  ] )            \n",
    "    return  lst\n",
    "\n",
    "lst = [[104, 16, 394, 313, 0, 0, 0, 0, 0], [35, 83, 663, 53, 8, 837, 0, 0, 0]]\n",
    "\n",
    "seqId2Words(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[104]\n",
    "word2id['.']\n",
    "#gl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Rnn Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Rnn param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 4\n",
    "xvocab_size = len(id2word) \n",
    "emb_dim = emb_dims\n",
    "rnn_num_layers = 1\n",
    "rnn_num_hidden_nodes = 32\n",
    "dropout_rate = 0.2\n",
    "\n",
    "\n",
    "#  decay learning rate by 25% every 10 step\n",
    "lr_decay_rate =  0.75\n",
    "lr_decay_steps = 10\n",
    "max_grad_clip_to = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Word Embedding Dict to Array\n",
    "\n",
    "Because : EmbeddingInput layers expects the word embeddings to be in array format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9556146577706295, -0.13992412012575106, -0.056778121857022246, -0.05578928587387999, -0.3326397804950836, -0.5815672313892881, -0.6897877349801806, -0.4392319148462587, 0.9596660690177858, 0.23127292264927224, -0.6544596923728019, -0.12045775254096824, 0.2859743500718541, 0.4461833420822854, -0.9079563483546558, 0.7783693189280061, -0.07089576084196958, -0.6940928407847575, 0.21880936735927614, -0.2122010949993265, -0.7695859257099906, -0.4434590950489752, -0.9335484021055345, 0.8009717392555136, -0.3273243569590827, 0.47691780200103917, 0.943103382941558, -0.7072788852353091, 0.8702251345225276, 0.8974103106471725, -0.12553697499315497, 0.44361548472735524, 0.660990804311048, 0.8158677075553986, -0.329341895832826, -0.5775949330171826, -0.8516761334488863, 0.6007208935613095, -0.4022877603386501, -0.6146227561615496, -0.139944125196364, 0.9257560521873955, -0.966060405462764, 0.2482672569088098, -0.9290878696682858, 0.4030733060441958, -0.202556971992075, 0.3689511710987152, 0.22797773294610213, -0.8471847437718358]]\n"
     ]
    }
   ],
   "source": [
    "def getWordEmbeddingMatrix(word2id, wordvec_embedding):\n",
    "    word_embedding_matrix = []\n",
    "    for  word in word2id:    \n",
    "        #print('word not found :', word)\n",
    "        if word not in ('start_id','end_id'):\n",
    "            word_embedding_matrix.append(list(wordvec_embedding[word]) )              \n",
    "    return word_embedding_matrix        \n",
    "\n",
    "word_embedding_matrix = getWordEmbeddingMatrix(word2id, wordvec_embedding)\n",
    "print( word_embedding_matrix[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9556146577706295, -0.13992412012575106, -0.056778121857022246]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_matrix[1][0:3] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Matrix : Inference / Prediction Purpose\n",
    "\n",
    "Creating full embedding matrix from embedding dictionary to be used in the decoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word_embedding_matrix =  3814 . all_word_embedding_matrix 400159\n"
     ]
    }
   ],
   "source": [
    "all_word_embedding_matrix = getWordEmbeddingMatrix(all_word2id, all_wordvec_embedding)\n",
    "\n",
    "print('length of word_embedding_matrix = ', len(word_embedding_matrix),\\\n",
    "       '. all_word_embedding_matrix', len(all_word_embedding_matrix))\n",
    "assert len(word_embedding_matrix) < len(all_word_embedding_matrix) /10 , \"Train Embed matrix must be smaller than Inference.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Tf Placeholder\n",
    "\n",
    "1. **Training Input Placeholder** :\n",
    "2. **Inference Input Placeholders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# model for training\n",
    "encode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\")\n",
    "decode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\")\n",
    "target_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\")\n",
    "target_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\") \n",
    "# tl.prepro.sequences_get_mask()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.2 RNN Many to Many Model Defn\n",
    "\n",
    "1. **Encoding layer**: Look up layer to lookup word id into embedding.\n",
    "2. **Decoding layer**: Look up layer to lookup word id into embedding.\n",
    "3. **Seq2Seq layer**: A simple dynamic Rnn layer.\n",
    "      \n",
    "\n",
    "REF: https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3814 50\n",
      "400159 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( len( word_embedding_matrix), len( word_embedding_matrix[0]) )\n",
    "print( len( all_word_embedding_matrix), len( all_word_embedding_matrix[0]) )\n",
    "rnn_in_seq_len\n",
    "encode_seq_len = int(rnn_in_seq_len)\n",
    "encode_seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# model for training\n",
    "encode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\")\n",
    "decode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\")\n",
    "target_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\")\n",
    "target_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\") \n",
    "# tl.prepro.sequences_get_mask()\n",
    "\n",
    "##################################\n",
    "\n",
    "def model(encode_seqs, decode_seqs, is_train=True, reuse=False):\n",
    "    with tf.variable_scope(\"model\", reuse=reuse):\n",
    "        encode_seq_len = int(rnn_in_seq_len)\n",
    "        decode_seq_len = int(rnn_out_seq_len)\n",
    "\n",
    "\n",
    "        embed_words, embed_word_dim = len( word_embedding_matrix) , len( word_embedding_matrix[0])\n",
    "        encoder_emb_layer = tf.get_variable(name=\"encoder_emb_layer\", shape=[embed_words, embed_word_dim],\\\n",
    "                        initializer=tf.constant_initializer(word_embedding_matrix), trainable=False)\n",
    "\n",
    "        ## Convert encode_seqs to  embedded_encode_seqs\n",
    "        ## [batch_size, seq_len] ===>>  [batch_size, seq_len, feat]\n",
    "        # STEPS: [batch_size, seq_len] ===>> [batch_size*seq_len,1] => embed_lookup => # [batch_size, seq_len, feat]\n",
    "        encoder_emb_inp = tf.reshape(encode_seqs, [batch_size*encode_seq_len,1])\n",
    "        encoder_emb_inp = tf.nn.embedding_lookup(encoder_emb_layer, encoder_emb_inp)\n",
    "        encoder_emb_inp = tf.reshape(encoder_emb_inp, [batch_size, encode_seq_len, emb_dim])\n",
    "\n",
    "\n",
    "        all_embed_words, all_embed_word_dim = len( all_word_embedding_matrix) , len( all_word_embedding_matrix[0])\n",
    "        decoder_emb_layer = tf.get_variable(name=\"decoder_emb_layer\", shape=[all_embed_words, all_embed_word_dim],\\\n",
    "                        initializer=tf.constant_initializer(all_word_embedding_matrix), trainable=False)\n",
    "\n",
    "        ## Repeat same process for decode seq\n",
    "        ## [batch_size, seq_len] ===>>  [batch_size, seq_len, feat]\n",
    "        # STEPS: [batch_size, seq_len] ===>> [batch_size*seq_len,1] => embed_lookup => # [batch_size, seq_len, feat]\n",
    "        decoder_emb_inp = tf.reshape(decode_seqs, [batch_size*decode_seq_len,1])\n",
    "        decoder_emb_inp = tf.nn.embedding_lookup(decoder_emb_layer, decoder_emb_inp)\n",
    "        decoder_emb_inp = tf.reshape(decoder_emb_inp, [batch_size, decode_seq_len, emb_dim])\n",
    "\n",
    "        #####################\n",
    "        ## Define Encoder\n",
    "        ####################\n",
    "        #encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=rnn_num_hidden_nodes)\n",
    "        encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=rnn_num_hidden_nodes)\n",
    "        initial_state = encoder_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "            encoder_cell, encoder_emb_inp, initial_state=initial_state,    \n",
    "            #sequence_length=[batch_size for _ in range(encode_seq_len)], \n",
    "            #sequence_length=[encode_seq_len for _ in range(batch_size)], \n",
    "            time_major=False, swap_memory=True)\n",
    "\n",
    "        #####################\n",
    "        ## Define Decoder\n",
    "        ####################\n",
    "        from tensorflow.python.layers.core import Dense\n",
    "        # Build RNN cell\n",
    "        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units= rnn_num_hidden_nodes)\n",
    "        projection_layer = Dense(units=xvocab_size, use_bias=True, activation=tf.nn.relu)\n",
    "        # A Helper to be used during training only.\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(  decoder_emb_inp,\\\n",
    "                        [ decode_seq_len for _ in range(batch_size)], time_major=False           )\n",
    "\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                decoder_cell, helper, encoder_state,  output_layer=projection_layer)\n",
    "        # Dynamic decoding\n",
    "        net_out, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder, output_time_major=False,  swap_memory=True  )\n",
    "\n",
    "    return net_out, _\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Model Train Defn\n",
    "\n",
    "VIMP : During training, the model should not be reused because we will be iteratively making it more and more better. However in test  case, we must set the model reuse to true, as we do want the previously trained model without any modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_out, _ = model(encode_seqs, decode_seqs, is_train=True, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'model_4/decoder/transpose:0' shape=(4, ?, 3816) dtype=float32>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model for inferencing\n",
    "encode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\n",
    "decode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\n",
    "net, net_rnn = model(encode_seqs2, decode_seqs2, is_train=False, reuse=True)\n",
    "#y = tf.nn.softmax(net.rnn_output)\n",
    "net.rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO : Compute cosine similarity of y with all_embedding_matrix to retrieve  predicted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Loss  Defn\n",
    "\n",
    "1. **Mask Loss**: is used. Mask is multiplied by calculated loss, to ensure padding losses are not accounted during loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'model/decoder/transpose:0' shape=(4, ?, 3816) dtype=float32>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_out.rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_classes = xvocab_size\n",
    "#logits =tf.reshape(net_out.rnn_output, [batch_size*decode_seq_len,n_classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "##logits : expected shape 2d [batch_size*:,n_classes]\n",
    "n_classes = xvocab_size\n",
    "logits =tf.reshape(net_out.rnn_output, [batch_size*decode_seq_len,n_classes])\n",
    "\n",
    "#  we calculate  cross entropy with mask \n",
    "# because we do not want  the padding to have any effect on the lonss\n",
    "loss = tl.cost.cross_entropy_seq_with_mask(logits=logits, target_seqs=target_seqs,\\\n",
    "                                    input_mask=target_mask, return_details=False, name='cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Optimizer Defn : Decaying rate & Grad Clip\n",
    "\n",
    "1. **Vanishing Gradient :** Since we are using relu, vanishing gradient is not a  big problem. \n",
    "2. **Exploding Gradient :** : Clipping gradients to prevent the problem\n",
    "3. **Optimiser :** Adam Optimisers tend to converge faster. SGD although slow tend to outperfom Adam however\n",
    "4. **Decaying learning rate**: For adam it is not required, because ADAM guarantees square root decay as per theorem 4.1 \n",
    "\n",
    "t <- t +1\n",
    "\n",
    "lr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)\n",
    "\n",
    "PS:  Theorem 4.1 of their ICLR article, one of their hypotheses is that the learning rate has a square root decay, t=/t. Furthermore, for their logistic regression experiments they use the square root decay as well. \n",
    "Ref: https://stats.stackexchange.com/questions/200063/adam-optimizer-with-exponential-decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate : 0.0001\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "\n",
    "# Learning rate decay ~ lr * decay_rate ^(global_step/ decay_step)\n",
    "print('learning rate :', learning_rate)\n",
    "#learning_rate = tf.train.exponential_decay(\n",
    "#    lr, global_step, decay_steps=lr_decay_steps,\\\n",
    "#    decay_rate=lr_decay_rate, staircase=True)\n",
    "\n",
    "#lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "# We define Adam Optimizer\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Gradient clipping\n",
    "#gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "#gradients, _ = tf.clip_by_global_norm(gradients, max_grad_clip_to)\n",
    "#optimizer = optimizer.apply_gradients(\n",
    "#    zip(gradients, v))\n",
    "\n",
    "#train_op = optimizer\n",
    "\n",
    "#net_out.print_params(False)\n",
    "#train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX)\n",
    "len(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[28, 72, 398, 163],\n",
       " [37, 49, 1844, 9, 2, 532, 4, 167, 181],\n",
       " [34, 12, 2, 2044, 163],\n",
       " [89, 12, 298, 248, 163],\n",
       " [29, 72, 449, 46, 7, 532, 163],\n",
       " [77, 72, 2968, 439, 163],\n",
       " [77, 72, 2265, 229, 55, 163],\n",
       " [29, 72, 1380, 5, 442, 163],\n",
       " [37, 218, 10, 72, 28, 8, 586, 284, 457, 5, 1544, 229, 3],\n",
       " [1600, 3, 107, 3216, 3030, 229, 3],\n",
       " [169, 149, 29, 48, 46, 2758, 163],\n",
       " [89, 212, 18, 869, 5, 26, 348, 163],\n",
       " [48, 164, 638, 54, 532, 69, 51, 3],\n",
       " [169, 157, 79, 72, 2081, 163],\n",
       " [169, 157, 2008, 79, 72, 29, 163],\n",
       " [28, 72, 8, 2111, 163],\n",
       " [89, 12, 298, 248, 163],\n",
       " [79, 72, 104, 240, 163],\n",
       " [30, 1042, 3],\n",
       " [79, 72, 532, 229, 163, 252, 163],\n",
       " [37, 575, 1805, 3],\n",
       " [30, 37, 241, 69, 1497, 3],\n",
       " [37, 575, 681, 5, 2907, 3],\n",
       " [37, 83, 26, 298, 769, 3],\n",
       " [12, 18, 8, 1512, 163],\n",
       " [79, 72, 269, 163],\n",
       " [79, 72, 104, 649, 54, 229, 163],\n",
       " [1497, 3],\n",
       " [37, 107, 3216, 921, 72, 388, 3],\n",
       " [37, 80, 568, 72, 1550, 3],\n",
       " [181, 3598, 905, 3],\n",
       " [72, 28, 107, 25, 2776, 3],\n",
       " [167, 586, 12, 986, 3],\n",
       " [124, 163, 39, 63, 218, 334, 4, 229, 107, 100, 37, 785, 2560, 2704, 163],\n",
       " [181, 12, 181],\n",
       " [48,\n",
       "  3317,\n",
       "  327,\n",
       "  761,\n",
       "  100,\n",
       "  48,\n",
       "  3317,\n",
       "  268,\n",
       "  649,\n",
       "  3,\n",
       "  559,\n",
       "  84,\n",
       "  20,\n",
       "  263,\n",
       "  340,\n",
       "  1632,\n",
       "  2,\n",
       "  618,\n",
       "  5,\n",
       "  136,\n",
       "  69,\n",
       "  761,\n",
       "  559,\n",
       "  84,\n",
       "  173,\n",
       "  445,\n",
       "  3],\n",
       " [29, 72, 46, 5, 2, 1333, 163],\n",
       " [28, 72, 1054, 55, 15, 167, 1910, 120, 163],\n",
       " [72, 28, 2351, 229, 101, 3],\n",
       " [167, 769, 3657, 178, 72, 28, 25, 2816, 2299, 3],\n",
       " [72, 312, 26, 936, 5, 2, 906, 566, 3, 3, 3],\n",
       " [100, 37, 587, 72, 3],\n",
       " [37, 269, 8, 166, 185, 548, 887, 7, 277, 3],\n",
       " [34, 28, 72, 163],\n",
       " [37, 3216, 129, 269, 72, 3],\n",
       " [49, 72, 411, 241, 5, 136, 909, 163],\n",
       " [738, 335, 14, 163, 58, 12, 75, 532, 80, 1498, 4, 532, 3],\n",
       " [37,\n",
       "  575,\n",
       "  8,\n",
       "  348,\n",
       "  3,\n",
       "  72,\n",
       "  28,\n",
       "  8,\n",
       "  586,\n",
       "  3,\n",
       "  72,\n",
       "  1379,\n",
       "  7,\n",
       "  586,\n",
       "  614,\n",
       "  3,\n",
       "  37,\n",
       "  1379,\n",
       "  7,\n",
       "  2,\n",
       "  1027,\n",
       "  76,\n",
       "  3],\n",
       " [2437, 3598, 188, 124, 163, 12, 18, 163],\n",
       " [298,\n",
       "  880,\n",
       "  2398,\n",
       "  43,\n",
       "  4,\n",
       "  1726,\n",
       "  3322,\n",
       "  163,\n",
       "  3155,\n",
       "  3366,\n",
       "  703,\n",
       "  2690,\n",
       "  3128,\n",
       "  9,\n",
       "  1963,\n",
       "  7,\n",
       "  1766,\n",
       "  3],\n",
       " [96, 63, 638, 7, 40, 1497, 3],\n",
       " [28, 63, 333, 65, 1396, 163],\n",
       " [37, 3216, 269, 163, 30, 200, 3280, 69, 1549, 121, 5, 69, 62, 887, 3],\n",
       " [75, 37, 575, 188, 5, 1033, 3],\n",
       " [72, 297, 26, 557, 1838, 3],\n",
       " [3123, 2753, 6, 1256, 4, 3288, 1833, 3, 3, 3],\n",
       " [2886, 1894, 12, 17, 607, 17, 2886, 516, 3],\n",
       " [1601,\n",
       "  10,\n",
       "  33,\n",
       "  252,\n",
       "  252,\n",
       "  3228,\n",
       "  49,\n",
       "  2533,\n",
       "  163,\n",
       "  2730,\n",
       "  6,\n",
       "  1294,\n",
       "  611,\n",
       "  67,\n",
       "  8,\n",
       "  2962,\n",
       "  3],\n",
       " [37,\n",
       "  1353,\n",
       "  9,\n",
       "  583,\n",
       "  30,\n",
       "  10,\n",
       "  72,\n",
       "  312,\n",
       "  749,\n",
       "  163,\n",
       "  749,\n",
       "  11,\n",
       "  298,\n",
       "  213,\n",
       "  388,\n",
       "  15,\n",
       "  3630,\n",
       "  222,\n",
       "  3],\n",
       " [754, 229, 439, 51, 23, 298, 2106, 3],\n",
       " [37, 1353, 37, 13, 8, 2140, 3],\n",
       " [89, 54, 234, 163],\n",
       " [754, 229, 439, 54, 1619, 3],\n",
       " [12, 583, 439, 163],\n",
       " [112, 37, 529, 72, 3563, 157, 1789, 7, 2, 275, 406, 3],\n",
       " [1113, 163, 89, 28, 72, 936, 54, 163],\n",
       " [72, 218, 48, 28, 648, 163],\n",
       " [72, 28, 8, 333, 3504, 74, 37, 3216, 1610, 5, 72, 3],\n",
       " [77, 72, 2932, 2, 886, 4, 815, 163],\n",
       " [169, 143, 28, 72, 163],\n",
       " [37, 575, 188, 5, 1413, 475, 3],\n",
       " [3689, 12, 3690, 3],\n",
       " [33, 12, 2257, 3],\n",
       " [58, 12, 43, 608, 3],\n",
       " [1201,\n",
       "  163,\n",
       "  124,\n",
       "  163,\n",
       "  37,\n",
       "  575,\n",
       "  642,\n",
       "  72,\n",
       "  28,\n",
       "  8,\n",
       "  314,\n",
       "  15,\n",
       "  2,\n",
       "  1915,\n",
       "  3,\n",
       "  28,\n",
       "  72,\n",
       "  8,\n",
       "  2121,\n",
       "  728,\n",
       "  163],\n",
       " [124,\n",
       "  163,\n",
       "  167,\n",
       "  1263,\n",
       "  12,\n",
       "  946,\n",
       "  2344,\n",
       "  163,\n",
       "  30,\n",
       "  325,\n",
       "  6,\n",
       "  1916,\n",
       "  28,\n",
       "  8,\n",
       "  185,\n",
       "  2377,\n",
       "  9,\n",
       "  10,\n",
       "  3],\n",
       " [77, 72, 459, 1659, 163],\n",
       " [72, 28, 8, 1530, 2902, 163, 3521, 3],\n",
       " [37, 609, 2764, 3],\n",
       " [72, 28, 1655, 3],\n",
       " [18, 3523, 79, 229, 114, 185, 5, 32, 104, 2629, 3],\n",
       " [2550, 163, 559, 84, 207, 74, 72, 77, 1183, 3, 89, 1685, 575, 37, 163],\n",
       " [559, 84, 32, 638, 54, 229, 59, 2, 70, 89, 79, 72, 79, 9, 8, 494, 163],\n",
       " [72, 136, 2391, 450, 104, 1464, 2669, 163, 6, 2391, 283, 7, 8, 2343, 3237, 3],\n",
       " [37, 3440, 163, 754, 229, 8, 1922, 3],\n",
       " [37, 575, 1805, 10, 2, 1512, 3598, 2184, 72, 3],\n",
       " [79, 72, 269, 1427, 3657, 163],\n",
       " [2, 1926, 248, 12, 1936, 163, 10, 12, 1451, 3],\n",
       " [72, 28, 2, 85, 1658, 566, 37, 29, 449, 520, 3],\n",
       " [185, 2369, 3],\n",
       " [79, 72, 241, 5, 1085, 163],\n",
       " [2015, 638, 54, 439, 881, 3],\n",
       " [2257, 3],\n",
       " [411, 3, 3, 3, 1451, 3, 3, 37, 516, 10, 72, 31, 8, 586, 3, 3],\n",
       " [559, 84, 136, 532, 163, 32, 119, 3],\n",
       " [8, 2900, 12, 2, 182, 17, 8, 348, 163, 80, 15, 358, 2083, 3],\n",
       " [37, 1736, 436, 955, 3],\n",
       " [70, 9, 229, 5, 200, 112, 3],\n",
       " [89, 28, 72, 369, 9, 1757, 163],\n",
       " [79, 72, 1379, 163],\n",
       " [1670, 48, 1050, 163],\n",
       " [79, 72, 480, 7, 815, 163],\n",
       " [37, 480, 72, 28, 8, 1081, 3],\n",
       " [37, 241, 5, 638, 54, 1005, 72, 3],\n",
       " [2481, 79, 72, 241, 5, 200, 60, 9, 1325, 1524, 163],\n",
       " [79, 72, 104, 2319, 737, 163],\n",
       " [37, 575, 1805, 3, 37, 3440, 869, 5, 995, 298, 1635, 3],\n",
       " [37, 104, 1343, 3],\n",
       " [87, 163, 89, 106, 72, 29, 9, 1476, 163],\n",
       " [89, 29, 37, 491, 10, 12, 2085, 163],\n",
       " [414, 5, 458, 8, 181, 12, 8, 608, 163],\n",
       " [6,\n",
       "  2,\n",
       "  51,\n",
       "  3468,\n",
       "  24,\n",
       "  961,\n",
       "  28,\n",
       "  5,\n",
       "  2,\n",
       "  830,\n",
       "  2,\n",
       "  51,\n",
       "  2024,\n",
       "  6,\n",
       "  2,\n",
       "  770,\n",
       "  2402,\n",
       "  7,\n",
       "  88,\n",
       "  39,\n",
       "  16,\n",
       "  895,\n",
       "  3479,\n",
       "  163],\n",
       " [1600, 163, 559, 84, 537, 5, 827, 3013, 3],\n",
       " [28, 72, 8, 239, 41, 8, 508, 163],\n",
       " [72, 29, 75, 2428, 169, 5, 2934, 3222, 15, 63, 3],\n",
       " [298, 2520, 3],\n",
       " [684, 12, 779, 30, 58, 28, 69, 445, 10, 943, 5, 2806, 43, 146, 3],\n",
       " [212, 70, 1379, 163],\n",
       " [29, 583, 5, 173, 3],\n",
       " [37, 575, 626, 163, 145, 229, 60, 4, 162, 3],\n",
       " [169, 96, 2755, 79, 72, 29, 163],\n",
       " [1600, 163, 70, 9, 1476],\n",
       " [124, 37, 575, 205, 163, 3552, 37, 163],\n",
       " [28, 72, 889, 163],\n",
       " [1690, 163, 169, 29, 72, 46, 163],\n",
       " [98, 881, 164, 35, 26, 163],\n",
       " [169, 143, 575, 37, 163],\n",
       " [16, 12, 185, 104, 10, 3],\n",
       " [16, 12, 8, 239, 3],\n",
       " [37, 104, 955, 3],\n",
       " [3097,\n",
       "  12,\n",
       "  2,\n",
       "  3596,\n",
       "  4,\n",
       "  85,\n",
       "  580,\n",
       "  1145,\n",
       "  163,\n",
       "  294,\n",
       "  15,\n",
       "  64,\n",
       "  1545,\n",
       "  695,\n",
       "  163,\n",
       "  3074,\n",
       "  3],\n",
       " [169, 143, 28, 72, 163],\n",
       " [89, 12, 2, 1022, 5, 181, 163, 2, 1656, 6, 684, 163],\n",
       " [77, 48, 345, 66, 163],\n",
       " [2, 1826, 12, 151, 824, 163],\n",
       " [3590, 936, 3075, 3],\n",
       " [483, 79, 72, 781, 229, 163],\n",
       " [72, 36, 2, 182, 516, 163],\n",
       " [169, 79, 72, 450, 163],\n",
       " [28, 72, 642, 72, 28, 348, 163],\n",
       " [1080],\n",
       " [72, 28, 32, 167, 769],\n",
       " [98, 106, 72, 200, 11, 298, 1737, 163],\n",
       " [72, 28, 936, 5, 229, 3],\n",
       " [167, 3295, 12, 386, 4, 2983, 3],\n",
       " [79, 72, 532, 229, 163],\n",
       " [79, 72, 29, 114, 1858, 11, 169, 37, 77, 518, 2387, 163, 2162, 163],\n",
       " [1600, 3, 22, 37, 532, 72, 163, 2162, 3],\n",
       " [1201, 3, 79, 72, 104, 298, 248, 163],\n",
       " [89, 79, 63, 173, 54, 72, 163],\n",
       " [79, 229, 8, 1132, 3],\n",
       " [3717, 3],\n",
       " [1697, 2900, 3, 3, 3],\n",
       " [37, 575, 8, 728, 3],\n",
       " [87, 107, 100, 37, 789, 445, 2512, 163, 72, 218, 37, 575, 8, 922, 163],\n",
       " [72, 28, 1593, 11, 33, 1134],\n",
       " [72, 77, 3, 3, 3, 107, 79, 18, 3],\n",
       " [28, 72, 8, 398, 566, 163, 100, 37, 575, 163, 169, 49, 48, 269, 163],\n",
       " [169, 49, 37, 208, 2531, 72, 163],\n",
       " [79, 72, 29, 114, 761, 4, 1784, 163],\n",
       " [89, 578, 4, 590, 1964, 106, 72, 29, 163],\n",
       " [37, 575, 8, 566, 163, 2158, 6, 782, 3],\n",
       " [34, 103, 72, 163],\n",
       " [1416, 81, 298, 490, 3],\n",
       " [72, 28, 32, 157, 9, 1512, 286, 3],\n",
       " [28, 72, 8, 586, 163],\n",
       " [37, 575, 1130, 7, 1869, 163, 1953, 163, 6, 2001, 3],\n",
       " [63, 669, 145, 89, 12, 505, 5, 88, 3, 3, 3, 1013, 18, 27, 46, 2294, 3],\n",
       " [89, 49, 72, 104, 5, 1554, 163],\n",
       " [28, 72, 2274, 163],\n",
       " [49, 72, 104, 5, 26, 845, 163],\n",
       " [37, 104, 117, 2249, 3],\n",
       " [79, 72, 480, 7, 815, 163],\n",
       " [2559, 3],\n",
       " [259, 651, 5, 26, 8, 2074, 30, 37, 39, 405, 72, 442, 3],\n",
       " [89, 12, 298, 248, 163],\n",
       " [49, 72, 104, 5, 26, 348, 3],\n",
       " [37, 781, 72, 5, 8, 2400],\n",
       " [735, 3],\n",
       " [2416, 3],\n",
       " [2207, 3],\n",
       " [754, 229, 8, 373, 3],\n",
       " [37, 575, 8, 2317, 3],\n",
       " [754, 229, 8, 373, 54, 8, 1826, 3],\n",
       " [2420, 1622, 3],\n",
       " [754, 229, 8, 373, 3372, 3],\n",
       " [72, 28, 167, 166, 208, 769, 3],\n",
       " [37, 1676, 15, 251, 1676, 3],\n",
       " [79, 72, 480, 10, 1853, 707, 12, 8, 185, 2824, 163],\n",
       " [169, 28, 72, 1125, 163, 286, 163],\n",
       " [89, 164, 37, 26, 576, 205, 101, 163],\n",
       " [79, 72, 218, 37, 575, 1071, 41, 849, 163],\n",
       " [28, 72, 8, 398, 566, 41, 28, 72, 8, 586, 163],\n",
       " [298, 248, 12, 163, 163],\n",
       " [2550, 163, 1805, 3],\n",
       " [89, 79, 72, 218, 2, 1850, 12, 163],\n",
       " [169, 212, 8, 586, 1383, 1789, 163],\n",
       " [89, 79, 72, 269, 54, 815, 163],\n",
       " [89, 28, 72, 163],\n",
       " [72, 3216, 136, 761, 3],\n",
       " [483, 79, 72, 218, 37, 575, 8, 2900, 163],\n",
       " [37, 269, 72, 28, 163, 30, 89, 575, 37, 163],\n",
       " [173, 439, 2318, 3],\n",
       " [37, 3216, 241, 5, 26, 8, 2738, 3],\n",
       " [1931, 7, 928, 3, 10, 12, 166, 605, 1099, 3],\n",
       " [37,\n",
       "  575,\n",
       "  1805,\n",
       "  3,\n",
       "  169,\n",
       "  157,\n",
       "  12,\n",
       "  2,\n",
       "  718,\n",
       "  1694,\n",
       "  4,\n",
       "  3729,\n",
       "  203,\n",
       "  2,\n",
       "  718,\n",
       "  4,\n",
       "  50,\n",
       "  163],\n",
       " [89, 212, 1035, 229, 749, 9, 163],\n",
       " [37, 104, 3003, 3],\n",
       " [1089, 1684, 15, 72, 3],\n",
       " [169, 79, 37, 145, 1126, 163],\n",
       " [37, 213, 8, 70, 1081, 3],\n",
       " [2664, 3, 3, 2608, 3],\n",
       " [167, 248, 12, 2368, 3],\n",
       " [2865, 3],\n",
       " [2785, 55, 23, 24, 2070, 3],\n",
       " [77, 72, 1033, 866, 163],\n",
       " [169, 28, 72, 163],\n",
       " [37, 3216, 1033, 866, 3],\n",
       " [79, 72, 29, 8, 761, 4, 1784, 163],\n",
       " [34, 79, 37, 2161, 72, 4, 163],\n",
       " [483, 212, 8, 3102, 1415, 152, 1041, 74, 16, 775, 5, 1497, 163],\n",
       " [37, 575, 8, 2900, 3],\n",
       " [49, 72, 1051, 2, 2636, 811, 196, 5, 196, 163],\n",
       " [79, 72, 29, 860, 322, 1020, 163],\n",
       " [37, 575, 298, 1712, 3],\n",
       " [1313, 29, 1570, 3],\n",
       " [50, 3],\n",
       " [89, 28, 72, 163],\n",
       " [75, 37, 575, 8, 2111, 3],\n",
       " [79, 72, 1353, 72, 83, 1885, 163],\n",
       " [169, 27, 298, 108, 46, 163],\n",
       " [37, 575, 8, 348, 3],\n",
       " [72, 28, 8, 2584, 343, 284, 163, 30, 72, 39, 1085, 167, 343, 3611, 3],\n",
       " [18, 12, 1701, 15, 688, 1589, 3],\n",
       " [72, 995, 167, 1635, 3],\n",
       " [124, 37, 575, 2576, 3],\n",
       " [216, 39, 1862, 72],\n",
       " [37, 480, 10, 37, 79, 1379, 3],\n",
       " [37, 575, 7, 532, 15, 8, 887, 3],\n",
       " [37, 1627, 101, 49, 26, 8, 274, 70, 3],\n",
       " [1600, 163, 79, 72, 241, 5, 537, 379, 696, 163],\n",
       " [89, 79, 72, 218, 54, 2, 847, 20, 2, 841, 163],\n",
       " [3521,\n",
       "  3472,\n",
       "  8,\n",
       "  1162,\n",
       "  1154,\n",
       "  7,\n",
       "  8,\n",
       "  990,\n",
       "  6,\n",
       "  58,\n",
       "  12,\n",
       "  75,\n",
       "  43,\n",
       "  58,\n",
       "  5,\n",
       "  1051,\n",
       "  18,\n",
       "  163,\n",
       "  212,\n",
       "  18,\n",
       "  136,\n",
       "  8,\n",
       "  804,\n",
       "  163],\n",
       " [39, 586, 2001, 43, 108, 165, 66, 2, 76, 163],\n",
       " [37, 127, 3216, 269, 34, 37, 575, 9, 642, 3],\n",
       " [642, 3, 532, 229, 53, 3],\n",
       " [37, 575, 20, 139, 3, 72, 163],\n",
       " [79, 72, 1359, 5, 26, 313, 23, 33, 1081, 163, 5, 850, 60, 67, 321, 163],\n",
       " [1201, 3, 37, 3589, 752, 3],\n",
       " [77, 37, 145, 298, 189, 163],\n",
       " [37, 575, 1151],\n",
       " [2369, 3],\n",
       " [3744, 10, 12, 167, 765, 32, 8, 248],\n",
       " [811, 59, 2022, 3476],\n",
       " [37, 104, 1928, 3],\n",
       " [79, 72, 104, 229, 163],\n",
       " [89, 823, 4, 566, 28, 72, 163],\n",
       " [28, 72, 398, 163],\n",
       " [405, 3, 229, 3, 3363, 3],\n",
       " [2414, 2303, 3],\n",
       " [37, 575, 936, 5, 738, 45, 101, 3, 1710, 72, 1710, 72, 1710, 72, 3],\n",
       " [72, 28, 8, 586, 3],\n",
       " [79, 72, 941, 229, 163],\n",
       " [340, 229, 59, 298, 3245, 3],\n",
       " [3073, 163, 37, 241, 5, 602, 8, 1643, 1722, 3],\n",
       " [37, 207, 169, 420, 18, 12, 117, 8, 3288, 101, 3],\n",
       " [79, 3104, 29, 742, 163],\n",
       " [483, 79, 3318, 1383, 63, 163],\n",
       " [72, 83, 32, 129, 726, 2, 3077, 477, 3],\n",
       " [754, 229, 163, 39, 37, 449, 1489, 167, 1712, 163],\n",
       " [167, 248, 12, 1671, 3260, 6, 37, 575, 8, 2625, 23, 2, 408, 4, 1198, 2826, 3],\n",
       " [28, 72, 701, 163],\n",
       " [169, 143, 28, 72, 163],\n",
       " [28, 72, 8, 1081, 163],\n",
       " [89, 77, 72, 79, 9, 229, 163],\n",
       " [173, 439, 1697, 3],\n",
       " [79, 72, 218, 163],\n",
       " [1600, 559, 84, 227, 3],\n",
       " [3105, 3],\n",
       " [1853, 707, 12, 398, 3],\n",
       " [75, 643, 3],\n",
       " [169, 143, 28, 72, 163],\n",
       " [28, 72, 8, 1081, 163],\n",
       " [89, 77, 72, 79, 9, 229, 163],\n",
       " [173, 439, 1697, 3],\n",
       " [79, 72, 218, 163],\n",
       " [1600, 559, 84, 227, 3],\n",
       " [3105, 3],\n",
       " [1853, 707, 12, 398, 3],\n",
       " [75, 643, 3],\n",
       " [169, 143, 28, 72, 112, 163],\n",
       " [72, 28, 591, 205, 3],\n",
       " [77, 37, 754, 72, 8, 902, 163],\n",
       " [89, 28, 72, 163],\n",
       " [72, 28, 1530, 3],\n",
       " [28, 72, 8, 2111, 163],\n",
       " [72, 28, 1835, 3],\n",
       " [3521, 3],\n",
       " [2, 217, 177, 4, 2, 177, 4, 398, 741, 12, 2948, 893, 65, 2734, 3],\n",
       " [79, 72, 104, 2, 3209, 163],\n",
       " [1201, 163, 37, 575, 3],\n",
       " [48, 28, 128, 117, 2249, 1524, 3],\n",
       " [89, 28, 72, 1125, 163],\n",
       " [185, 72, 28, 1189, 3],\n",
       " [72, 28, 8, 586, 3],\n",
       " [37, 575, 1002, 3],\n",
       " [37, 29, 8, 1004, 722, 3],\n",
       " [37, 218, 298, 614, 28, 1439, 2352, 3],\n",
       " [106, 72, 104, 18, 163],\n",
       " [79,\n",
       "  72,\n",
       "  29,\n",
       "  114,\n",
       "  392,\n",
       "  7,\n",
       "  2094,\n",
       "  163,\n",
       "  37,\n",
       "  104,\n",
       "  5,\n",
       "  468,\n",
       "  45,\n",
       "  63,\n",
       "  15,\n",
       "  45,\n",
       "  1819,\n",
       "  6,\n",
       "  2931,\n",
       "  3],\n",
       " [37, 104, 72, 3],\n",
       " [37, 575, 1130, 3],\n",
       " [72, 3],\n",
       " [3542, 3, 3, 3, 3, 3, 101, 72, 28, 557, 3160, 3],\n",
       " [37, 29, 8, 348, 6, 72, 3216, 3],\n",
       " [37, 575, 188, 5, 1413, 3],\n",
       " [72, 28, 166, 1638, 3],\n",
       " [72, 28, 166, 1655, 163, 106, 72, 269, 10, 163],\n",
       " [815, 12, 167, 2032, 3, 3, 3],\n",
       " [10, 12, 1655, 163, 167, 586, 1484, 37, 575, 2752, 3],\n",
       " [75, 163, 37, 575, 8, 348, 117, 3],\n",
       " [79, 72, 104, 1153, 3473, 163],\n",
       " [37, 1353, 167, 181, 36, 51, 1065, 3],\n",
       " [77, 72, 861, 163],\n",
       " [79, 72, 480, 7, 815, 163],\n",
       " [754, 229, 8, 373, 3, 3, 3],\n",
       " [72, 218, 72, 28, 348, 3],\n",
       " [89, 79, 72, 218, 37, 575, 163],\n",
       " [2414, 163, 43, 702, 163, 37, 29, 198, 439, 1451, 3],\n",
       " [89, 28, 72, 163],\n",
       " [1262],\n",
       " [37, 2763, 638, 5, 2, 1530, 922, 289, 3],\n",
       " [37, 29, 5, 200, 101, 6, 563, 72, 3, 3, 3],\n",
       " [98, 28, 72, 20, 205, 101, 163],\n",
       " [2, 1065, 4, 181, 3],\n",
       " [37, 575, 188, 5, 2, 1714, 3],\n",
       " [754, 229, 2, 2879, 3],\n",
       " [18, 12, 8, 2879, 163, 8, 3772, 4, 2043, 3],\n",
       " [185, 402, 3],\n",
       " [77, 37, 29, 298, 1048, 163],\n",
       " [28, 72, 2, 2799, 163],\n",
       " [37, 575, 1932, 48, 83, 29, 33, 2022, 1512, 3],\n",
       " [37, 575, 32, 936, 5, 72, 114, 51, 3],\n",
       " [89, 12, 298, 344, 163],\n",
       " [89, 12, 8, 2900, 163],\n",
       " [3777, 72, 28, 2699],\n",
       " [1487, 8, 2749, 6, 2960, 1278, 9, 1151],\n",
       " [3427, 12, 75, 1498, 4, 707, 3, 79, 72, 3443, 1957, 163],\n",
       " [1519, 483, 28, 72, 1519, 163],\n",
       " [112, 89, 79, 72, 241, 5, 79, 163],\n",
       " [241, 5, 227, 994, 163],\n",
       " [1191, 3, 72, 28, 2207, 3],\n",
       " [124, 163, 37, 29, 5, 200, 101, 3],\n",
       " [37, 575, 8, 348, 163, 72, 269, 10, 163, 205, 163],\n",
       " [48, 306, 146, 3350, 3],\n",
       " [559, 84, 29, 1476, 3],\n",
       " [450,\n",
       "  163,\n",
       "  72,\n",
       "  28,\n",
       "  2343,\n",
       "  20,\n",
       "  2,\n",
       "  841,\n",
       "  163,\n",
       "  87,\n",
       "  74,\n",
       "  72,\n",
       "  3216,\n",
       "  345,\n",
       "  557,\n",
       "  1451,\n",
       "  37,\n",
       "  218,\n",
       "  37,\n",
       "  39,\n",
       "  107,\n",
       "  563,\n",
       "  3],\n",
       " [89, 12, 298, 248, 163],\n",
       " [89, 12, 298, 2798, 163],\n",
       " [1428, 3597, 52, 174, 6, 340, 229, 298, 248, 3],\n",
       " [77, 72, 480, 90, 163],\n",
       " [89, 575, 37, 163],\n",
       " [72, 28, 8, 2111, 3],\n",
       " [106, 72, 165, 298, 2079, 286, 163],\n",
       " [1404, 39, 1344, 5, 1379, 335, 48, 29, 676, 3178, 2003, 3],\n",
       " [72, 28, 2304, 121, 2, 617, 3],\n",
       " [35, 28, 1584, 3],\n",
       " [37, 107, 198, 2, 1022, 3],\n",
       " [256, 11, 7, 163, 59, 4, 72, 3, 10, 12, 18, 3, 2872, 185, 3],\n",
       " [89, 79, 72, 104, 5, 79, 9, 1235, 163],\n",
       " [37, 575, 8, 3786, 37, 575, 8, 3338, 117, 3],\n",
       " [59, 298, 522, 28, 1796, 5, 84, 3],\n",
       " [89, 49, 72, 1736, 163],\n",
       " [37, 575, 8, 3055, 887, 3],\n",
       " [744, 1619, 15, 2093, 3],\n",
       " [72, 297, 29, 229, 2305, 15, 738, 881, 3],\n",
       " [75, 75, 75, 3, 89, 79, 72, 241, 5, 1554, 163],\n",
       " [2143, 163, 3317, 72, 163],\n",
       " [167, 472, 12, 607, 3],\n",
       " [89, 575, 37, 112, 163],\n",
       " [1600, 163, 89, 575, 37, 1099, 3],\n",
       " [34, 12, 298, 2031, 163],\n",
       " [1508, 847, 3],\n",
       " [2369, 2369, 3],\n",
       " [79, 72, 104, 2758, 5, 229, 163],\n",
       " [25, 1908, 12, 8, 185, 155, 5, 29, 2, 68, 765, 3],\n",
       " [37, 575, 166, 1530, 3],\n",
       " [169, 77, 72, 137, 6, 32, 136, 245, 163],\n",
       " [89, 79, 72, 537, 5, 79, 163],\n",
       " [37, 107, 516, 37, 49, 365, 2, 868, 3],\n",
       " [72, 28, 1658],\n",
       " [48, 218, 3],\n",
       " [37, 29, 5, 563, 101, 3, 18, 12, 87, 1668, 3],\n",
       " [89, 12, 2, 1149, 2500, 4, 2476, 163],\n",
       " [37, 575, 8, 1695, 3],\n",
       " [72, 28, 75, 1175, 3],\n",
       " [89, 79, 72, 269, 54, 1315, 163],\n",
       " [169, 143, 28, 72, 163],\n",
       " [1941, 1048, 3],\n",
       " [98, 72, 256, 23, 163],\n",
       " [79, 72, 145, 60, 157, 163],\n",
       " [89, 12, 55, 163],\n",
       " [3442, 3],\n",
       " [575, 37, 2617, 163],\n",
       " [89, 12, 1331, 163],\n",
       " [72, 28, 8, 1697, 922, 3],\n",
       " [37, 575, 32, 642, 169, 157, 222, 37, 77, 26, 3],\n",
       " [2485, 3],\n",
       " [1448, 12, 2, 875, 4, 1549, 3],\n",
       " [37, 575, 1668, 72, 77, 32, 1913, 3],\n",
       " [34, 28, 72, 163],\n",
       " [28, 72, 1071, 163],\n",
       " [35, 3361, 6, 3426, 66, 72, 3],\n",
       " [169, 77, 48, 468, 163],\n",
       " [72, 269, 163, 72, 28, 166, 1697, 3],\n",
       " [89, 79, 72, 241, 5, 789, 163],\n",
       " [38, 3086, 106, 72, 517, 20, 3],\n",
       " [89, 49, 72, 104, 5, 638, 54, 163],\n",
       " [89, 79, 72, 218, 4, 2833, 163],\n",
       " [37, 575, 2, 1850, 3],\n",
       " [89, 12, 2, 76, 505, 5, 163],\n",
       " [89, 12, 167, 248, 163],\n",
       " [79, 72, 1389, 494, 11, 2, 580, 163],\n",
       " [37, 486, 5, 873, 72, 54, 2, 1441, 606, 9, 2, 3606, 952, 3],\n",
       " [37, 575, 8, 612, 2943, 3],\n",
       " [1600],\n",
       " [89, 881, 12, 58, 5, 638, 54, 163],\n",
       " [169, 157, 1181, 79, 72, 29, 163],\n",
       " [1273, 163, 72, 1389, 1942, 163],\n",
       " [79, 72, 104, 1370, 163],\n",
       " [72, 29, 305, 8, 554, 54, 1270, 3],\n",
       " [89, 12, 194, 1312, 194, 163],\n",
       " [2550, 163, 754, 229, 439, 54, 1619, 3],\n",
       " [37, 104, 2570],\n",
       " [28, 72, 107, 43, 2111, 163],\n",
       " [56, 31, 72, 587, 112, 163],\n",
       " [72, 349, 96, 697, 286, 3, 6, 72, 463, 943, 5, 1535, 2, 1679, 3],\n",
       " [77, 48, 200, 120, 5, 2, 1512, 54, 815, 163, 1416, 163],\n",
       " [663, 5, 8, 1084, 4, 809, 3, 3, 3]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate = 0.3\n",
    "\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run( tf.global_variables_initializer() )\n",
    "\n",
    "n_epoch = 1\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    total_err, steps = 0, 0\n",
    "    epoch_time = time.time()    \n",
    "    trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
    "    \n",
    "    # use minibatches generated from trainX, trainY \n",
    "    for X, Y in tl.iterate.minibatches(inputs=trainX, targets=trainY,\\\n",
    "                                batch_size=batch_size, shuffle=False):\n",
    "        step_time = time.time()\n",
    "        #_encode_seqs, _decode_seqs, _target_seqs, _target_mask = getEncodeNDecode(X,Y)\n",
    "        \n",
    "        #print(_encode_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_wordvec_embedding['birthday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/200000] Iter: 0 loss:8.253335 learning rate:1.000000 \n",
      "Epoch[0/200000] averaged loss:8.201649 took:20.35158s Bot Words 0 / 3816\n",
      "Epoch[1/200000] averaged loss:8.028055 took:18.77680s Bot Words 0 / 3816\n",
      "Epoch[2/200000] averaged loss:7.777000 took:18.82467s Bot Words 0 / 3816\n",
      "Epoch[3/200000] averaged loss:7.519334 took:19.76317s Bot Words 0 / 3816\n",
      "Epoch[4/200000] averaged loss:7.292147 took:18.86457s Bot Words 0 / 3816\n",
      "Epoch[5/200000] averaged loss:7.091491 took:18.83564s Bot Words 0 / 3816\n",
      "Epoch[6/200000] averaged loss:6.911118 took:18.13452s Bot Words 0 / 3816\n",
      "Epoch[7/200000] averaged loss:6.746180 took:18.18638s Bot Words 0 / 3816\n",
      "Epoch[8/200000] averaged loss:6.606684 took:18.44269s Bot Words 0 / 3816\n",
      "Epoch[9/200000] averaged loss:6.495537 took:20.35658s Bot Words 0 / 3816\n",
      "Epoch[10/200000] averaged loss:6.385993 took:18.99023s Bot Words 0 / 3816\n",
      "Epoch[11/200000] averaged loss:6.280138 took:18.34795s Bot Words 0 / 3816\n",
      "Epoch[12/200000] averaged loss:6.193547 took:18.83166s Bot Words 0 / 3816\n",
      "Epoch[13/200000] averaged loss:6.138730 took:27.22609s Bot Words 0 / 3816\n",
      "Epoch[14/200000] averaged loss:6.078363 took:30.47049s Bot Words 0 / 3816\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-7655ddc97c33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m         _, err = sess.run([train_op, loss],\n\u001b[0;32m     22\u001b[0m                         {encode_seqs: _encode_seqs,    decode_seqs: _decode_seqs,\n\u001b[1;32m---> 23\u001b[1;33m                         target_seqs: _target_seqs,     target_mask: _target_mask } )\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;31m#                lr: learning_rate }  )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mtotal_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run( tf.global_variables_initializer() )\n",
    "\n",
    "n_epoch = 200000\n",
    "bot_used_words = []\n",
    "\n",
    "for epoch in range(n_epoch):    \n",
    "    total_err, steps = 0, 0\n",
    "    epoch_time = time.time()    \n",
    "    trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
    "    \n",
    "    \n",
    "    # use minibatches generated from trainX, trainY \n",
    "    for X, Y in tl.iterate.minibatches(inputs=trainX, targets=trainY,\\\n",
    "                                batch_size=batch_size, shuffle=False):\n",
    "        step_time = time.time()\n",
    "        _encode_seqs, _decode_seqs, _target_seqs, _target_mask = getEncodeNDecode(X,Y)\n",
    "        \n",
    "        _, err = sess.run([train_op, loss],\n",
    "                        {encode_seqs: _encode_seqs,    decode_seqs: _decode_seqs,\n",
    "                        target_seqs: _target_seqs,     target_mask: _target_mask } )\n",
    "        #                lr: learning_rate }  )\n",
    "        total_err += err; \n",
    "        \n",
    "        #if steps % (100) == 0:\n",
    "        if epoch % (2000) == 0 and steps   == 0:\n",
    "            print(\"Epoch[%d/%d] Iter: %d loss:%f learning rate:%f \" \\\n",
    "                    % (epoch, n_epoch, steps,  err, 1) )\n",
    "            #print('global steps is ', global_step)\n",
    "        steps += 1\n",
    "\n",
    "                    \n",
    "    ##########\n",
    "    ## Decay Learning rate @ every epoch\n",
    "    ##########\n",
    "    #learning_rate = learning_rate * lr_decay_rate\n",
    "    #if learning_rate < min_learning_rate:\n",
    "    #    learning_rate = min_learning_rate\n",
    "        \n",
    "        \n",
    "    print(\"Epoch[%d/%d] averaged loss:%f took:%.5fs Bot Words %d / %d\" \\\n",
    "          % (epoch, n_epoch, total_err/steps, time.time()-epoch_time, len(bot_used_words) , len(word2id) ))\n",
    "    \n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2.0 : Faster Training + Infer words not in training\n",
    "1. Using small training embeddings\n",
    "2. For inference  using large / full word vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V 3.0 Must not remove .\n",
    "Because if we do not train it with end of sentence, then it is notgoing to be able to learn, when to enter full stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Computation Space/ time reduction :\n",
    "\n",
    "1. Negative Sampling for target sequences :\n",
    "2. Training : Smaller / limited WordVector embeddings\n",
    "3. Prediction : Trained embedding + Bigger embedding containing words not in the trained embeddings\n",
    "\n",
    "During training, use  only the words in the training instance. But since we want to be able to predict words that  are limited not only  by the  training words. During the prediction or chat bot resonse generation phase, we can use bigger embedding matrix for lookup to supplement the trained embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Future  :  To Dos\n",
    "\n",
    "\n",
    "\n",
    "1. **Loss Function** : Evaluate it against other loss functions such as perplexity, BLEU or other measures\n",
    "2. **Beam Search**: Increased performance implied by literatures.\n",
    "3. **Attention Mechanism** : Increased performance implied by literatures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
